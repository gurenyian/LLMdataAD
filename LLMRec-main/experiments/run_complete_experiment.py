import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple
import numpy as np
import logging
import asyncio
import aiohttp
from dataclasses import dataclass
import re

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'core'))

# å¯¼å…¥å¤–éƒ¨æ•°æ®é›†åŠ è½½å™¨
from external_dataset_loader import NetflixDatasetLoader, DatasetEnhancer

DEEPSEEK_CONFIG = {
    'api_key': os.getenv('DEEPSEEK_API_KEY', ''),  # ä»ç¯å¢ƒå˜é‡è·å–
    'base_url': 'https://api.deepseek.com/v1/chat/completions',
    'model': 'deepseek-chat',
    'timeout': 60,  # å¢åŠ è¶…æ—¶æ—¶é—´
    'max_retries': 3
}

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
print("ğŸ” æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
try:
    from core.data_distribution_analyzer import DataDistributionAnalyzer
    from core.dynamic_prompt_tuner import DynamicPromptTuner
    from core.adversarial_quality_module import AdversarialQualityModule
    from core.evaluation_metrics import InnovativeEvaluationMetrics
    print("âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    raise

# æ ¸å¿ƒæ¨¡å—æ˜ å°„
MODULES = {
    'DataDistributionAnalyzer': DataDistributionAnalyzer,
    'DynamicPromptTuner': DynamicPromptTuner,
    'AdversarialQualityAssurance': AdversarialQualityModule,
    'InnovativeEvaluationMetrics': InnovativeEvaluationMetrics
}

@dataclass
class AdversarialReport:
    """å¯¹æŠ—åˆ†ææŠ¥å‘Š"""
    discriminator_score: float
    identified_weaknesses: List[str]
    quality_metrics: Dict[str, float]
    improvement_suggestions: List[str]
    sample_scores: List[Tuple[str, float, str]]  # (sample, score, reason)

def setup_logging(experiment_name: str):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = os.path.join(project_root, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f'{experiment_name}_{timestamp}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    return logging.getLogger(__name__)

class DeepSeekLLMGenerator:
    """DeepSeek API è°ƒç”¨ç”Ÿæˆå™¨ - ä¿®å¤ Session ç®¡ç†"""
    
    def __init__(self, config: Dict = None):
        self.config = {**DEEPSEEK_CONFIG, **(config or {})}
        self.session = None
        self.logger = logging.getLogger(self.__class__.__name__)
        self._session_lock = asyncio.Lock()
        
        # æ·»åŠ APIè°ƒç”¨ç»Ÿè®¡
        self.api_call_count = 0
        self.api_success_count = 0
        self.api_fail_count = 0
    
    async def _ensure_session(self):
        """ç¡®ä¿ session å¯ç”¨"""
        async with self._session_lock:
            if self.session is None or self.session.closed:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.config['timeout'])
                )
    
    async def _close_session(self):
        """å®‰å…¨å…³é—­ session"""
        async with self._session_lock:
            if self.session and not self.session.closed:
                await self.session.close()
                self.session = None
    
    async def __aenter__(self):
        await self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self._close_session()
    
    def _build_recommendation_prompt(self, base_prompt: str, real_stats: Dict, 
                                   dataset_metadata: Dict, iteration: int = 0) -> str:
        """æ„å»ºåŸºäºçœŸå®æ•°æ®é›†çš„æ¨èPrompt"""
        
        user_stats = real_stats.get('user_stats', {})
        item_stats = real_stats.get('item_stats', {})
        
        context = f"""
ä½ æ˜¯ä¸€ä¸ªæ¨èç³»ç»Ÿæ•°æ®å¢å¼ºä¸“å®¶ã€‚è¯·æ ¹æ®Netflixæ•°æ®é›†ç‰¹å¾ç”Ÿæˆç”¨æˆ·-ç‰©å“äº¤äº’è®°å½•ã€‚

æ•°æ®é›†èƒŒæ™¯:
- åŸºäºNetflixç”µå½±æ•°æ®é›† ({dataset_metadata.get('total_items', 0)}éƒ¨ç”µå½±)
- å¹´ä»½èŒƒå›´: {dataset_metadata.get('year_range', (1900, 2023))}
- åŒ…å«ç»å…¸ç”µå½±å’Œç°ä»£ç”µå½±

çœŸå®ç”¨æˆ·äº¤äº’ç‰¹å¾:
- ç”¨æˆ·å¹³å‡äº¤äº’æ•°: {user_stats.get('mean', 3):.2f}
- ç”¨æˆ·æ´»è·ƒåº¦æ ‡å‡†å·®: {user_stats.get('std', 1.5):.2f}
- ç‰©å“æµè¡Œåº¦åŸºå°¼ç³»æ•°: {item_stats.get('gini', 0.5):.3f}
- é•¿å°¾ç‰©å“æ¯”ä¾‹: {item_stats.get('long_tail_ratio', 0.3):.3f}

ç”Ÿæˆè¦æ±‚:
1. æ¯è¡Œæ ¼å¼: ç”¨æˆ·ID ç‰©å“ID1 ç‰©å“ID2 ç‰©å“ID3...
2. ç”¨æˆ·IDèŒƒå›´: 0-{dataset_metadata.get('max_user_id', 999)}
3. ç‰©å“IDèŒƒå›´: 0-{dataset_metadata.get('total_items', 500)-1}
4. æ¯ä¸ªç”¨æˆ·2-8ä¸ªä¸é‡å¤ç‰©å“äº¤äº’
5. ä½“ç°çœŸå®çš„ç”µå½±è§‚çœ‹åå¥½ï¼ˆå¦‚å¹´ä»£åå¥½ã€ç±»å‹åå¥½ï¼‰
6. åŒ…å«é€‚é‡é•¿å°¾ç”µå½±ï¼Œå¢å¼ºæ•°æ®å¤šæ ·æ€§

{base_prompt}

è¯·ç”Ÿæˆ10æ¡ç¬¦åˆNetflixç”¨æˆ·è§‚å½±è¡Œä¸ºçš„äº¤äº’è®°å½•:
"""
        
        return context
    
    async def generate_samples_async(self, prompt: str, num_samples: int = 10) -> List[str]:
        """å¼‚æ­¥ç”Ÿæˆæ ·æœ¬ - ä¿®å¤ç‰ˆæœ¬"""
        self.api_call_count += 1
        
        # ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½æœ‰æ–°çš„ session
        await self._ensure_session()
        
        headers = {
            'Authorization': f'Bearer {self.config["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': self.config['model'],
            'messages': [{'role': 'user', 'content': prompt}],
            'max_tokens': 2000,
            'temperature': 0.7,
            'top_p': 0.9
        }
        
        samples = []
        
        try:
            batch_size = min(num_samples, 10)
            num_batches = (num_samples + batch_size - 1) // batch_size
            
            for batch in range(num_batches):
                # æ¯ä¸ªæ‰¹æ¬¡ç¡®ä¿ session å¯ç”¨
                await self._ensure_session()
                
                current_batch_size = min(batch_size, num_samples - batch * batch_size)
                batch_prompt = prompt + f"\nè¯·ç”Ÿæˆ{current_batch_size}æ¡è®°å½•:"
                
                payload['messages'][0]['content'] = batch_prompt
                
                for retry in range(self.config['max_retries']):
                    try:
                        async with self.session.post(
                            self.config['base_url'], 
                            headers=headers, 
                            json=payload
                        ) as response:
                            
                            if response.status == 200:
                                result = await response.json()
                                content = result['choices'][0]['message']['content']
                                
                                self.logger.info(f"ğŸ” DeepSeekå“åº”æˆåŠŸ - æ‰¹æ¬¡{batch+1}")
                                
                                batch_samples = self._parse_generated_content(content)
                                samples.extend(batch_samples)
                                
                                self.logger.info(f"æ‰¹æ¬¡ {batch+1}/{num_batches} ç”ŸæˆæˆåŠŸ: {len(batch_samples)} æ¡")
                                self.api_success_count += 1
                                break
                                
                            else:
                                error_text = await response.text()
                                self.logger.warning(f"APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status}): {error_text}")
                                
                    except Exception as e:
                        self.logger.warning(f"æ‰¹æ¬¡ {batch+1} é‡è¯• {retry+1}: {e}")
                        # åœ¨é‡è¯•å‰é‡æ–°åˆ›å»º session
                        await self._close_session()
                        await asyncio.sleep(2 ** retry)
                        await self._ensure_session()
                        
                        if retry == self.config['max_retries'] - 1:
                            self.logger.error(f"æ‰¹æ¬¡ {batch+1} æœ€ç»ˆå¤±è´¥")
                            self.api_fail_count += 1
                
                if batch < num_batches - 1:
                    await asyncio.sleep(1)
        
        except Exception as e:
            self.logger.error(f"DeepSeek API è°ƒç”¨å¤±è´¥ ({self.api_fail_count}/{self.api_call_count}): {e}")
            self.api_fail_count += 1
        
        return samples[:num_samples]
    
    def _parse_generated_content(self, content: str) -> List[str]:
        """è§£æLLMç”Ÿæˆçš„å†…å®¹ä¸ºæ ‡å‡†æ ¼å¼ - ä¿®å¤ç‰ˆæœ¬"""
        samples = []
        lines = content.strip().split('\n')
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        self.logger.info(f"ğŸ” å¼€å§‹è§£æå†…å®¹ï¼Œå…±{len(lines)}è¡Œ")
        
        for line_idx, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # ç§»é™¤å¸¸è§çš„å‰ç¼€ï¼Œä½†ä¿ç•™æ•°å­—
            prefixes_to_remove = ['â€¢', '-', '*', 'ç¤ºä¾‹:', 'æ ·æœ¬:', 'Example:', 'Sample:', 'ç”¨æˆ·']
            for prefix in prefixes_to_remove:
                if line.startswith(prefix):
                    line = line[len(prefix):].strip()
                    break
            
            # å°è¯•æå–æ•°å­—åºåˆ—
            numbers = re.findall(r'\d+', line)
            
            if len(numbers) >= 2:  # è‡³å°‘åŒ…å«ç”¨æˆ·IDå’Œä¸€ä¸ªç‰©å“ID
                try:
                    user_id = int(numbers[0])
                    items = [int(x) for x in numbers[1:]]
                    
                    # æ”¾å®½éªŒè¯æ¡ä»¶
                    if (0 <= user_id <= 99999 and  # æ”¾å®½ç”¨æˆ·IDèŒƒå›´
                        len(items) >= 1 and len(items) <= 15 and  # æ”¾å®½ç‰©å“æ•°é‡
                        all(0 <= item <= 99999 for item in items)):  # æ”¾å®½ç‰©å“IDèŒƒå›´
                        
                        # å»é‡ä½†ä¿æŒé¡ºåº
                        unique_items = []
                        seen = set()
                        for item in items:
                            if item not in seen:
                                unique_items.append(item)
                                seen.add(item)
                        
                        if len(unique_items) >= 1:
                            formatted_sample = f"{user_id} " + " ".join(map(str, unique_items))
                            samples.append(formatted_sample)
                            self.logger.debug(f"âœ… è§£ææˆåŠŸç¬¬{line_idx+1}è¡Œ: {formatted_sample}")
                            
                except ValueError as e:
                    self.logger.debug(f"âŒ è§£æå¤±è´¥ç¬¬{line_idx+1}è¡Œ: {e}")
                    continue
        
        self.logger.info(f"âœ… ä»DeepSeekå“åº”ä¸­è§£æå‡º {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        if samples:
            self.logger.info(f"ğŸ“‹ æ ·æœ¬ç¤ºä¾‹: {samples[0]}")
        else:
            self.logger.warning("âš ï¸ æœªèƒ½è§£æå‡ºä»»ä½•æœ‰æ•ˆæ ·æœ¬")
        
        return samples
    
    def generate_samples_sync(self, prompt: str, num_samples: int = 10) -> List[str]:
        """åŒæ­¥ç”Ÿæˆæ ·æœ¬æ¥å£ - ä¿®å¤ç‰ˆæœ¬"""
        async def _async_generate():
            try:
                await self._ensure_session()
                result = await self.generate_samples_async(prompt, num_samples)
                return result
            finally:
                await self._close_session()
        
        return asyncio.run(_async_generate())

class TrueAdversarialQualityModule:
    """çœŸæ­£çš„åŒLLMå¯¹æŠ—ç”Ÿæˆæ¨¡å— - ä¿®å¤åˆ¤åˆ«å™¨é—®é¢˜"""
    
    def __init__(self, llm_generator, config: Dict = None):
        self.llm_generator = llm_generator
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ç­–ç•¥è¿›åŒ–å†å²
        self.strategy_evolution = []
        self.discriminator_insights = []
        self.current_generation_strategy = self._get_initial_strategy()
        
        # å¯¹æŠ—è½®æ¬¡è®¡æ•°
        self.total_adversarial_rounds = 0
        self.successful_deceits = 0
        
    def _get_initial_strategy(self) -> Dict:
        """è·å–åˆå§‹ç”Ÿæˆç­–ç•¥"""
        return {
            'focus_areas': ['user_diversity', 'item_distribution', 'interaction_patterns'],
            'emphasis_weights': {'popularity': 0.6, 'long_tail': 0.4},
            'behavior_patterns': ['sequential_preferences', 'genre_consistency'],
            'quality_targets': {'authenticity': 0.8, 'diversity': 0.7, 'realism': 0.9}
        }
    
    async def run_true_adversarial_round(self, real_samples: List[str], 
                                       synthetic_samples: List[str], 
                                       round_idx: int) -> Dict:
        """çœŸæ­£çš„å¯¹æŠ—è½®æ¬¡ - åŒLLMåšå¼ˆ - ä¿®å¤ç‰ˆæœ¬"""
        
        self.logger.info(f"âš”ï¸ å¼€å§‹ç¬¬{round_idx}è½®çœŸå®å¯¹æŠ—åšå¼ˆ...")
        
        try:
            # Phase 1: åˆ¤åˆ«å™¨åˆ†æ - å¢åŠ é‡è¯•æœºåˆ¶
            max_retries = 2
            discriminator_report = None
            
            for retry in range(max_retries):
                try:
                    discriminator_report = await self._run_discriminator_analysis(
                        real_samples, synthetic_samples, round_idx
                    )
                    break
                except Exception as e:
                    self.logger.warning(f"åˆ¤åˆ«å™¨åˆ†æé‡è¯• {retry+1}/{max_retries}: {e}")
                    if retry == max_retries - 1:
                        # ä½¿ç”¨ç®€åŒ–çš„å›é€€åˆ¤åˆ«å™¨
                        discriminator_report = self._create_fallback_report(synthetic_samples)
            
            # ç¡®ä¿æœ‰æœ‰æ•ˆçš„åˆ¤åˆ«å™¨æŠ¥å‘Š
            if discriminator_report is None:
                discriminator_report = self._create_fallback_report(synthetic_samples)
            
            # Phase 2: ç”Ÿæˆå™¨åæ€ - LLM2 ä½œä¸ºç­–ç•¥ä¼˜åŒ–è€…  
            reflection_result = await self._run_generator_reflection(
                discriminator_report, round_idx
            )
            
            # Phase 3: ç­–ç•¥è¿›åŒ–
            evolved_strategy = self._evolve_generation_strategy(
                discriminator_report, reflection_result
            )
            
            # Phase 4: å¯¹æŠ—é‡ç”Ÿæˆ
            improved_samples = await self._adversarial_regeneration(
                evolved_strategy, len(synthetic_samples), round_idx
            )
            
            # Phase 5: æœ€ç»ˆéªŒè¯
            final_validation = await self._final_adversarial_validation(
                real_samples, improved_samples
            )
            
            # æ›´æ–°å¯¹æŠ—ç»Ÿè®¡
            self.total_adversarial_rounds += 1
            if final_validation.get('deception_success_rate', 0) > 0.7:
                self.successful_deceits += 1
            
            return {
                'round_idx': round_idx,
                'discriminator_report': discriminator_report,
                'generator_reflection': reflection_result,
                'evolved_strategy': evolved_strategy,
                'improved_samples': improved_samples,
                'final_validation': final_validation,
                'adversarial_metrics': {
                    'total_rounds': self.total_adversarial_rounds,
                    'success_rate': self.successful_deceits / max(1, self.total_adversarial_rounds),
                    'strategy_evolution_depth': len(self.strategy_evolution)
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å¯¹æŠ—è½®æ¬¡{round_idx}å¤±è´¥: {e}")
            return {
                'error': str(e), 
                'round_idx': round_idx,
                'fallback_used': True
            }
    
    async def _run_discriminator_analysis(self, real_samples: List[str], 
                                        synthetic_samples: List[str], 
                                        round_idx: int) -> AdversarialReport:
        """åˆ¤åˆ«å™¨åˆ†æé˜¶æ®µ - ä¿®å¤ç‰ˆæœ¬"""
        
        # ç®€åŒ–åˆ¤åˆ«å™¨Promptï¼Œå‡å°‘å¤æ‚åº¦ï¼Œæé«˜æˆåŠŸç‡
        discriminator_prompt = f"""
ä½ æ˜¯æ•°æ®çœŸå®æ€§ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®çš„çœŸå®æ€§ã€‚

çœŸå®æ•°æ®æ ·æœ¬(å‰5ä¸ª):
{self._format_samples_for_analysis(real_samples[:5])}

å¾…åˆ†ææ ·æœ¬(å‰10ä¸ª):
{self._format_samples_for_analysis(synthetic_samples[:10])}

è¯·è¯„ä¼°æ•°æ®çœŸå®æ€§å¹¶æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š

çœŸå®æ€§è¯„åˆ†: [0.0-1.0çš„æ•°å€¼]
ä¸»è¦é—®é¢˜: [é—®é¢˜1, é—®é¢˜2, é—®é¢˜3]
æ”¹è¿›å»ºè®®: [å»ºè®®1, å»ºè®®2, å»ºè®®3]

ç¤ºä¾‹:
çœŸå®æ€§è¯„åˆ†: 0.75
ä¸»è¦é—®é¢˜: [ç”¨æˆ·IDåˆ†å¸ƒä¸è‡ªç„¶, ç‰©å“ç»„åˆç¼ºä¹é€»è¾‘, äº¤äº’æ•°é‡è¿‡äºè§„æ•´]
æ”¹è¿›å»ºè®®: [å¢åŠ ç”¨æˆ·è¡Œä¸ºå¤šæ ·æ€§, ä¼˜åŒ–ç‰©å“é€‰æ‹©é€»è¾‘, æ¨¡æ‹ŸçœŸå®æ´»è·ƒåº¦åˆ†å¸ƒ]
"""
        
        try:
            # å¢åŠ è¶…æ—¶æ—¶é—´å’Œé‡è¯•é€»è¾‘
            await self.llm_generator._ensure_session()
            
            # ä½¿ç”¨æ›´çŸ­çš„prompté¿å…è¶…æ—¶
            response = await asyncio.wait_for(
                self.llm_generator.generate_samples_async(discriminator_prompt, 1),
                timeout=90  # 90ç§’è¶…æ—¶
            )
            
            if not response or not response[0].strip():
                self.logger.warning("åˆ¤åˆ«å™¨å“åº”ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åˆ†æ")
                return self._create_fallback_report(synthetic_samples)
            
            # ç®€åŒ–çš„å“åº”è§£æ
            report_data = self._parse_simple_discriminator_response(response[0])
            
            # æ„å»ºç»“æ„åŒ–æŠ¥å‘Š
            report = AdversarialReport(
                discriminator_score=report_data.get('authenticity_score', 0.5),
                identified_weaknesses=report_data.get('weaknesses', ['æœªè¯†åˆ«å…·ä½“é—®é¢˜']),
                quality_metrics={'overall_quality': report_data.get('authenticity_score', 0.5)},
                improvement_suggestions=report_data.get('suggestions', ['éœ€è¦è¿›ä¸€æ­¥åˆ†æ']),
                sample_scores=[]
            )
            
            self.discriminator_insights.append(report)
            self.logger.info(f"ğŸ” åˆ¤åˆ«å™¨åˆ†æå®Œæˆ - æ•´ä½“çœŸå®æ€§è¯„åˆ†: {report.discriminator_score:.3f}")
            
            return report
            
        except asyncio.TimeoutError:
            self.logger.error("âŒ åˆ¤åˆ«å™¨åˆ†æè¶…æ—¶")
            return self._create_fallback_report(synthetic_samples)
        except Exception as e:
            self.logger.error(f"âŒ åˆ¤åˆ«å™¨åˆ†æå¤±è´¥: {e}")
            return self._create_fallback_report(synthetic_samples)
    
    def _parse_simple_discriminator_response(self, response: str) -> Dict:
        """ç®€åŒ–çš„åˆ¤åˆ«å™¨å“åº”è§£æ - ä¿®å¤ç‰ˆæœ¬"""
        result = {
            'authenticity_score': 0.5,
            'weaknesses': ['å“åº”è§£æå¤±è´¥'],
            'suggestions': ['éœ€è¦é‡æ–°åˆ†æ']
        }
        
        try:
            self.logger.info(f"ğŸ” åˆ¤åˆ«å™¨åŸå§‹å“åº”: {response[:200]}...")  # è®°å½•å‰200å­—ç¬¦
            
            # æå–çœŸå®æ€§è¯„åˆ† - æ›´å®½æ¾çš„æ¨¡å¼
            score_patterns = [
                r'çœŸå®æ€§è¯„åˆ†[ï¼š:]\s*([0-9.]+)',
                r'è¯„åˆ†[ï¼š:]\s*([0-9.]+)', 
                r'åˆ†æ•°[ï¼š:]\s*([0-9.]+)',
                r'score[ï¼š:]\s*([0-9.]+)',
                r'([0-9]+\.?[0-9]*)/10',  # x/10æ ¼å¼
                r'([0-9]+\.?[0-9]*)\s*åˆ†',  # xåˆ†æ ¼å¼
                r'([0-9]+\.?[0-9]*)\s*%',   # ç™¾åˆ†æ¯”æ ¼å¼
            ]
            
            for pattern in score_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    try:
                        score = float(match.group(1))
                        if 0 <= score <= 1:
                            result['authenticity_score'] = score
                            self.logger.info(f"âœ… æå–åˆ°è¯„åˆ†: {score}")
                            break
                        elif score > 1 and score <= 10:  # 10åˆ†åˆ¶
                            result['authenticity_score'] = score / 10
                            self.logger.info(f"âœ… æå–åˆ°10åˆ†åˆ¶è¯„åˆ†: {score} -> {score/10}")
                            break
                        elif score > 10 and score <= 100:  # ç™¾åˆ†åˆ¶
                            result['authenticity_score'] = score / 100
                            self.logger.info(f"âœ… æå–åˆ°ç™¾åˆ†åˆ¶è¯„åˆ†: {score} -> {score/100}")
                            break
                    except ValueError:
                        continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯„åˆ†ï¼Œå°è¯•ä»æ–‡æœ¬å†…å®¹æ¨æ–­
            if result['authenticity_score'] == 0.5:
                response_lower = response.lower()
                if any(word in response_lower for word in ['å¾ˆå¥½', 'ä¼˜ç§€', 'é«˜è´¨é‡', 'good', 'excellent']):
                    result['authenticity_score'] = 0.8
                    self.logger.info("ğŸ“ æ ¹æ®ç§¯æè¯æ±‡æ¨æ–­è¯„åˆ†: 0.8")
                elif any(word in response_lower for word in ['è¾ƒå·®', 'é—®é¢˜', 'ä¸çœŸå®', 'poor', 'fake']):
                    result['authenticity_score'] = 0.3
                    self.logger.info("ğŸ“ æ ¹æ®æ¶ˆæè¯æ±‡æ¨æ–­è¯„åˆ†: 0.3")
                elif any(word in response_lower for word in ['ä¸€èˆ¬', 'ä¸­ç­‰', 'average', 'medium']):
                    result['authenticity_score'] = 0.6
                    self.logger.info("ğŸ“ æ ¹æ®ä¸­æ€§è¯æ±‡æ¨æ–­è¯„åˆ†: 0.6")
            
            # æå–é—®é¢˜åˆ—è¡¨ - æ›´çµæ´»çš„åŒ¹é…
            weakness_patterns = [
                r'ä¸»è¦é—®é¢˜[ï¼š:]\s*\[(.*?)\]',
                r'é—®é¢˜[ï¼š:]\s*\[(.*?)\]',
                r'å¼±ç‚¹[ï¼š:]\s*\[(.*?)\]',
                r'ç¼ºé™·[ï¼š:]\s*(.+?)(?=æ”¹è¿›|å»ºè®®|$)',
                r'é—®é¢˜åŒ…æ‹¬[ï¼š:](.+?)(?=å»ºè®®|æ”¹è¿›|$)',
            ]
            
            for pattern in weakness_patterns:
                match = re.search(pattern, response)
                if match:
                    weaknesses_text = match.group(1)
                    # å¤„ç†åˆ—è¡¨æ ¼å¼
                    if '[' in weaknesses_text and ']' in weaknesses_text:
                        weaknesses_text = weaknesses_text.strip('[]')
                    
                    weaknesses = [w.strip().strip('"\'') for w in re.split('[,ï¼Œ]', weaknesses_text) if w.strip()]
                    if weaknesses:
                        result['weaknesses'] = weaknesses[:5]
                        self.logger.info(f"âœ… æå–åˆ°é—®é¢˜: {weaknesses[:3]}")
                        break
            
            # æå–æ”¹è¿›å»ºè®®
            suggestion_patterns = [
                r'æ”¹è¿›å»ºè®®[ï¼š:]\s*\[(.*?)\]',
                r'å»ºè®®[ï¼š:]\s*\[(.*?)\]',
                r'æ”¹è¿›æ–¹å‘[ï¼š:]\s*(.+?)(?=é—®é¢˜|$)',
                r'å»ºè®®åŒ…æ‹¬[ï¼š:](.+?)$',
            ]
            
            for pattern in suggestion_patterns:
                match = re.search(pattern, response)
                if match:
                    suggestions_text = match.group(1)
                    if '[' in suggestions_text and ']' in suggestions_text:
                        suggestions_text = suggestions_text.strip('[]')
                    
                    suggestions = [s.strip().strip('"\'') for s in re.split('[,ï¼Œ]', suggestions_text) if s.strip()]
                    if suggestions:
                        result['suggestions'] = suggestions[:5]
                        self.logger.info(f"âœ… æå–åˆ°å»ºè®®: {suggestions[:3]}")
                        break
            
            self.logger.info(f"âœ… åˆ¤åˆ«å™¨è§£æå®Œæˆ - è¯„åˆ†: {result['authenticity_score']:.3f}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ¤åˆ«å™¨å“åº”è§£æå¼‚å¸¸: {e}")
            # æä¾›æ›´å¥½çš„é»˜è®¤å€¼
            result['authenticity_score'] = 0.5
            result['weaknesses'] = ['è§£æå¼‚å¸¸ï¼Œæ— æ³•è¯†åˆ«å…·ä½“é—®é¢˜']
            result['suggestions'] = ['é‡æ–°ä¼˜åŒ–ç”Ÿæˆç­–ç•¥']
        
        return result

    def _create_fallback_report(self, synthetic_samples: List[str]) -> AdversarialReport:
        """åˆ›å»ºå›é€€åˆ¤åˆ«å™¨æŠ¥å‘Š"""
        # åŸºäºæ ·æœ¬è¿›è¡Œç®€å•çš„å¯å‘å¼åˆ†æ
        if not synthetic_samples:
            return AdversarialReport(
                discriminator_score=0.3,
                identified_weaknesses=['æ— æ ·æœ¬å¯åˆ†æ'],
                quality_metrics={'overall_quality': 0.3},
                improvement_suggestions=['é‡æ–°ç”Ÿæˆæ ·æœ¬'],
                sample_scores=[]
            )
        
        # ç®€å•çš„è´¨é‡è¯„ä¼°
        user_ids = []
        item_counts = []
        
        for sample in synthetic_samples[:20]:  # åªåˆ†æå‰20ä¸ªæ ·æœ¬
            try:
                parts = sample.strip().split()
                if len(parts) >= 2:
                    user_ids.append(int(parts[0]))
                    item_counts.append(len(parts) - 1)
            except:
                continue
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        if user_ids and item_counts:
            user_diversity = len(set(user_ids)) / len(user_ids) if user_ids else 0
            avg_items = sum(item_counts) / len(item_counts)
            
            # å¯å‘å¼è¯„åˆ†
            score = 0.5  # åŸºç¡€åˆ†
            if user_diversity > 0.8:
                score += 0.1
            if 3 <= avg_items <= 8:
                score += 0.1
            
            weaknesses = []
            suggestions = []
            
            if user_diversity < 0.5:
                weaknesses.append('ç”¨æˆ·IDé‡å¤ç‡è¿‡é«˜')
                suggestions.append('å¢åŠ ç”¨æˆ·å¤šæ ·æ€§')
            
            if avg_items > 10:
                weaknesses.append('å¹³å‡äº¤äº’æ•°è¿‡é«˜')
                suggestions.append('å‡å°‘æ¯ç”¨æˆ·çš„äº¤äº’æ•°é‡')
            elif avg_items < 2:
                weaknesses.append('å¹³å‡äº¤äº’æ•°è¿‡ä½')
                suggestions.append('å¢åŠ æ¯ç”¨æˆ·çš„äº¤äº’æ•°é‡')
            
            if not weaknesses:
                weaknesses = ['æ ¼å¼åŸºæœ¬æ­£ç¡®ä½†éœ€è¦ä¼˜åŒ–']
                suggestions = ['æå‡æ•°æ®çœŸå®æ€§']
        else:
            score = 0.3
            weaknesses = ['æ ·æœ¬æ ¼å¼æœ‰é—®é¢˜']
            suggestions = ['æ£€æŸ¥æ•°æ®æ ¼å¼']
        
        return AdversarialReport(
            discriminator_score=min(score, 0.9),
            identified_weaknesses=weaknesses,
            quality_metrics={'overall_quality': score},
            improvement_suggestions=suggestions,
            sample_scores=[]
        )
    
    async def _run_generator_reflection(self, discriminator_report: AdversarialReport, 
                                      round_idx: int) -> Dict:
        """ç”Ÿæˆå™¨åæ€é˜¶æ®µ - ç­–ç•¥ä¼˜åŒ–"""
        
        reflection_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆç­–ç•¥ä¼˜åŒ–ä¸“å®¶ã€‚åˆšæ‰ï¼Œä¸€ä¸ªä¸“ä¸šçš„é‰´åˆ«å™¨å¯¹ä½ ç”Ÿæˆçš„æ•°æ®è¿›è¡Œäº†æ·±åº¦åˆ†æï¼Œå‘ç°äº†ä¸€äº›é—®é¢˜ã€‚
ç°åœ¨ä½ éœ€è¦åŸºäºè¿™äº›åé¦ˆè¿›è¡Œæ·±å…¥åæ€ï¼Œå¹¶åˆ¶å®šæ›´å¥½çš„ç”Ÿæˆç­–ç•¥ã€‚

## é‰´åˆ«å™¨çš„å‘ç°
æ•´ä½“çœŸå®æ€§è¯„åˆ†ï¼š{discriminator_report.discriminator_score:.3f}

### è¯†åˆ«å‡ºçš„ä¸»è¦å¼±ç‚¹ï¼š
{chr(10).join(f"- {weakness}" for weakness in discriminator_report.identified_weaknesses)}

### è¯¦ç»†è´¨é‡æŒ‡æ ‡ï¼š
{json.dumps(discriminator_report.quality_metrics, indent=2, ensure_ascii=False)}

### é‰´åˆ«å™¨çš„æ”¹è¿›å»ºè®®ï¼š
{chr(10).join(f"- {suggestion}" for suggestion in discriminator_report.improvement_suggestions)}

## å½“å‰ç”Ÿæˆç­–ç•¥
{json.dumps(self.current_generation_strategy, indent=2, ensure_ascii=False)}

## åæ€ä»»åŠ¡
è¯·åŸºäºé‰´åˆ«å™¨çš„å‘ç°ï¼Œè¿›è¡Œæ·±åº¦è‡ªæˆ‘åæ€å¹¶åˆ¶å®šæ”¹è¿›ç­–ç•¥ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š

é¢„æœŸæ”¹è¿›: [0.0-1.0çš„æ•°å€¼ï¼Œè¡¨ç¤ºä½ è®¤ä¸ºæ”¹è¿›åè´¨é‡èƒ½æå‡å¤šå°‘]
æ ¸å¿ƒé—®é¢˜: [é—®é¢˜1, é—®é¢˜2, é—®é¢˜3]
ä¼˜åŒ–ç­–ç•¥: [ç­–ç•¥1, ç­–ç•¥2, ç­–ç•¥3]

ç¤ºä¾‹:
é¢„æœŸæ”¹è¿›: 0.7
æ ¸å¿ƒé—®é¢˜: [ç”¨æˆ·è¡Œä¸ºæ¨¡å¼è¿‡äºè§„æ•´, ç‰©å“é€‰æ‹©ç¼ºä¹ä¸ªæ€§åŒ–, äº¤äº’åºåˆ—ä¸å¤Ÿè‡ªç„¶]
ä¼˜åŒ–ç­–ç•¥: [å¢åŠ è¡Œä¸ºéšæœºæ€§, å¼ºåŒ–ä¸ªäººåå¥½å»ºæ¨¡, æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ä¹ æƒ¯]
"""
        
        try:
            # ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½æœ‰æ–°çš„ session
            await self.llm_generator._ensure_session()
            
            # è°ƒç”¨LLMè¿›è¡Œåæ€
            response = await self.llm_generator.generate_samples_async(reflection_prompt, 1)
            if not response:
                raise ValueError("ç”Ÿæˆå™¨åæ€å“åº”ä¸ºç©º")
            
            # è§£æåæ€ç»“æœ
            reflection_data = self._parse_generator_reflection(response[0])
            
            self.logger.info(f"ğŸ¤” ç”Ÿæˆå™¨åæ€å®Œæˆ - é¢„æœŸæ”¹è¿›: {reflection_data.get('confidence_assessment', {}).get('expected_improvement', 0):.3f}")
            
            return reflection_data
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå™¨åæ€å¤±è´¥: {e}")
            return {
                'root_cause_analysis': {'primary_issues': ['åæ€è¿‡ç¨‹å¼‚å¸¸']},
                'optimization_strategy': {'immediate_fixes': []},
                'new_generation_principles': ['éœ€è¦é‡æ–°åæ€'],
                'confidence_assessment': {'expected_improvement': 0.4}  # ç»™ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
            }
    
    def _parse_generator_reflection(self, response: str) -> Dict:
        """è§£æç”Ÿæˆå™¨åæ€å“åº” - ä¿®å¤ç‰ˆæœ¬"""
        self.logger.info(f"ğŸ” åæ€åŸå§‹å“åº”: {response[:200]}...")
        
        # å°è¯•è§£æJSON
        parsed_json = self._parse_json_response(response)
        if parsed_json and 'confidence_assessment' in parsed_json:
            return parsed_json
        
        # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬è§£æå›é€€
        result = {
            'root_cause_analysis': {'primary_issues': []},
            'optimization_strategy': {'immediate_fixes': []},
            'new_generation_principles': [],
            'confidence_assessment': {'expected_improvement': 0.1}
        }
        
        try:
            # æå–é¢„æœŸæ”¹è¿›å€¼
            improvement_patterns = [
                r'expected_improvement["\']?\s*:\s*([0-9.]+)',
                r'é¢„æœŸæ”¹è¿›[ï¼š:]\s*([0-9.]+)',
                r'æ”¹è¿›é¢„æœŸ[ï¼š:]\s*([0-9.]+)',
                r'æå‡å¹…åº¦[ï¼š:]\s*([0-9.]+)',
            ]
            
            for pattern in improvement_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    try:
                        improvement = float(match.group(1))
                        if 0 <= improvement <= 1:
                            result['confidence_assessment']['expected_improvement'] = improvement
                            self.logger.info(f"âœ… æå–åˆ°é¢„æœŸæ”¹è¿›: {improvement}")
                            break
                        elif improvement > 1 and improvement <= 100:  # ç™¾åˆ†æ¯”æ ¼å¼
                            result['confidence_assessment']['expected_improvement'] = improvement / 100
                            self.logger.info(f"âœ… æå–åˆ°ç™¾åˆ†æ¯”æ”¹è¿›: {improvement}% -> {improvement/100}")
                            break
                    except ValueError:
                        continue
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œæ ¹æ®æ–‡æœ¬å†…å®¹æ¨æ–­
            if result['confidence_assessment']['expected_improvement'] == 0.1:
                response_lower = response.lower()
                if any(word in response_lower for word in ['æ˜¾è‘—æå‡', 'å¤§å¹…æ”¹è¿›', 'significant', 'substantial']):
                    result['confidence_assessment']['expected_improvement'] = 0.8
                    self.logger.info("ğŸ“ æ ¹æ®ç§¯æè¯æ±‡æ¨æ–­æ”¹è¿›: 0.8")
                elif any(word in response_lower for word in ['é€‚åº¦æå‡', 'ä¸­ç­‰æ”¹è¿›', 'moderate', 'medium']):
                    result['confidence_assessment']['expected_improvement'] = 0.5
                    self.logger.info("ğŸ“ æ ¹æ®ä¸­æ€§è¯æ±‡æ¨æ–­æ”¹è¿›: 0.5")
                elif any(word in response_lower for word in ['è½»å¾®æå‡', 'å°å¹…æ”¹è¿›', 'slight', 'minor']):
                    result['confidence_assessment']['expected_improvement'] = 0.3
                    self.logger.info("ğŸ“ æ ¹æ®ä¿å®ˆè¯æ±‡æ¨æ–­æ”¹è¿›: 0.3")
            
            # æå–ä¸»è¦é—®é¢˜
            issue_patterns = [
                r'primary_issues["\']?\s*:\s*\[(.*?)\]',
                r'æ ¸å¿ƒé—®é¢˜[ï¼š:]\s*\[(.*?)\]',
                r'ä¸»è¦é—®é¢˜[ï¼š:]\s*(.+?)(?=ç­–ç•¥|å»ºè®®|$)',
            ]
            
            for pattern in issue_patterns:
                match = re.search(pattern, response)
                if match:
                    issues_text = match.group(1)
                    issues = [i.strip().strip('"\'') for i in re.split('[,ï¼Œ]', issues_text) if i.strip()]
                    if issues:
                        result['root_cause_analysis']['primary_issues'] = issues[:3]
                        self.logger.info(f"âœ… æå–åˆ°ä¸»è¦é—®é¢˜: {issues[:2]}")
                        break
            
            self.logger.info(f"âœ… åæ€è§£æå®Œæˆ - é¢„æœŸæ”¹è¿›: {result['confidence_assessment']['expected_improvement']:.3f}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åæ€å“åº”è§£æå¼‚å¸¸: {e}")
            result['confidence_assessment']['expected_improvement'] = 0.4  # ç»™ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
        
        return result
    
    def _evolve_generation_strategy(self, discriminator_report: AdversarialReport, 
                                  reflection_result: Dict) -> Dict:
        """ç­–ç•¥è¿›åŒ– - åŸºäºå¯¹æŠ—åé¦ˆä¼˜åŒ–ç”Ÿæˆç­–ç•¥"""
        
        current_strategy = self.current_generation_strategy.copy()
        
        # åŸºäºåˆ¤åˆ«å™¨æŠ¥å‘Šè°ƒæ•´ç­–ç•¥
        if discriminator_report.discriminator_score < 0.7:
            # çœŸå®æ€§ä¸è¶³ï¼ŒåŠ å¼ºçœŸå®æ€§ç­–ç•¥
            if 'authenticity_enhancement' not in current_strategy:
                current_strategy['authenticity_enhancement'] = {}
            
            current_strategy['authenticity_enhancement'].update({
                'behavioral_realism': 0.9,
                'pattern_concealment': 0.8,
                'natural_randomness': 0.7
            })
        
        # åŸºäºåæ€ç»“æœè°ƒæ•´ç­–ç•¥
        optimization_strategy = reflection_result.get('optimization_strategy', {})
        immediate_fixes = optimization_strategy.get('immediate_fixes', [])
        
        for fix in immediate_fixes:
            if fix.get('priority') == 'high':
                issue = fix.get('issue', '')
                solution = fix.get('solution', '')
                
                if 'distribution' in issue.lower():
                    current_strategy['emphasis_weights']['distribution_realism'] = 0.9
                elif 'sequence' in issue.lower():
                    current_strategy['behavior_patterns'].append('sequence_naturalness')
                elif 'diversity' in issue.lower():
                    current_strategy['quality_targets']['diversity'] = min(0.95, current_strategy['quality_targets']['diversity'] + 0.1)
        
        # é›†æˆæ–°çš„ç”ŸæˆåŸåˆ™
        new_principles = reflection_result.get('new_generation_principles', [])
        if 'generation_principles' not in current_strategy:
            current_strategy['generation_principles'] = []
        current_strategy['generation_principles'].extend(new_principles[:3])  # é¿å…è¿‡å¤šåŸåˆ™
        
        # æ›´æ–°è´¨é‡ç›®æ ‡
        confidence = reflection_result.get('confidence_assessment', {})
        expected_improvement = confidence.get('expected_improvement', 0)
        if expected_improvement > 0.5:
            for target in current_strategy['quality_targets']:
                current_strategy['quality_targets'][target] *= (1 + expected_improvement * 0.2)
                current_strategy['quality_targets'][target] = min(1.0, current_strategy['quality_targets'][target])
        
        # è®°å½•ç­–ç•¥è¿›åŒ–
        evolution_record = {
            'round': len(self.strategy_evolution),
            'discriminator_score': discriminator_report.discriminator_score,
            'main_issues': discriminator_report.identified_weaknesses[:3],
            'strategy_changes': self._compute_strategy_diff(self.current_generation_strategy, current_strategy),
            'expected_improvement': expected_improvement
        }
        
        self.strategy_evolution.append(evolution_record)
        self.current_generation_strategy = current_strategy
        
        self.logger.info(f"ğŸ§¬ ç­–ç•¥è¿›åŒ–å®Œæˆ - ç¬¬{len(self.strategy_evolution)}ä»£ç­–ç•¥")
        
        return current_strategy
    
    async def _adversarial_regeneration(self, evolved_strategy: Dict, 
                                      num_samples: int, round_idx: int) -> List[str]:
        """å¯¹æŠ—é‡ç”Ÿæˆ - ä½¿ç”¨è¿›åŒ–ç­–ç•¥ç”Ÿæˆæ›´éš¾è¯†åˆ«çš„æ•°æ®"""
        
        # æ„å»ºå¯¹æŠ—ç”ŸæˆPrompt
        adversarial_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆå¤§å¸ˆï¼Œç°åœ¨éœ€è¦ç”Ÿæˆæå…¶çœŸå®çš„ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®ã€‚
ä½ å·²ç»é€šè¿‡æ·±åº¦å­¦ä¹ äº†è§£äº†å¦‚ä½•é¿å…è¢«é‰´åˆ«å™¨è¯†åˆ«ï¼Œç°åœ¨è¦å®æ–½æœ€æ–°çš„"åæ£€æµ‹"ç­–ç•¥ã€‚

## è¿›åŒ–åçš„ç”Ÿæˆç­–ç•¥
{json.dumps(evolved_strategy, indent=2, ensure_ascii=False)}

## å¯¹æŠ—ä»»åŠ¡ç›®æ ‡
1. ç”Ÿæˆçš„æ•°æ®å¿…é¡»æå…¶çœŸå®ï¼Œéš¾ä»¥è¢«ä¸“ä¸šé‰´åˆ«å™¨è¯†åˆ«
2. é¿å…ä¹‹å‰è¢«å‘ç°çš„æ‰€æœ‰å¼±ç‚¹å’Œæ¨¡å¼
3. é‡‡ç”¨æœ€è‡ªç„¶çš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼
4. ç¡®ä¿ç‰©å“é€‰æ‹©ç¬¦åˆçœŸå®è§‚å½±é€»è¾‘

## åæ£€æµ‹è¦æ±‚
- é¿å…è§„å¾‹æ€§å’Œé‡å¤æ¨¡å¼
- ä½¿ç”¨è‡ªç„¶çš„éšæœºæ€§è€Œéäººå·¥éšæœº
- æ¨¡æ‹ŸçœŸå®çš„ç”¨æˆ·åå¥½æ¼”åŒ–
- èå…¥ç»†å¾®çš„ä¸ªæ€§åŒ–ç‰¹å¾

## è´¨é‡æ ‡å‡†ï¼ˆç¬¬{round_idx+1}è½®å¼ºåŒ–ï¼‰
- è¡Œä¸ºçœŸå®æ€§: â‰¥{evolved_strategy.get('quality_targets', {}).get('authenticity', 0.8):.2f}
- å¤šæ ·æ€§æ°´å¹³: â‰¥{evolved_strategy.get('quality_targets', {}).get('diversity', 0.7):.2f}  
- ç°å®æ€§ç¨‹åº¦: â‰¥{evolved_strategy.get('quality_targets', {}).get('realism', 0.9):.2f}

## ç”ŸæˆæŒ‡ä»¤
è¯·ç”Ÿæˆ{num_samples}æ¡Netflixç”¨æˆ·äº¤äº’è®°å½•ï¼Œæ¯æ¡è®°å½•æ ¼å¼ä¸ºï¼š
ç”¨æˆ·ID ç‰©å“ID1 ç‰©å“ID2 ç‰©å“ID3...

ç‰¹åˆ«æ³¨æ„ï¼š
1. æ¯ä¸ªç”¨æˆ·çš„ç‰©å“é€‰æ‹©è¦ä½“ç°ä¸ªäººå“å‘³
2. äº¤äº’æ•°é‡è¦è‡ªç„¶åˆ†å¸ƒï¼ˆ2-8ä¸ªç‰©å“ï¼‰
3. èå…¥çœŸå®çš„è§‚å½±é€»è¾‘ï¼ˆå¦‚ç±»å‹åå¥½ã€å¹´ä»£åå¥½ï¼‰
4. é¿å…è¢«é‰´åˆ«å™¨å‘ç°çš„ç”Ÿæˆç—•è¿¹

å¼€å§‹ç”Ÿæˆï¼š
"""
        
        try:
            # ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½æœ‰æ–°çš„ session
            await self.llm_generator._ensure_session()
            
            # ä½¿ç”¨è¿›åŒ–ç­–ç•¥ç”Ÿæˆæ ·æœ¬
            improved_samples = await self.llm_generator.generate_samples_async(
                adversarial_prompt, num_samples
            )
            
            if not improved_samples:
                self.logger.warning("âš ï¸ å¯¹æŠ—é‡ç”Ÿæˆè¿”å›ç©ºç»“æœ")
                return []
            
            self.logger.info(f"ğŸ”„ å¯¹æŠ—é‡ç”Ÿæˆå®Œæˆ - ç”Ÿæˆ{len(improved_samples)}ä¸ªæ”¹è¿›æ ·æœ¬")
            return improved_samples
            
        except Exception as e:
            self.logger.error(f"âŒ å¯¹æŠ—é‡ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    async def _final_adversarial_validation(self, real_samples: List[str], 
                                          improved_samples: List[str]) -> Dict:
        """æœ€ç»ˆå¯¹æŠ—éªŒè¯ - ç®€åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘å¤±è´¥æ¦‚ç‡"""
        
        # å¦‚æœæ²¡æœ‰æ”¹è¿›æ ·æœ¬ï¼Œè¿”å›é»˜è®¤ç»“æœ
        if not improved_samples:
            return {
                'deception_success_rate': 0.3,
                'anti_detection_score': 0.3,
                'improvement_over_baseline': 0.1,
                'overall_assessment': 'æ— æ”¹è¿›æ ·æœ¬'
            }
        
        # ç®€åŒ–çš„éªŒè¯é€»è¾‘ï¼Œé¿å…å¤æ‚çš„LLMè°ƒç”¨
        try:
            # åŸºæœ¬è´¨é‡æ£€æŸ¥
            valid_samples = 0
            for sample in improved_samples:
                try:
                    parts = sample.strip().split()
                    if len(parts) >= 2 and all(part.isdigit() for part in parts):
                        valid_samples += 1
                except:
                    continue
            
            if valid_samples == 0:
                quality_rate = 0.2
            else:
                quality_rate = min(valid_samples / len(improved_samples), 0.9)
            
            # è®¡ç®—æ¬ºéª—æˆåŠŸç‡ï¼ˆåŸºäºæ ·æœ¬è´¨é‡çš„å¯å‘å¼è¯„ä¼°ï¼‰
            deception_rate = min(quality_rate + 0.2, 0.8)
            
            self.logger.info(f"âœ… æœ€ç»ˆå¯¹æŠ—éªŒè¯å®Œæˆ - æ¬ºéª—æˆåŠŸç‡: {deception_rate:.3f}")
            
            return {
                'deception_success_rate': deception_rate,
                'anti_detection_score': quality_rate,
                'improvement_over_baseline': max(0.1, quality_rate - 0.3),
                'overall_assessment': f'è´¨é‡ç‡: {quality_rate:.2f}, æœ‰æ•ˆæ ·æœ¬: {valid_samples}/{len(improved_samples)}',
                'remaining_risks': ['åŸºäºå¯å‘å¼è¯„ä¼°'],
                'adversarial_strength': {
                    'concealment_level': quality_rate,
                    'authenticity_mimicry': min(quality_rate + 0.1, 0.8),
                    'pattern_disruption': quality_rate * 0.8
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æœ€ç»ˆéªŒè¯è®¡ç®—å¤±è´¥: {e}")
            return {
                'deception_success_rate': 0.4,
                'anti_detection_score': 0.4,
                'improvement_over_baseline': 0.1,
                'overall_assessment': 'éªŒè¯è®¡ç®—å¼‚å¸¸'
            }
    
    def _format_samples_for_analysis(self, samples: List[str]) -> str:
        """æ ¼å¼åŒ–æ ·æœ¬ç”¨äºåˆ†æ"""
        formatted = []
        for i, sample in enumerate(samples):
            formatted.append(f"æ ·æœ¬{i+1}: {sample}")
        return "\n".join(formatted)
    
    def _parse_discriminator_response(self, response: str) -> Dict:
        """è§£æåˆ¤åˆ«å™¨å“åº”"""
        try:
            # å°è¯•æå–JSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                # åŸºäºæ–‡æœ¬è§£æçš„å›é€€æ–¹æ¡ˆ
                return self._parse_text_response(response)
        except Exception as e:
            self.logger.warning(f"åˆ¤åˆ«å™¨å“åº”è§£æå¤±è´¥: {e}")
            return {
                'overall_authenticity_score': 0.5,
                'identified_weaknesses': ['å“åº”è§£æå¤±è´¥'],
                'improvement_directions': ['éœ€è¦é‡æ–°åˆ†æ']
            }
    
    def _parse_json_response(self, response: str) -> Dict:
        """é€šç”¨JSONå“åº”è§£æ - å¢å¼ºç‰ˆæœ¬"""
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned = response.strip()
            
            # ç§»é™¤markdownæ ‡è®°
            if cleaned.startswith('```json'):
                cleaned = cleaned[7:]
            elif cleaned.startswith('```'):
                cleaned = cleaned[3:]
            if cleaned.endswith('```'):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            # å°è¯•ç›´æ¥è§£æ
            try:
                return json.loads(cleaned)
            except json.JSONDecodeError:
                pass
            
            # å°è¯•æ‰¾åˆ°JSONå—
            json_patterns = [
                r'\{.*\}',  # ç®€å•çš„å¤§æ‹¬å·åŒ¹é…
                r'\{[\s\S]*\}',  # åŒ…å«æ¢è¡Œçš„åŒ¹é…
            ]
            
            for pattern in json_patterns:
                json_match = re.search(pattern, cleaned, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    try:
                        return json.loads(json_str)
                    except json.JSONDecodeError:
                        continue
            
            self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
            return {}
            
        except Exception as e:
            self.logger.warning(f"JSONè§£æå¤±è´¥: {e}")
            return {}
    
    def _parse_text_response(self, response: str) -> Dict:
        """æ–‡æœ¬å“åº”çš„å›é€€è§£æ"""
        # ç®€å•çš„æ–‡æœ¬æ¨¡å¼è¯†åˆ«
        result = {
            'overall_authenticity_score': 0.5,
            'identified_weaknesses': [],
            'improvement_directions': []
        }
        
        # å¯»æ‰¾è¯„åˆ†
        score_match = re.search(r'(?:è¯„åˆ†|åˆ†æ•°|score)[ï¼š:]\s*([0-9.]+)', response)
        if score_match:
            try:
                result['overall_authenticity_score'] = float(score_match.group(1))
            except:
                pass
        
        # å¯»æ‰¾é—®é¢˜åˆ—è¡¨
        weakness_patterns = [r'é—®é¢˜[ï¼š:](.+)', r'å¼±ç‚¹[ï¼š:](.+)', r'ç¼ºé™·[ï¼š:](.+)']
        for pattern in weakness_patterns:
            matches = re.findall(pattern, response)
            result['identified_weaknesses'].extend(matches)
        
        return result
    
    def _compute_strategy_diff(self, old_strategy: Dict, new_strategy: Dict) -> List[str]:
        """è®¡ç®—ç­–ç•¥å˜åŒ–"""
        changes = []
        
        # ç®€å•çš„é”®å€¼æ¯”è¾ƒ
        for key in new_strategy:
            if key not in old_strategy:
                changes.append(f"æ–°å¢: {key}")
            elif old_strategy[key] != new_strategy[key]:
                changes.append(f"ä¿®æ”¹: {key}")
        
        return changes[:5]  # é™åˆ¶å˜åŒ–è®°å½•é•¿åº¦

class TrueAdversarialAdapter:
    """çœŸå®å¯¹æŠ—è®­ç»ƒé€‚é…å™¨ - ä¿®å¤ Session ç®¡ç†"""
    
    def __init__(self, llm_generator, config: Dict = None):
        self.true_adversarial = TrueAdversarialQualityModule(llm_generator, config)
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
    async def run_true_adversarial_training(self, real_samples: List[str], 
                                          synthetic_samples: List[str], 
                                          num_rounds: int = 3) -> Dict:
        """è¿è¡ŒçœŸå®å¯¹æŠ—è®­ç»ƒ - ä¿®å¤ç‰ˆæœ¬"""
        
        self.logger.info(f"ğŸš€ å¯åŠ¨çœŸå®å¯¹æŠ—è®­ç»ƒ - {num_rounds}è½®åšå¼ˆ")
        
        current_samples = synthetic_samples.copy()
        all_results = []
        
        # ä½¿ç”¨ async with ç¡®ä¿ session æ­£ç¡®ç®¡ç†
        async with self.true_adversarial.llm_generator:
            for round_idx in range(num_rounds):
                self.logger.info(f"âš”ï¸ ç¬¬{round_idx+1}è½®å¯¹æŠ—åšå¼ˆå¼€å§‹...")
                
                try:
                    round_result = await self.true_adversarial.run_true_adversarial_round(
                        real_samples, current_samples, round_idx
                    )
                    
                    if 'error' not in round_result:
                        # æ›´æ–°æ ·æœ¬ä¸ºæ”¹è¿›ç‰ˆæœ¬
                        if 'improved_samples' in round_result and round_result['improved_samples']:
                            current_samples = round_result['improved_samples']
                            self.logger.info(f"âœ… ç¬¬{round_idx+1}è½®å¯¹æŠ—å®Œæˆ - æ ·æœ¬è´¨é‡æå‡")
                        
                        all_results.append(round_result)
                    else:
                        self.logger.error(f"âŒ ç¬¬{round_idx+1}è½®å¯¹æŠ—å¤±è´¥: {round_result['error']}")
                        # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­å°è¯•ä¸‹ä¸€è½®
                        continue
                        
                except Exception as e:
                    self.logger.error(f"âŒ ç¬¬{round_idx+1}è½®å¼‚å¸¸: {e}")
                    continue
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        final_result = self._compute_final_adversarial_result(all_results, current_samples)
        
        self.logger.info(f"ğŸ çœŸå®å¯¹æŠ—è®­ç»ƒå®Œæˆ - æœ€ç»ˆè´¨é‡: {final_result.get('final_quality_score', 0):.3f}")
        
        return final_result
    
    def _compute_final_adversarial_result(self, all_results: List[Dict], 
                                        final_samples: List[str]) -> Dict:
        """è®¡ç®—æœ€ç»ˆå¯¹æŠ—ç»“æœ"""
        
        if not all_results:
            return {
                'final_samples': final_samples,
                'final_quality_score': 0.5,
                'total_rounds': 0,
                'avg_deception_rate': 0.3,
                'avg_quality_improvement': 0.1,
                'adversarial_evolution': 0,
                'adversarial_summary': 'å¯¹æŠ—è®­ç»ƒæœªå®Œæˆæˆ–å…¨éƒ¨å¤±è´¥'
            }
        
        # æå–å…³é”®æŒ‡æ ‡
        deception_rates = []
        quality_improvements = []
        
        for result in all_results:
            final_validation = result.get('final_validation', {})
            if 'deception_success_rate' in final_validation:
                deception_rates.append(final_validation['deception_success_rate'])
            if 'improvement_over_baseline' in final_validation:
                quality_improvements.append(final_validation['improvement_over_baseline'])
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡
        avg_deception_rate = np.mean(deception_rates) if deception_rates else 0.3
        avg_quality_improvement = np.mean(quality_improvements) if quality_improvements else 0.1
        
        final_quality_score = min(0.95, 0.5 + avg_deception_rate * 0.3 + avg_quality_improvement * 0.2)
        
        return {
            'final_samples': final_samples,
            'final_quality_score': final_quality_score,
            'total_rounds': len(all_results),
            'avg_deception_rate': avg_deception_rate,
            'avg_quality_improvement': avg_quality_improvement,
            'adversarial_evolution': len(self.true_adversarial.strategy_evolution),
            'adversarial_summary': f'å®Œæˆ{len(all_results)}è½®å¯¹æŠ—åšå¼ˆï¼Œå¹³å‡æ¬ºéª—ç‡{avg_deception_rate:.2f}',
            'detailed_results': all_results
        }

# ä¿®å¤çš„é€‚é…å™¨ç±»
class PromptTunerAdapter:
    """Promptè°ƒä¼˜é€‚é…å™¨ - ä¿®å¤ç‰ˆæœ¬"""
    
    def __init__(self, original_module, config):
        self.original_module = original_module
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åªæœ‰å½“åŸå§‹æ¨¡å—å­˜åœ¨æ—¶æ‰è®¾ç½®å±æ€§
        if original_module is not None and hasattr(original_module, '__dict__'):
            original_module.divergence_threshold = config.get('divergence_threshold', 0.1)
            original_module.adaptation_rate = config.get('adaptation_rate', 0.2)
            original_module.prompt_history = []
    
    def optimize_prompt(self, real_stats, synthetic_stats_history, iteration):
        """é€‚é…çš„Promptä¼˜åŒ–æ–¹æ³• - ä¿®å¤ç‰ˆæœ¬"""
        # å¦‚æœåŸå§‹æ¨¡å—ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨å›é€€å®ç°
        if self.original_module is None:
            return self._fallback_optimize_prompt(real_stats, synthetic_stats_history, iteration)
        
        try:
            # ç¡®ä¿å¿…è¦å±æ€§å­˜åœ¨
            if hasattr(self.original_module, 'optimize_prompt'):
                return self.original_module.optimize_prompt(
                    real_stats, synthetic_stats_history, iteration
                )
            else:
                # å›é€€å®ç°
                return self._fallback_optimize_prompt(
                    real_stats, synthetic_stats_history, iteration
                )
                
        except Exception as e:
            error_msg = str(e)
            if 'divergence_threshold' in error_msg or 'NoneType' in error_msg:
                self.logger.warning(f"Promptä¼˜åŒ–åŸå§‹æ¨¡å—å¤±è´¥ï¼Œä½¿ç”¨å›é€€å®ç°: {e}")
                return self._fallback_optimize_prompt(
                    real_stats, synthetic_stats_history, iteration
                )
            else:
                raise
    
    def _fallback_optimize_prompt(self, real_stats, synthetic_stats_history, iteration):
        """å›é€€çš„Promptä¼˜åŒ–å®ç° - å¢å¼ºç‰ˆæœ¬"""
        
        # åŸºç¡€promptç­–ç•¥
        base_prompts = [
            "ç”Ÿæˆé«˜è´¨é‡çš„ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®ï¼Œç¡®ä¿æ•°æ®çš„çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚",
            "åˆ›å»ºç¬¦åˆçœŸå®ç”¨æˆ·è¡Œä¸ºæ¨¡å¼çš„æ¨èæ•°æ®ï¼Œæ³¨é‡é•¿å°¾ç‰©å“çš„è¦†ç›–ã€‚",
            "æ„å»ºå¹³è¡¡çš„ç”¨æˆ·äº¤äº’è®°å½•ï¼Œå…¼é¡¾æµè¡Œç‰©å“å’Œå°ä¼—ç‰©å“ã€‚",
            "ç”Ÿæˆä½“ç°ç”¨æˆ·ä¸ªæ€§åŒ–åå¥½çš„äº¤äº’æ•°æ®ï¼Œå¢å¼ºæ•°æ®é›†çš„ä»£è¡¨æ€§ã€‚",
            "æ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„è§‚å½±è¡Œä¸ºï¼ŒåŒ…å«ä¸åŒç±»å‹å’Œå¹´ä»£çš„ç”µå½±åå¥½ã€‚"
        ]
        
        # åŸºäºè¿­ä»£é€‰æ‹©ä¸åŒçš„promptç­–ç•¥
        selected_prompt = base_prompts[iteration % len(base_prompts)]
        
        # åŸºäºçœŸå®æ•°æ®ç»Ÿè®¡è°ƒæ•´prompt
        user_stats = real_stats.get('user_stats', {})
        item_stats = real_stats.get('item_stats', {})
        
        mean_activity = user_stats.get('mean', 3)
        gini_coefficient = item_stats.get('gini', 0.5)
        long_tail_ratio = item_stats.get('long_tail_ratio', 0.3)
        
        # æ ¹æ®æ•°æ®ç‰¹å¾è°ƒæ•´prompt
        if mean_activity > 5:
            selected_prompt += " æ³¨é‡ç”Ÿæˆæ´»è·ƒç”¨æˆ·çš„å¤šæ ·åŒ–äº¤äº’ï¼Œæ¯ä¸ªç”¨æˆ·åº”æœ‰è¾ƒå¤šçš„ç”µå½±è§‚çœ‹è®°å½•ã€‚"
        elif mean_activity < 3:
            selected_prompt += " å…³æ³¨ä½æ´»è·ƒåº¦ç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼ï¼Œç”Ÿæˆç®€æ´ä½†æœ‰ä»£è¡¨æ€§çš„äº¤äº’ã€‚"
        else:
            selected_prompt += " ç”Ÿæˆä¸­ç­‰æ´»è·ƒåº¦ç”¨æˆ·çš„å¹³è¡¡äº¤äº’è®°å½•ã€‚"
        
        if gini_coefficient > 0.7:
            selected_prompt += " å¢åŠ å¯¹é•¿å°¾ç‰©å“çš„å…³æ³¨ï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒï¼ŒåŒ…å«æ›´å¤šå°ä¼—ç”µå½±ã€‚"
        elif gini_coefficient < 0.3:
            selected_prompt += " ä¿æŒç›¸å¯¹å‡åŒ€çš„ç‰©å“åˆ†å¸ƒï¼Œé¿å…è¿‡åº¦é›†ä¸­åœ¨çƒ­é—¨ç”µå½±ã€‚"
        
        # åŸºäºå†å²åˆæˆæ•°æ®è°ƒæ•´
        if synthetic_stats_history:
            recent_stats = synthetic_stats_history[-1]
            recent_user_mean = recent_stats.get('user_stats', {}).get('mean', mean_activity)
            
            if abs(recent_user_mean - mean_activity) > 1:
                if recent_user_mean > mean_activity:
                    selected_prompt += " é€‚å½“å‡å°‘ç”¨æˆ·äº¤äº’æ•°é‡ï¼Œæ›´è´´è¿‘çœŸå®æ•°æ®åˆ†å¸ƒã€‚"
                else:
                    selected_prompt += " é€‚å½“å¢åŠ ç”¨æˆ·äº¤äº’æ•°é‡ï¼Œæå‡æ•°æ®ä¸°å¯Œåº¦ã€‚"
        
        # æ ¹æ®è¿­ä»£è½®æ¬¡æ·»åŠ ç‰¹å®šæŒ‡å¯¼
        if iteration == 0:
            selected_prompt += " é¦–è½®ç”Ÿæˆï¼Œæ³¨é‡æ•°æ®çš„åŸºç¡€è´¨é‡å’Œæ ¼å¼æ­£ç¡®æ€§ã€‚"
        elif iteration == 1:
            selected_prompt += " åœ¨é¦–è½®åŸºç¡€ä¸Šï¼Œä¼˜åŒ–æ•°æ®å¤šæ ·æ€§å’Œç”¨æˆ·è¡Œä¸ºçš„çœŸå®æ€§ã€‚"
        else:
            selected_prompt += f" ç¬¬{iteration+1}è½®ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡æ•°æ®è´¨é‡å’Œåˆ†å¸ƒå¹³è¡¡æ€§ã€‚"
        
        self.logger.info(f"ğŸ¯ ä½¿ç”¨å†…ç½®Promptä¼˜åŒ–ç­–ç•¥ - ç¬¬{iteration}è½®")
        
        return {
            'optimized_prompt': selected_prompt,
            'optimization_info': {
                'iteration': iteration,
                'strategy': f'fallback_strategy_{iteration % len(base_prompts)}',
                'user_mean_activity': mean_activity,
                'item_gini': gini_coefficient,
                'long_tail_ratio': long_tail_ratio,
                'adjustments_made': [
                    'activity_level' if mean_activity != 3 else None,
                    'distribution_balance' if gini_coefficient > 0.7 or gini_coefficient < 0.3 else None,
                    'historical_adjustment' if synthetic_stats_history else None
                ],
                'prompt_length': len(selected_prompt)
            }
        }

class AdversarialQualityAdapter:
    """å¯¹æŠ—è´¨é‡æ¨¡å—é€‚é…å™¨ - æ”¯æŒçœŸå®å¯¹æŠ—è®­ç»ƒ"""
    
    def __init__(self, original_module, config):
        self.original_module = original_module
        self.config = config
        self.simulation_mode = config.get('simulation_mode', True)
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åªæœ‰å½“åŸå§‹æ¨¡å—å­˜åœ¨æ—¶æ‰è®¾ç½®å±æ€§
        if original_module is not None and hasattr(original_module, '__dict__'):
            original_module.simulation_mode = self.simulation_mode
            original_module.quality_threshold = config.get('quality_threshold', 0.7)
            original_module.use_real_llm = config.get('use_real_llm', False)
    
    def run_adversarial_round(self, real_samples, synthetic_samples, round_idx):
        """é€‚é…çš„å¯¹æŠ—è½®æ¬¡æ–¹æ³•"""
        # å¦‚æœåŸå§‹æ¨¡å—ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨å›é€€å®ç°
        if self.original_module is None:
            return self._fallback_adversarial_round(real_samples, synthetic_samples, round_idx)
        
        try:
            # ç¡®ä¿simulation_modeå±æ€§å­˜åœ¨
            if hasattr(self.original_module, 'run_adversarial_round'):
                if not hasattr(self.original_module, 'simulation_mode'):
                    self.original_module.simulation_mode = self.simulation_mode
                
                return self.original_module.run_adversarial_round(
                    real_samples, synthetic_samples, round_idx
                )
            else:
                return self._fallback_adversarial_round(
                    real_samples, synthetic_samples, round_idx
                )
                
        except Exception as e:
            error_msg = str(e)
            if 'simulation_mode' in error_msg or 'NoneType' in error_msg:
                self.logger.warning(f"å¯¹æŠ—è®­ç»ƒåŸå§‹æ¨¡å—å¤±è´¥ï¼Œä½¿ç”¨å›é€€å®ç°: {e}")
                return self._fallback_adversarial_round(
                    real_samples, synthetic_samples, round_idx
                )
            else:
                raise
    
    def _fallback_adversarial_round(self, real_samples, synthetic_samples, round_idx):
        """å›é€€çš„å¯¹æŠ—è½®æ¬¡å®ç° - å®Œæ•´ç‰ˆæœ¬"""
        self.logger.info(f"ğŸ”§ ä½¿ç”¨å†…ç½®å¯¹æŠ—è´¨é‡ä¿è¯ - ç¬¬{round_idx}è½®")
        
        filtered_samples = []
        quality_threshold = self.config.get('quality_threshold', 0.7)
        
        # åˆ†æçœŸå®æ ·æœ¬çš„ç‰¹å¾
        real_user_activities = []
        real_item_frequencies = {}
        real_user_ids = set()
        real_item_ids = set()
        
        for sample in real_samples[:200]:  
            try:
                parts = sample.strip().split()
                if len(parts) >= 2:
                    user_id = int(parts[0])
                    items = [int(x) for x in parts[1:]]
                    real_user_activities.append(len(items))
                    real_user_ids.add(user_id)
                    
                    for item in items:
                        real_item_frequencies[item] = real_item_frequencies.get(item, 0) + 1
                        real_item_ids.add(item)
            except:
                continue
        
        avg_real_activity = np.mean(real_user_activities) if real_user_activities else 3
        max_real_user_id = max(real_user_ids) if real_user_ids else 1000
        max_real_item_id = max(real_item_ids) if real_item_ids else 500
        
        # è¿‡æ»¤åˆæˆæ ·æœ¬
        quality_scores = []
        for sample in synthetic_samples:
            try:
                parts = sample.strip().split()
                if len(parts) >= 2:
                    user_id = int(parts[0])
                    items = [int(x) for x in parts[1:]]
                    
                    # åŸºæœ¬æ ¼å¼æ£€æŸ¥
                    if not (len(items) >= 1 and len(items) <= 15):
                        continue
                    
                    if not (user_id >= 0 and all(item >= 0 for item in items)):
                        continue
                    
                    # IDèŒƒå›´åˆç†æ€§æ£€æŸ¥
                    if user_id > max_real_user_id * 2:  # å…è®¸ä¸€å®šæ‰©å±•
                        continue
                    
                    if any(item > max_real_item_id * 2 for item in items):  # å…è®¸ä¸€å®šæ‰©å±•
                        continue
                    
                    # è®¡ç®—è´¨é‡åˆ†æ•°
                    # 1. å¤šæ ·æ€§åˆ†æ•°
                    diversity_score = len(set(items)) / len(items) if items else 0
                    
                    # 2. æ´»è·ƒåº¦ç›¸ä¼¼æ€§
                    activity_diff = abs(len(items) - avg_real_activity)
                    activity_similarity = max(0, 1 - activity_diff / max(avg_real_activity, 1))
                    
                    # 3. ç‰©å“åˆ†å¸ƒåˆç†æ€§
                    item_distribution_score = 0.5
                    if real_item_frequencies:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸€äº›å¸¸è§ç‰©å“
                        common_items = [item for item, freq in real_item_frequencies.items() if freq > 1]
                        if common_items:
                            common_item_overlap = len([item for item in items if item in common_items])
                            item_distribution_score = min(common_item_overlap / len(items), 0.8)
                    
                    # ç»¼åˆè´¨é‡åˆ†æ•°
                    quality_score = (
                        diversity_score * 0.4 + 
                        activity_similarity * 0.4 + 
                        item_distribution_score * 0.2
                    )
                    
                    quality_scores.append((quality_score, sample))
                        
            except Exception as e:
                continue
        
        # æ ¹æ®è´¨é‡åˆ†æ•°å’Œè½®æ¬¡åŠ¨æ€è°ƒæ•´é˜ˆå€¼
        base_threshold = quality_threshold * (0.7 + round_idx * 0.1)
        
        # é€‰æ‹©é«˜è´¨é‡æ ·æœ¬
        high_quality_samples = [sample for score, sample in quality_scores if score >= base_threshold]
        
        # å¦‚æœé«˜è´¨é‡æ ·æœ¬å¤ªå°‘ï¼Œé€‰æ‹©æœ€å¥½çš„ä¸€éƒ¨åˆ†
        if len(high_quality_samples) < len(synthetic_samples) * 0.2:
            quality_scores.sort(reverse=True, key=lambda x: x[0])
            target_count = max(len(synthetic_samples) // 3, 1)
            filtered_samples = [sample for _, sample in quality_scores[:target_count]]
        else:
            filtered_samples = high_quality_samples
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
        if not filtered_samples and synthetic_samples:
            # éšæœºé€‰æ‹©ä¸€äº›åŸºæœ¬åˆæ³•çš„æ ·æœ¬
            for sample in synthetic_samples:
                try:
                    parts = sample.strip().split()
                    if len(parts) >= 2:
                        user_id = int(parts[0])
                        items = [int(x) for x in parts[1:]]
                        if (user_id >= 0 and len(items) >= 1 and len(items) <= 10 and
                            all(item >= 0 for item in items)):
                            filtered_samples.append(sample)
                            if len(filtered_samples) >= max(1, len(synthetic_samples) // 4):
                                break
                except:
                    continue
        
        # è®¡ç®—æœ€ç»ˆè´¨é‡åˆ†æ•°
        if quality_scores:
            avg_quality = np.mean([score for score, _ in quality_scores])
        else:
            avg_quality = 0.5
        
        final_quality = min(0.5 + round_idx * 0.1 + avg_quality * 0.3, 1.0)
        
        self.logger.info(f"âœ… å†…ç½®è´¨é‡ä¿è¯å®Œæˆ - ç¬¬{round_idx}è½®: {len(filtered_samples)}/{len(synthetic_samples)} æ ·æœ¬é€šè¿‡")
        
        return {
            'filtered_samples': filtered_samples,
            'quality_score': final_quality,
            'round_info': f'å†…ç½®è´¨é‡ä¿è¯è½®æ¬¡{round_idx}',
            'filter_ratio': len(filtered_samples) / len(synthetic_samples) if synthetic_samples else 0,
            'quality_details': {
                'avg_quality': avg_quality,
                'threshold_used': base_threshold,
                'samples_analyzed': len(quality_scores)
            }
        }

class EnhancedDatasetRecommendationFramework:
    """åŸºäºå¢å¼ºæ•°æ®é›†çš„LLMæ¨èæŠ€æœ¯åˆ›æ–°æ¡†æ¶ - é›†æˆçœŸå®å¯¹æŠ—è®­ç»ƒ"""
    
    def __init__(self, original_dataset, enhanced_dataset, config: Dict = None):
        self.original_dataset = original_dataset
        self.enhanced_dataset = enhanced_dataset
        self.config = self._get_default_config(config)
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # æ£€æŸ¥APIé…ç½®
        api_key = self.config.get('deepseek_api_key', '')
        if (api_key and api_key.strip() and api_key.startswith('sk-') and len(api_key.strip()) > 30):
            self.use_real_llm = True
            self.logger.info("âœ… å·²å¯ç”¨DeepSeekçœŸå®LLMæ¨¡å¼")
        else:
            self.use_real_llm = False
            self.logger.warning("âš ï¸ æœªé…ç½®æœ‰æ•ˆçš„DeepSeek API Keyï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        
        self._initialize_modules()
        
        self.real_stats = None
        self.synthetic_stats_history = []
    
    def _get_default_config(self, config):
        """è·å–é»˜è®¤é…ç½®"""
        default = {
            'max_iterations': 4,
            'samples_per_iteration': 30,
            'adversarial_rounds': 3,
            'quality_threshold': 0.7,
            'convergence_tolerance': 0.05,
            'early_stopping': True,
            'min_improvement_threshold': 0.02,
            'max_no_improvement_iterations': 2,
            'deepseek_api_key': DEEPSEEK_CONFIG['api_key'],
            'deepseek_model': DEEPSEEK_CONFIG['model'],
            'divergence_threshold': 0.1,
            'adaptation_rate': 0.2,
            'simulation_mode': True,
            'enable_true_adversarial': True
        }
        if config:
            default.update(config)
        return default
    
    def _initialize_modules(self):
        """åˆå§‹åŒ–æ¨¡å— - é›†æˆçœŸå®å¯¹æŠ—è®­ç»ƒ"""
        try:
            # ä½¿ç”¨å¢å¼ºæ•°æ®é›†åˆå§‹åŒ–åˆ†æå™¨
            self.data_analyzer = MODULES['DataDistributionAnalyzer'](self.enhanced_dataset)
            self.evaluator = MODULES['InnovativeEvaluationMetrics']()
            
            if self.use_real_llm:
                deepseek_config = {
                    'api_key': self.config['deepseek_api_key'],
                    'model': self.config['deepseek_model'],
                    'quality_threshold': self.config['quality_threshold']
                }
                self.llm_generator = DeepSeekLLMGenerator(deepseek_config)
                self.logger.info("ğŸ¤– DeepSeek APIç”Ÿæˆå™¨å·²å°±ç»ª")
                
                # åˆå§‹åŒ–çœŸå®å¯¹æŠ—è®­ç»ƒæ¨¡å—
                if self.config.get('enable_true_adversarial', True):
                    adversarial_config = {
                        'quality_threshold': self.config['quality_threshold'],
                        'max_rounds': self.config['adversarial_rounds']
                    }
                    self.true_adversarial_adapter = TrueAdversarialAdapter(
                        self.llm_generator, adversarial_config
                    )
                    self.logger.info("âš”ï¸ çœŸå®å¯¹æŠ—è®­ç»ƒæ¨¡å—å·²å¯ç”¨")
                else:
                    self.true_adversarial_adapter = None
            else:
                self.llm_generator = None
                self.true_adversarial_adapter = None
                self.logger.info("ğŸ”§ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            
            # ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–Promptè°ƒä¼˜æ¨¡å—
            prompt_config = {
                'max_iterations': self.config['max_iterations'],
                'convergence_tolerance': self.config['convergence_tolerance'],
                'divergence_threshold': self.config['divergence_threshold'],
                'adaptation_rate': self.config['adaptation_rate']
            }
            
            original_prompt_tuner = None
            try:
                original_prompt_tuner = MODULES['DynamicPromptTuner'](prompt_config)
                self.logger.info("âœ… åŸå§‹Promptè°ƒä¼˜æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŸå§‹Promptè°ƒä¼˜æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            
            self.prompt_tuner = PromptTunerAdapter(original_prompt_tuner, prompt_config)
            
            # ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–å¯¹æŠ—è´¨é‡æ¨¡å—
            adversarial_config = {
                'quality_threshold': self.config['quality_threshold'],
                'max_rounds': self.config['adversarial_rounds'],
                'simulation_mode': not self.use_real_llm,
                'use_real_llm': self.use_real_llm
            }
            
            original_adversarial = None
            try:
                original_adversarial = MODULES['AdversarialQualityAssurance'](adversarial_config)
                self.logger.info("âœ… åŸå§‹å¯¹æŠ—æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŸå§‹å¯¹æŠ—æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            
            self.adversarial_module = AdversarialQualityAdapter(original_adversarial, adversarial_config)
            
            self.logger.info("âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—(é€‚é…å™¨ç‰ˆæœ¬)åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _run_adversarial_training_adapter(self, real_samples, synthetic_samples, num_rounds):
        """å¯¹æŠ—è®­ç»ƒé€‚é…å™¨æ–¹æ³• - æ”¯æŒçœŸå®å¯¹æŠ—è®­ç»ƒ"""
        try:
            # å¦‚æœå¯ç”¨çœŸå®å¯¹æŠ—è®­ç»ƒä¸”æœ‰çœŸå®LLMï¼Œåˆ™ä½¿ç”¨çœŸå®å¯¹æŠ—æ¨¡å—
            if (self.true_adversarial_adapter and self.use_real_llm and 
                self.config.get('enable_true_adversarial', True)):
                
                print(f"   âš”ï¸ å¯åŠ¨çœŸå®å¯¹æŠ—åšå¼ˆè®­ç»ƒ...")
                
                # è¿è¡Œå¼‚æ­¥çœŸå®å¯¹æŠ—è®­ç»ƒ
                try:
                    final_result = asyncio.run(
                        self.true_adversarial_adapter.run_true_adversarial_training(
                            real_samples, synthetic_samples, num_rounds
                        )
                    )
                    
                    print(f"   ğŸ¯ çœŸå®å¯¹æŠ—åšå¼ˆå®Œæˆ - æœ€ç»ˆè´¨é‡: {final_result.get('final_quality_score', 0):.3f}")
                    print(f"   ğŸ“Š å¯¹æŠ—ç»Ÿè®¡: {final_result.get('adversarial_summary', '')}")
                    
                    return {
                        'final_filtered_samples': final_result.get('final_samples', synthetic_samples),
                        'quality_summary': {
                            'avg_quality': final_result.get('final_quality_score', 0.5),
                            'rounds_completed': final_result.get('total_rounds', 0),
                            'final_sample_count': len(final_result.get('final_samples', [])),
                            'deception_rate': final_result.get('avg_deception_rate', 0),
                            'adversarial_evolution': final_result.get('adversarial_evolution', 0)
                        },
                        'true_adversarial_used': True,
                        'adversarial_details': final_result
                    }
                    
                except Exception as e:
                    print(f"   âŒ çœŸå®å¯¹æŠ—è®­ç»ƒå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†å¯¹æŠ—è®­ç»ƒ: {e}")
                    # å›é€€åˆ°æ ‡å‡†å¯¹æŠ—è®­ç»ƒ
                    return self._run_standard_adversarial_training(real_samples, synthetic_samples, num_rounds)
            else:
                # ä½¿ç”¨æ ‡å‡†å¯¹æŠ—è®­ç»ƒ
                return self._run_standard_adversarial_training(real_samples, synthetic_samples, num_rounds)
                
        except Exception as e:
            print(f"   âŒ å¯¹æŠ—è®­ç»ƒé€‚é…å™¨å¤±è´¥: {e}")
            return {
                'final_filtered_samples': synthetic_samples,
                'quality_summary': {
                    'avg_quality': 0.5,
                    'rounds_completed': 0,
                    'final_sample_count': len(synthetic_samples)
                }
            }
    
    def _run_standard_adversarial_training(self, real_samples, synthetic_samples, num_rounds):
        """æ ‡å‡†å¯¹æŠ—è®­ç»ƒå®ç°"""
        filtered_samples = synthetic_samples.copy()
        quality_scores = []
        
        print(f"   ğŸ›¡ï¸ å¼€å§‹æ ‡å‡†å¯¹æŠ—è´¨é‡ä¿è¯ ({num_rounds}è½®)...")
        
        for round_idx in range(num_rounds):
            try:
                round_result = self.adversarial_module.run_adversarial_round(
                    real_samples, filtered_samples, round_idx
                )
                
                if 'filtered_samples' in round_result and round_result['filtered_samples']:
                    filtered_samples = round_result['filtered_samples']
                    
                    if 'quality_score' in round_result:
                        quality_scores.append(round_result['quality_score'])
                        print(f"     è½®æ¬¡{round_idx+1}: è´¨é‡åˆ†æ•°={round_result['quality_score']:.3f}, "
                              f"å‰©ä½™æ ·æœ¬={len(filtered_samples)}")
                    else:
                        quality_scores.append(0.6 + round_idx * 0.1)
                        print(f"     è½®æ¬¡{round_idx+1}: å®Œæˆè¿‡æ»¤ï¼Œå‰©ä½™æ ·æœ¬={len(filtered_samples)}")
                else:
                    print(f"     è½®æ¬¡{round_idx+1}: æ— æœ‰æ•ˆæ ·æœ¬")
                    break
                    
            except Exception as e:
                print(f"     è½®æ¬¡{round_idx+1}å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ²¡æœ‰è¿‡æ»¤åçš„æ ·æœ¬ï¼Œä½¿ç”¨åŸå§‹æ ·æœ¬
        if not filtered_samples:
            print("   âš ï¸ å¯¹æŠ—è®­ç»ƒæœªè¿”å›æœ‰æ•ˆæ ·æœ¬ï¼Œä½¿ç”¨åŸå§‹ç”Ÿæˆæ ·æœ¬")
            filtered_samples = synthetic_samples
        
        return {
            'final_filtered_samples': filtered_samples,
            'quality_summary': {
                'avg_quality': np.mean(quality_scores) if quality_scores else 0.5,
                'rounds_completed': len(quality_scores),
                'final_sample_count': len(filtered_samples)
            },
            'true_adversarial_used': False
        }
    
    def run_framework(self) -> Dict:
        """è¿è¡Œå®Œæ•´æ¡†æ¶ - é›†æˆçœŸå®å¯¹æŠ—è®­ç»ƒ"""
        start_time = time.time()
        
        # ç¡®å®šè¿è¡Œæ¨¡å¼
        if self.use_real_llm and self.config.get('enable_true_adversarial', True):
            mode = "å¢å¼ºæ•°æ®é›†+DeepSeek LLM+çœŸå®å¯¹æŠ—è®­ç»ƒ"
        elif self.use_real_llm:
            mode = "å¢å¼ºæ•°æ®é›†+DeepSeek LLM"
        else:
            mode = "å¢å¼ºæ•°æ®é›†+æ ¸å¿ƒæ¨¡å—"
            
        self.logger.info(f"ğŸš€ å¯åŠ¨{mode}æ¨èæŠ€æœ¯åˆ›æ–°æ¡†æ¶")
        
        try:
            print(f"ğŸ“Š [{mode}æ¨¡å¼] åˆ†æå¢å¼ºæ•°æ®é›†åˆ†å¸ƒ...")
            self.real_stats = self.data_analyzer.generate_feature_vector()
            real_samples = self._extract_real_samples()
            
            # è·å–æ•°æ®é›†å…ƒæ•°æ®
            dataset_metadata = self.original_dataset.get_item_metadata()
            dataset_metadata['max_user_id'] = self.enhanced_dataset.n_users - 1
            
            print(f"ğŸ“ˆ å¢å¼ºæ•°æ®é›†ç‰¹å¾: ç”¨æˆ·æ´»è·ƒåº¦å‡å€¼={self.real_stats.get('user_stats', {}).get('mean', 0):.2f}, "
                  f"ç‰©å“åŸºå°¼ç³»æ•°={self.real_stats.get('item_stats', {}).get('gini', 0):.3f}")
            
            print(f"ğŸ“¦ æ•°æ®é›†è§„æ¨¡: {self.enhanced_dataset.n_users}ç”¨æˆ·, "
                  f"{self.enhanced_dataset.n_items}ç‰©å“, "
                  f"{self.enhanced_dataset.n_train}äº¤äº’")
            
            # æ‰“å°å¢å¼ºç»Ÿè®¡
            enhancement_stats = getattr(self.enhanced_dataset, 'enhancement_stats', {})
            if enhancement_stats:
                print(f"ğŸ”§ å¢å¼ºæ•ˆæœ: åŸå§‹{enhancement_stats.get('original_users', 0)}ç”¨æˆ· â†’ "
                      f"å¢å¼º{enhancement_stats.get('enhanced_users', 0)}ç”¨æˆ· "
                      f"(+{enhancement_stats.get('enhancement_ratio', 0):.1%})")
            
            # çœŸå®å¯¹æŠ—è®­ç»ƒä¿¡æ¯
            if self.true_adversarial_adapter:
                print(f"âš”ï¸ çœŸå®å¯¹æŠ—è®­ç»ƒ: å·²å¯ç”¨ï¼Œæ”¯æŒåŒLLMåšå¼ˆ")
            else:
                print(f"ğŸ”§ å¯¹æŠ—è®­ç»ƒ: ä½¿ç”¨æ ‡å‡†æ¨¡å¼")
            
            results = {
                'dataset_info': {
                    'original_dataset': {
                        'users': len(self.original_dataset.train_items),
                        'items': self.original_dataset.n_items,
                        'interactions': self.original_dataset.n_train
                    },
                    'enhanced_dataset': {
                        'users': self.enhanced_dataset.n_users,
                        'items': self.enhanced_dataset.n_items,
                        'interactions': self.enhanced_dataset.n_train
                    },
                    'enhancement_stats': enhancement_stats,
                    'dataset_metadata': dataset_metadata
                },
                'real_data_analysis': self.real_stats,
                'iterations': [],
                'convergence_history': [],
                'quality_history': [],
                'config': self.config.copy(),
                'llm_mode': mode,
                'true_adversarial_enabled': bool(self.true_adversarial_adapter)
            }
            
            best_score = 0
            no_improvement = 0
            
            print(f"\nğŸ”„ å¼€å§‹{mode}è¿­ä»£ä¼˜åŒ– (æœ€å¤š{self.config['max_iterations']}è½®)...")
            
            for iteration in range(self.config['max_iterations']):
                print(f"\n--- ç¬¬{iteration + 1}è½®è¿­ä»£ [{mode}] ---")
                
                iter_result = self._run_iteration(iteration, real_samples, dataset_metadata)
                if 'error' in iter_result:
                    print(f"âš ï¸ ç¬¬{iteration + 1}è½®å‡ºç°é”™è¯¯: {iter_result['error']}")
                    continue
                
                results['iterations'].append(iter_result)
                results['convergence_history'].append(iter_result['convergence_score'])
                results['quality_history'].append(iter_result['quality_metrics']['avg_quality'])
                
                print(f"   ğŸ“ˆ æ”¶æ•›åˆ†æ•°: {iter_result['convergence_score']:.4f}")
                print(f"   ğŸ¯ è´¨é‡åˆ†æ•°: {iter_result['quality_metrics']['avg_quality']:.3f}")
                print(f"   ğŸ“¦ ç”Ÿæˆæ ·æœ¬: {iter_result['generated_count']} â†’ è¿‡æ»¤å: {iter_result['filtered_count']}")
                
                # æ˜¾ç¤ºå¯¹æŠ—è®­ç»ƒä¿¡æ¯
                if iter_result.get('true_adversarial_used', False):
                    deception_rate = iter_result.get('quality_metrics', {}).get('deception_rate', 0)
                    evolution = iter_result.get('quality_metrics', {}).get('adversarial_evolution', 0)
                    print(f"   âš”ï¸ å¯¹æŠ—åšå¼ˆ: æ¬ºéª—ç‡={deception_rate:.3f}, ç­–ç•¥è¿›åŒ–={evolution}ä»£")
                
                current_score = iter_result['convergence_score']
                if current_score > best_score + self.config['min_improvement_threshold']:
                    best_score = current_score
                    no_improvement = 0
                    results['best_samples'] = iter_result['filtered_samples']
                    print(f"   âœ¨ å‘ç°æ”¹è¿›! æœ€ä½³åˆ†æ•°: {best_score:.4f}")
                else:
                    no_improvement += 1
                    print(f"   ğŸ˜ æ— æ˜æ˜¾æ”¹è¿› ({no_improvement}/{self.config['max_no_improvement_iterations']})")
                
                if (self.config['early_stopping'] and 
                    no_improvement >= self.config['max_no_improvement_iterations']):
                    print("ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶")
                    break
            
            execution_time = time.time() - start_time
            results['final_metrics'] = self._calculate_final_metrics(results, execution_time)
            results['comprehensive_evaluation'] = self.evaluator.calculate_comprehensive_metrics(results)
            
            print(f"\nâœ… {mode}æ¡†æ¶æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ {mode}æ¡†æ¶æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}
    
    def _run_iteration(self, iteration: int, real_samples: List[str], dataset_metadata: Dict) -> Dict:
        """è¿è¡Œå•æ¬¡è¿­ä»£ - æ”¯æŒçœŸå®å¯¹æŠ—è®­ç»ƒ"""
        try:
            # ä½¿ç”¨é€‚é…å™¨ç‰ˆæœ¬çš„Promptè°ƒä¼˜
            prompt_result = self.prompt_tuner.optimize_prompt(
                real_stats=self.real_stats,
                synthetic_stats_history=self.synthetic_stats_history,
                iteration=iteration
            )
            
            # ç”Ÿæˆåˆæˆæ ·æœ¬
            if self.use_real_llm and self.llm_generator:
                full_prompt = self.llm_generator._build_recommendation_prompt(
                    prompt_result['optimized_prompt'],
                    self.real_stats,
                    dataset_metadata,
                    iteration
                )
                synthetic_samples = self.llm_generator.generate_samples_sync(
                    full_prompt, self.config['samples_per_iteration']
                )
            else:
                # ä½¿ç”¨åå¤‡æ ·æœ¬ç”Ÿæˆæ–¹æ¡ˆ
                synthetic_samples = self._generate_samples_fallback(self.config['samples_per_iteration'])
            
            if not synthetic_samples:
                return {'error': 'æ— æ³•ç”Ÿæˆæ ·æœ¬'}
            
            # ä½¿ç”¨å¯¹æŠ—è®­ç»ƒé€‚é…å™¨ï¼ˆæ”¯æŒçœŸå®å¯¹æŠ—è®­ç»ƒï¼‰
            adversarial_result = self._run_adversarial_training_adapter(
                real_samples, synthetic_samples, self.config['adversarial_rounds']
            )
            
            filtered_samples = adversarial_result['final_filtered_samples']
            if not filtered_samples:
                return {'error': 'è´¨é‡ä¿è¯åæ— å‰©ä½™æ ·æœ¬'}
            
            # åˆ†æåˆæˆæ•°æ®åˆ†å¸ƒ
            synthetic_stats = self._analyze_synthetic_distribution(filtered_samples)
            self.synthetic_stats_history.append(synthetic_stats)
            
            # è®¡ç®—æ”¶æ•›æŒ‡æ ‡
            convergence_metrics = self.evaluator.calculate_convergence_metrics(
                self.real_stats, synthetic_stats, iteration
            )
            
            return {
                'iteration': iteration + 1,
                'prompt_result': prompt_result,
                'generated_count': len(synthetic_samples),
                'filtered_samples': filtered_samples,
                'filtered_count': len(filtered_samples),
                'convergence_score': convergence_metrics['overall_convergence_score'],
                'quality_metrics': adversarial_result['quality_summary'],
                'llm_used': self.use_real_llm,
                'true_adversarial_used': adversarial_result.get('true_adversarial_used', False),
                'adversarial_details': adversarial_result.get('adversarial_details', {})
            }
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return {'error': str(e)}
    
    def _generate_samples_fallback(self, num_samples: int) -> List[str]:
        """åå¤‡æ ·æœ¬ç”Ÿæˆæ–¹æ¡ˆ"""
        samples = []
        user_stats = self.real_stats.get('user_stats', {})
        item_stats = self.real_stats.get('item_stats', {})
        
        for _ in range(num_samples):
            try:
                mean_activity = user_stats.get('mean', 3)
                user_activity = max(1, int(np.random.exponential(mean_activity)))
                user_activity = min(user_activity, 10)
                
                user_id = np.random.randint(0, self.enhanced_dataset.n_users)
                items = set()
                long_tail_ratio = item_stats.get('long_tail_ratio', 0.3)
                
                for _ in range(user_activity):
                    if np.random.random() < long_tail_ratio:
                        item_id = np.random.randint(self.enhanced_dataset.n_items // 2, self.enhanced_dataset.n_items)
                    else:
                        item_id = np.random.randint(0, self.enhanced_dataset.n_items // 2)
                    items.add(item_id)
                
                if items:
                    sample = f"{user_id} " + " ".join(map(str, sorted(items)))
                    samples.append(sample)
            except:
                continue
        
        return samples
    
    def _extract_real_samples(self, max_samples: int = 100) -> List[str]:
        """ä»å¢å¼ºæ•°æ®é›†ä¸­æå–çœŸå®æ ·æœ¬"""
        samples = []
        for user_id, items in list(self.enhanced_dataset.train_items.items())[:max_samples]:
            if items:
                samples.append(f"{user_id} " + " ".join(map(str, items)))
        return samples
    
    def _analyze_synthetic_distribution(self, samples: List[str]) -> Dict:
        """åˆ†æåˆæˆæ•°æ®åˆ†å¸ƒ"""
        try:
            # åˆ›å»ºä¸´æ—¶æ•°æ®ç”Ÿæˆå™¨æ¥åˆ†æåˆæˆæ•°æ®
            temp_generator = type('TempGenerator', (), {
                'train_items': {},
                'test_items': {},
                'n_users': self.enhanced_dataset.n_users,
                'n_items': self.enhanced_dataset.n_items,
                'n_train': 0,
                'n_test': 0
            })()
            
            for sample in samples:
                try:
                    parts = sample.strip().split()
                    if len(parts) >= 2:
                        user_id = int(parts[0])
                        items = [int(x) for x in parts[1:]]
                        temp_generator.train_items[user_id] = items
                        temp_generator.n_train += len(items)
                except:
                    continue
            
            # ä½¿ç”¨DataDistributionAnalyzeråˆ†æåˆæˆæ•°æ®
            temp_analyzer = MODULES['DataDistributionAnalyzer'](temp_generator)
            return temp_analyzer.generate_feature_vector()
        except Exception as e:
            print(f"   âš ï¸ åˆæˆæ•°æ®åˆ†æå¤±è´¥: {e}")
            return {'feature_vector': [], 'user_stats': {}, 'item_stats': {}}
    
    def _calculate_final_metrics(self, results: Dict, execution_time: float) -> Dict:
        """è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        iterations = results['iterations']
        convergence_history = results['convergence_history']
        quality_history = results['quality_history']
        
        # ç»Ÿè®¡çœŸå®å¯¹æŠ—è®­ç»ƒä½¿ç”¨æƒ…å†µ
        true_adversarial_iterations = sum(1 for iter_result in iterations 
                                        if iter_result.get('true_adversarial_used', False))
        
        return {
            'total_execution_time': execution_time,
            'total_iterations': len(iterations),
            'best_convergence_score': max(convergence_history) if convergence_history else 0,
            'final_convergence_score': convergence_history[-1] if convergence_history else 0,
            'best_quality_score': max(quality_history) if quality_history else 0,
            'total_generated_samples': sum(iter_result['generated_count'] for iter_result in iterations),
            'total_filtered_samples': sum(iter_result['filtered_count'] for iter_result in iterations),
            'llm_mode': results['llm_mode'],
            'dataset_enhancement_ratio': results['dataset_info']['enhancement_stats'].get('enhancement_ratio', 0),
            'true_adversarial_iterations': true_adversarial_iterations,
            'true_adversarial_usage_rate': true_adversarial_iterations / len(iterations) if iterations else 0
        }

def run_enhanced_dataset_experiment(dataset_path: str, config: Dict = None) -> Dict:
    """è¿è¡ŒåŸºäºå¢å¼ºæ•°æ®é›†çš„å®éªŒ - æ”¯æŒçœŸå®å¯¹æŠ—è®­ç»ƒ"""
    experiment_name = f"Enhanced_{config.get('experiment_name', 'Netflix')}" if config else 'Enhanced_Netflix'
    logger = setup_logging(experiment_name)
    
    try:
        # 1. åŠ è½½å¤–éƒ¨æ•°æ®é›†
        print("ğŸ“‚ åŠ è½½å¤–éƒ¨æ•°æ®é›†...")
        dataset_loader = NetflixDatasetLoader(dataset_path)
        load_result = dataset_loader.load_and_preprocess()
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {load_result}")
        
        # 2. å¢å¼ºæ•°æ®é›†
        print("ğŸ”§ å¼€å§‹æ•°æ®é›†å¢å¼º...")
        enhancement_config = config.get('enhancement_config', {}) if config else {}
        enhancer = DatasetEnhancer(dataset_loader, enhancement_config)
        enhancement_result = enhancer.enhance_dataset()
        
        enhanced_dataset = enhancer.get_enhanced_dataset()
        print(f"âœ… æ•°æ®é›†å¢å¼ºå®Œæˆ: {enhancement_result}")
        
        # 3. è¿è¡Œæ¨èæ¡†æ¶
        print("ğŸš€ å¯åŠ¨æ¨èæŠ€æœ¯åˆ›æ–°æ¡†æ¶...")
        framework_config = config.get('framework_config', {}) if config else {}
        framework = EnhancedDatasetRecommendationFramework(
            dataset_loader, enhanced_dataset, framework_config
        )
        framework_results = framework.run_framework()
        
        if 'error' in framework_results:
            return framework_results
        
        # 4. æ•´åˆå®éªŒç»“æœ
        experiment_results = {
            'experiment_metadata': {
                'name': experiment_name,
                'timestamp': datetime.now().isoformat(),
                'dataset_path': dataset_path,
                'config': config or {},
                'llm_provider': 'DeepSeek',
                'adversarial_mode': 'True Adversarial' if framework_results.get('true_adversarial_enabled', False) else 'Standard'
            },
            'dataset_loading': load_result,
            'dataset_enhancement': enhancement_result,
            'framework_results': framework_results
        }
        
        save_results(experiment_results, experiment_name)
        print_enhanced_summary(experiment_results)
        
        return experiment_results
        
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºæ•°æ®é›†å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}

def save_results(results: Dict, experiment_name: str):
    """ä¿å­˜å®éªŒç»“æœ"""
    results_dir = os.path.join(project_root, 'data', 'results')
    os.makedirs(results_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_file = os.path.join(results_dir, f'{experiment_name}_{timestamp}.json')
    
    try:
        def make_serializable(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [make_serializable(item) for item in obj]
            return obj
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(make_serializable(results), f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {json_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def print_enhanced_summary(results: Dict):
    """æ‰“å°å¢å¼ºæ•°æ®é›†å®éªŒæ€»ç»“ - åŒ…å«å¯¹æŠ—è®­ç»ƒä¿¡æ¯"""
    print("\n" + "="*80)
    print("ğŸ‰ å¢å¼ºæ•°æ®é›†LLMæ¨èæŠ€æœ¯åˆ›æ–°å®éªŒå®Œæˆ!")
    print("="*80)
    
    metadata = results['experiment_metadata']
    dataset_loading = results['dataset_loading']
    dataset_enhancement = results['dataset_enhancement']
    framework_results = results['framework_results']
    final_metrics = framework_results.get('final_metrics', {})
    
    print(f"ğŸ“‚ æ•°æ®é›†: {metadata['dataset_path']}")
    print(f"ğŸ¤– LLMæä¾›å•†: {metadata.get('llm_provider', 'DeepSeek')}")
    print(f"ğŸ”§ è¿è¡Œæ¨¡å¼: {framework_results.get('llm_mode', 'æœªçŸ¥')}")
    print(f"âš”ï¸ å¯¹æŠ—æ¨¡å¼: {metadata.get('adversarial_mode', 'æ ‡å‡†')}")
    
    # æ•°æ®é›†ä¿¡æ¯
    dataset_info = framework_results.get('dataset_info', {})
    original_info = dataset_info.get('original_dataset', {})
    enhanced_info = dataset_info.get('enhanced_dataset', {})
    
    print(f"\nğŸ“Š æ•°æ®é›†è§„æ¨¡:")
    print(f"   åŸå§‹: {original_info.get('users', 0)}ç”¨æˆ·, {original_info.get('interactions', 0)}äº¤äº’")
    print(f"   å¢å¼º: {enhanced_info.get('users', 0)}ç”¨æˆ·, {enhanced_info.get('interactions', 0)}äº¤äº’")
    
    enhancement_ratio = final_metrics.get('dataset_enhancement_ratio', 0)
    print(f"   å¢å¼ºæ¯”ä¾‹: +{enhancement_ratio:.1%}")
    
    # å®éªŒç»“æœ
    print(f"\nğŸ”„ å®éªŒæ‰§è¡Œ:")
    print(f"   å®Œæˆè¿­ä»£: {final_metrics.get('total_iterations', 0)}")
    print(f"   æœ€ä½³æ”¶æ•›: {final_metrics.get('best_convergence_score', 0):.4f}")
    print(f"   æœ€ä½³è´¨é‡: {final_metrics.get('best_quality_score', 0):.3f}")
    print(f"   æ‰§è¡Œæ—¶é—´: {final_metrics.get('total_execution_time', 0):.2f}ç§’")
    
    # å¯¹æŠ—è®­ç»ƒç»Ÿè®¡
    true_adversarial_iterations = final_metrics.get('true_adversarial_iterations', 0)
    usage_rate = final_metrics.get('true_adversarial_usage_rate', 0)
    if true_adversarial_iterations > 0:
        print(f"\nâš”ï¸ å¯¹æŠ—è®­ç»ƒç»Ÿè®¡:")
        print(f"   çœŸå®å¯¹æŠ—è½®æ¬¡: {true_adversarial_iterations}")
        print(f"   ä½¿ç”¨ç‡: {usage_rate:.1%}")
    
    # ç»¼åˆè¯„ä¼°
    comprehensive_eval = framework_results.get('comprehensive_evaluation', {})
    if comprehensive_eval:
        innovation_score = comprehensive_eval.get('overall_innovation_score', 0)
        if isinstance(innovation_score, (int, float)) and not np.isnan(innovation_score):
            print(f"\nğŸ’¯ åˆ›æ–°è¯„åˆ†: {innovation_score:.3f}")
            
            if innovation_score >= 0.8:
                grade = "ğŸŒŸ ä¼˜ç§€"
            elif innovation_score >= 0.6:
                grade = "ğŸ‘ è‰¯å¥½"  
            elif innovation_score >= 0.4:
                grade = "ğŸ˜ ä¸€èˆ¬"
            else:
                grade = "ğŸ˜ éœ€è¦æ”¹è¿›"
            
            print(f"ğŸ… è¯„ä¼°ç­‰çº§: {grade}")
            
            # å¦‚æœä½¿ç”¨äº†çœŸå®å¯¹æŠ—è®­ç»ƒï¼Œæ˜¾ç¤ºé¢å¤–ä¿¡æ¯
            if metadata.get('adversarial_mode') == 'True Adversarial':
                print(f"ğŸ¯ å¯¹æŠ—åŠ æˆ: å·²å¯ç”¨åŒLLMåšå¼ˆæœºåˆ¶")
    
    print("="*80)

def main_enhanced_experiment():
    """å¢å¼ºæ•°æ®é›†å®éªŒä¸»å‡½æ•° - æ”¯æŒçœŸå®å¯¹æŠ—è®­ç»ƒ"""
    print("ğŸ¤– å¢å¼ºæ•°æ®é›†LLMæ¨èæŠ€æœ¯åˆ›æ–°æ¡†æ¶ [å¯¹æŠ—ç”Ÿæˆç‰ˆ]")
    print("=" * 60)
    
    # è·å–æ•°æ®é›†è·¯å¾„
    default_path = os.path.join(project_root, 'data', 'netflix', 'item_attribute.csv')
    dataset_path = input(f"è¯·è¾“å…¥æ•°æ®é›†è·¯å¾„ (é»˜è®¤: {default_path}): ").strip()
    if not dataset_path:
        dataset_path = default_path
    
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        return
    
    # è·å–API Key
    api_key = input("è¯·è¾“å…¥DeepSeek API Key (ç•™ç©ºä½¿ç”¨æ ¸å¿ƒæ¨¡å—æ¨¡å¼): ").strip()
    use_real_api = False
    
    if api_key and api_key.startswith('sk-') and len(api_key) > 30:
        DEEPSEEK_CONFIG['api_key'] = api_key
        print("âœ… å·²é…ç½®DeepSeek APIï¼Œå°†ä½¿ç”¨çœŸå®LLMæ¨¡å¼")
        use_real_api = True
    else:
        print("âš ï¸ æœªé…ç½®æœ‰æ•ˆAPI Keyï¼Œå°†ä½¿ç”¨æ ¸å¿ƒæ¨¡å—æ¨¡å¼")
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    os.makedirs(os.path.join(project_root, 'data', 'results'), exist_ok=True)
    os.makedirs(os.path.join(project_root, 'logs'), exist_ok=True)
    
    # é…ç½®é€‰é¡¹
    configs = {
        "1": {
            'experiment_name': 'Netflix_Adversarial_Standard',
            'enhancement_config': {
                'augmentation_ratio': 0.3,
                'diversity_boost': True,
                'long_tail_emphasis': True,
                'synthetic_users': 0.2
            },
            'framework_config': {
                'max_iterations': 3,
                'samples_per_iteration': 25,
                'adversarial_rounds': 3,
                'enable_true_adversarial': use_real_api,
                'deepseek_api_key': api_key if use_real_api else ''
            }
        },
        "2": {
            'experiment_name': 'Netflix_Adversarial_Quick',
            'enhancement_config': {
                'augmentation_ratio': 0.2,
                'diversity_boost': True,
                'synthetic_users': 0.1
            },
            'framework_config': {
                'max_iterations': 2,
                'samples_per_iteration': 15,
                'adversarial_rounds': 2,
                'enable_true_adversarial': use_real_api,
                'deepseek_api_key': api_key if use_real_api else ''
            }
        },
        "3": {
            'experiment_name': 'Netflix_Adversarial_Advanced',
            'enhancement_config': {
                'augmentation_ratio': 0.4,
                'diversity_boost': True,
                'long_tail_emphasis': True,
                'synthetic_users': 0.3,
                'noise_level': 0.15
            },
            'framework_config': {
                'max_iterations': 4,
                'samples_per_iteration': 35,
                'adversarial_rounds': 4,
                'enable_true_adversarial': use_real_api,
                'deepseek_api_key': api_key if use_real_api else ''
            }
        }
    }
    
    print("é€‰æ‹©å®éªŒé…ç½®:")
    print("1. æ ‡å‡†å¯¹æŠ—é…ç½® (æ¨è)")
    print("2. å¿«é€Ÿå¯¹æŠ—éªŒè¯")
    print("3. é«˜çº§å¯¹æŠ—é…ç½®")
    
    try:
        choice = input("è¯·é€‰æ‹© [1-3] (é»˜è®¤1): ").strip() or "1"
        config = configs.get(choice, configs["1"])
        
        print(f"\nğŸš€ å¯åŠ¨ {config['experiment_name']} å®éªŒ...")
        print(f"ğŸ“‚ æ•°æ®é›†: {dataset_path}")
        print(f"ğŸ”§ å¢å¼ºæ¯”ä¾‹: {config['enhancement_config'].get('augmentation_ratio', 0):.1%}")
        print(f"ğŸ”„ è¿­ä»£è½®æ•°: {config['framework_config']['max_iterations']}")
        print(f"âš”ï¸ å¯¹æŠ—è½®æ•°: {config['framework_config']['adversarial_rounds']}")
        
        if use_real_api and config['framework_config'].get('enable_true_adversarial', False):
            print("ğŸ¤– ä½¿ç”¨çœŸå®DeepSeek API + åŒLLMå¯¹æŠ—åšå¼ˆ")
        elif use_real_api:
            print("ğŸ¤– ä½¿ç”¨çœŸå®DeepSeek API + æ ‡å‡†å¯¹æŠ—è®­ç»ƒ")
        else:
            print("ğŸ”§ ä½¿ç”¨æ ¸å¿ƒæ¨¡å— + æ ‡å‡†å¯¹æŠ—è®­ç»ƒ")
        
        results = run_enhanced_dataset_experiment(dataset_path, config)
        
        if 'error' in results:
            print(f"\nâŒ å®éªŒå¤±è´¥: {results['error']}")
        else:
            print("\nâœ… å¢å¼ºæ•°æ®é›†å¯¹æŠ—ç”Ÿæˆå®éªŒæˆåŠŸå®Œæˆï¼")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å®éªŒè¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main_enhanced_experiment()
