import pandas as pd
import numpy as np
import os
from typing import Dict, List, Tuple, Any
import logging
from tqdm import tqdm  # æ·»åŠ è¿›åº¦æ¡åº“
import time

class NetflixDatasetLoader:
    """Netflixæ•°æ®é›†åŠ è½½å™¨å’Œé¢„å¤„ç†å™¨"""
    
    def __init__(self, dataset_path: str):
        self.dataset_path = dataset_path
        self.logger = logging.getLogger(self.__class__.__name__)
        self.item_attributes = None
        self.user_interactions = {}
        self.item_id_mapping = {}
        self.n_users = 0
        self.n_items = 0
        self.n_train = 0
        self.n_test = 0
        self.train_items = {}
        self.test_items = {}
        
    def load_and_preprocess(self) -> Dict:
        """åŠ è½½å¹¶é¢„å¤„ç†Netflixæ•°æ®é›†"""
        try:
            # åŠ è½½item_attribute.csv
            self.logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {self.dataset_path}")
            print("ğŸ”„ æ­£åœ¨è¯»å–CSVæ–‡ä»¶...")
            
            df = pd.read_csv(self.dataset_path)
            
            # å‡è®¾CSVæ ¼å¼: item_id, year, title
            if len(df.columns) >= 3:
                df.columns = ['item_id', 'year', 'title']
            else:
                raise ValueError("æ•°æ®é›†æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
            
            print("ğŸ“Š æ­£åœ¨æ¸…æ´—æ•°æ®...")
            # æ•°æ®æ¸…æ´— - æ·»åŠ è¿›åº¦æ¡
            with tqdm(total=4, desc="æ•°æ®æ¸…æ´—", ncols=80) as pbar:
                # æ­¥éª¤1: åˆ é™¤ç¼ºå¤±å€¼
                df = df.dropna(subset=['item_id', 'title'])
                pbar.update(1)
                pbar.set_postfix({"æ­¥éª¤": "åˆ é™¤ç¼ºå¤±å€¼"})
                
                # æ­¥éª¤2: è½¬æ¢å¹´ä»½æ ¼å¼
                df['year'] = pd.to_numeric(df['year'], errors='coerce')
                pbar.update(1)
                pbar.set_postfix({"æ­¥éª¤": "è½¬æ¢å¹´ä»½"})
                
                # æ­¥éª¤3: åˆ é™¤å¹´ä»½ç¼ºå¤±çš„è®°å½•
                df = df.dropna(subset=['year'])
                pbar.update(1)
                pbar.set_postfix({"æ­¥éª¤": "è¿‡æ»¤å¹´ä»½"})
                
                # æ­¥éª¤4: åˆ›å»ºæ˜ å°„
                self.item_attributes = df
                self.n_items = len(df)
                
                # åˆ›å»ºç‰©å“IDæ˜ å°„
                unique_items = sorted(df['item_id'].unique())
                self.item_id_mapping = {item_id: idx for idx, item_id in enumerate(unique_items)}
                pbar.update(1)
                pbar.set_postfix({"æ­¥éª¤": "åˆ›å»ºæ˜ å°„"})
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {self.n_items} ä¸ªç‰©å“")
            
            # ç”Ÿæˆæ¨¡æ‹Ÿç”¨æˆ·äº¤äº’æ•°æ®
            self._generate_user_interactions()
            
            return {
                'n_items': self.n_items,
                'n_users': self.n_users,
                'items_loaded': len(df),
                'year_range': (df['year'].min(), df['year'].max())
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _generate_user_interactions(self):
        """åŸºäºç‰©å“å±æ€§ç”ŸæˆçœŸå®çš„ç”¨æˆ·äº¤äº’æ¨¡å¼"""
        np.random.seed(42)
        
        print("ğŸ¯ æ­£åœ¨åˆ†æç‰©å“ç‰¹å¾...")
        # åˆ†æç‰©å“ç‰¹å¾
        years = self.item_attributes['year'].values
        year_popularity = self._calculate_year_popularity(years)
        
        # ç”Ÿæˆç”¨æˆ·åå¥½æ¡£æ¡ˆ
        self.n_users = min(1000, self.n_items // 2)  # åŠ¨æ€è°ƒæ•´ç”¨æˆ·æ•°
        
        user_types = ['classic', 'modern', 'diverse', 'niche']
        type_ratios = [0.25, 0.35, 0.3, 0.1]
        
        self.logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {self.n_users} ä¸ªç”¨æˆ·çš„äº¤äº’æ•°æ®...")
        print(f"ğŸ‘¥ æ­£åœ¨ç”Ÿæˆ {self.n_users} ä¸ªç”¨æˆ·çš„äº¤äº’æ•°æ®...")
        
        # ä½¿ç”¨tqdmæ·»åŠ è¿›åº¦æ¡
        successful_users = 0
        train_users = 0
        test_users = 0
        
        with tqdm(total=self.n_users, desc="ç”Ÿæˆç”¨æˆ·äº¤äº’", ncols=100) as pbar:
            for user_id in range(self.n_users):
                user_type = np.random.choice(user_types, p=type_ratios)
                
                # æ ¹æ®ç”¨æˆ·ç±»å‹ç¡®å®šäº¤äº’æ•°é‡
                if user_type == 'classic':
                    n_interactions = np.random.poisson(4) + 1
                    year_preference = lambda y: 1.5 if y < 1990 else 0.8
                elif user_type == 'modern':
                    n_interactions = np.random.poisson(6) + 2
                    year_preference = lambda y: 1.5 if y > 2000 else 0.6
                elif user_type == 'diverse':
                    n_interactions = np.random.poisson(8) + 3
                    year_preference = lambda y: 1.0  # æ— æ˜æ˜¾åå¥½
                else:  # niche
                    n_interactions = np.random.poisson(3) + 1
                    year_preference = lambda y: 2.0 if np.random.random() < 0.3 else 0.1
                
                n_interactions = min(n_interactions, min(15, self.n_items // 10))
                
                # é€‰æ‹©ç‰©å“
                item_weights = []
                for _, row in self.item_attributes.iterrows():
                    year = row['year']
                    base_weight = year_popularity.get(year, 0.1)
                    preference_weight = year_preference(year)
                    item_weights.append(base_weight * preference_weight)
                
                # å½’ä¸€åŒ–æƒé‡
                if sum(item_weights) > 0:
                    item_weights = np.array(item_weights) / sum(item_weights)
                else:
                    item_weights = np.ones(len(item_weights)) / len(item_weights)
                
                # é€‰æ‹©ç‰©å“
                try:
                    selected_indices = np.random.choice(
                        len(self.item_attributes), 
                        size=min(n_interactions, len(self.item_attributes)), 
                        replace=False, 
                        p=item_weights
                    )
                    
                    selected_items = [self.item_attributes.iloc[idx]['item_id'] for idx in selected_indices]
                    
                    # æ˜ å°„åˆ°è¿ç»­ID
                    mapped_items = [self.item_id_mapping[item_id] for item_id in selected_items]
                    
                    # 80%ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œ20%ä½œä¸ºæµ‹è¯•æ•°æ®
                    if np.random.random() < 0.8:
                        self.train_items[user_id] = mapped_items
                        self.n_train += len(mapped_items)
                        train_users += 1
                    else:
                        self.test_items[user_id] = mapped_items
                        self.n_test += len(mapped_items)
                        test_users += 1
                    
                    successful_users += 1
                    
                except Exception as e:
                    # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªç”¨æˆ·
                    pass
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                pbar.set_postfix({
                    "æˆåŠŸ": successful_users,
                    "è®­ç»ƒ": train_users,
                    "æµ‹è¯•": test_users,
                    "ç±»å‹": user_type[:2]
                })
                
                # æ¯100ä¸ªç”¨æˆ·æš‚åœä¸€ä¸‹ï¼Œé¿å…è¿›åº¦æ¡æ›´æ–°å¤ªå¿«
                if user_id % 100 == 0 and user_id > 0:
                    time.sleep(0.01)
        
        print(f"âœ… ç”¨æˆ·äº¤äº’ç”Ÿæˆå®Œæˆ!")
        print(f"   ğŸ“Š æˆåŠŸç”Ÿæˆ: {successful_users}/{self.n_users} ç”¨æˆ·")
        print(f"   ğŸ‹ï¸ è®­ç»ƒç”¨æˆ·: {train_users} ({self.n_train} äº¤äº’)")
        print(f"   ğŸ§ª æµ‹è¯•ç”¨æˆ·: {test_users} ({self.n_test} äº¤äº’)")
        
        self.logger.info(f"âœ… ç”Ÿæˆç”¨æˆ·äº¤äº’: {len(self.train_items)}è®­ç»ƒç”¨æˆ·, "
                        f"{len(self.test_items)}æµ‹è¯•ç”¨æˆ·, "
                        f"{self.n_train}è®­ç»ƒäº¤äº’, {self.n_test}æµ‹è¯•äº¤äº’")
    
    def _calculate_year_popularity(self, years):
        """è®¡ç®—å¹´ä»½æµè¡Œåº¦æƒé‡"""
        print("ğŸ“ˆ æ­£åœ¨è®¡ç®—å¹´ä»½æµè¡Œåº¦...")
        year_counts = pd.Series(years).value_counts()
        year_popularity = {}
        
        # æ·»åŠ å¹´ä»½åˆ†æçš„è¿›åº¦æ¡
        with tqdm(year_counts.items(), desc="å¹´ä»½åˆ†æ", ncols=80) as pbar:
            for year, count in pbar:
                # æ–°ç”µå½±è·å¾—é¢å¤–æƒé‡
                recency_bonus = max(0, (year - 1990) / 20) if year >= 1990 else 0
                popularity = np.log(count + 1) + recency_bonus
                year_popularity[year] = popularity
                
                pbar.set_postfix({
                    "å¹´ä»½": int(year),
                    "æ•°é‡": count,
                    "æƒé‡": f"{popularity:.2f}"
                })
        
        # å½’ä¸€åŒ–
        total_pop = sum(year_popularity.values())
        if total_pop > 0:
            year_popularity = {k: v/total_pop for k, v in year_popularity.items()}
        
        print(f"âœ… å¹´ä»½æµè¡Œåº¦åˆ†æå®Œæˆï¼Œè¦†ç›– {len(year_popularity)} ä¸ªå¹´ä»½")
        return year_popularity
    
    def get_item_metadata(self) -> Dict:
        """è·å–ç‰©å“å…ƒæ•°æ®"""
        return {
            'total_items': self.n_items,
            'year_distribution': self.item_attributes['year'].describe().to_dict(),
            'sample_titles': self.item_attributes['title'].head(10).tolist()
        }

class DatasetEnhancer:
    """æ•°æ®é›†å¢å¼ºå™¨"""
    
    def __init__(self, original_dataset, enhancement_config: Dict = None):
        self.original_dataset = original_dataset
        self.config = enhancement_config or self._default_enhancement_config()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # å¢å¼ºåçš„æ•°æ®é›†
        self.enhanced_train_items = {}
        self.enhanced_test_items = {}
        self.enhancement_stats = {}
    
    def _default_enhancement_config(self):
        return {
            'augmentation_ratio': 0.3,  # å¢å¼º30%çš„æ•°æ®
            'noise_level': 0.1,         # 10%çš„å™ªå£°
            'diversity_boost': True,    # å¢åŠ å¤šæ ·æ€§
            'long_tail_emphasis': True, # å¼ºè°ƒé•¿å°¾ç‰©å“
            'synthetic_users': 0.2      # å¢åŠ 20%çš„åˆæˆç”¨æˆ·
        }
    
    def enhance_dataset(self) -> Dict:
        """æ‰§è¡Œæ•°æ®é›†å¢å¼º"""
        self.logger.info("ğŸ”§ å¼€å§‹æ•°æ®é›†å¢å¼º...")
        print("ğŸ”§ å¼€å§‹æ•°æ®é›†å¢å¼º...")
        
        # 1. å¤åˆ¶åŸå§‹æ•°æ®
        print("ğŸ“‹ å¤åˆ¶åŸå§‹æ•°æ®...")
        self.enhanced_train_items = self.original_dataset.train_items.copy()
        self.enhanced_test_items = self.original_dataset.test_items.copy()
        
        enhancement_results = {}
        
        # è®¡ç®—æ€»çš„å¢å¼ºæ­¥éª¤æ•°
        total_steps = sum([
            1 if self.config['diversity_boost'] else 0,
            1 if self.config['long_tail_emphasis'] else 0,
            1 if self.config['synthetic_users'] > 0 else 0,
            1 if self.config['augmentation_ratio'] > 0 else 0
        ])
        
        print(f"ğŸ¯ å°†æ‰§è¡Œ {total_steps} ä¸ªå¢å¼ºæ­¥éª¤")
        
        # 2. æ•°æ®å¢å¼ºç­–ç•¥ - æ·»åŠ æ€»ä½“è¿›åº¦æ¡
        with tqdm(total=total_steps, desc="æ•°æ®é›†å¢å¼º", ncols=100) as main_pbar:
            
            if self.config['diversity_boost']:
                main_pbar.set_postfix({"å½“å‰æ­¥éª¤": "å¤šæ ·æ€§å¢å¼º"})
                diversity_result = self._boost_diversity()
                enhancement_results['diversity_boost'] = diversity_result
                main_pbar.update(1)
            
            if self.config['long_tail_emphasis']:
                main_pbar.set_postfix({"å½“å‰æ­¥éª¤": "é•¿å°¾å¼ºè°ƒ"})
                longtail_result = self._emphasize_long_tail()
                enhancement_results['long_tail_emphasis'] = longtail_result
                main_pbar.update(1)
            
            if self.config['synthetic_users'] > 0:
                main_pbar.set_postfix({"å½“å‰æ­¥éª¤": "åˆæˆç”¨æˆ·"})
                synthetic_result = self._add_synthetic_users()
                enhancement_results['synthetic_users'] = synthetic_result
                main_pbar.update(1)
            
            if self.config['augmentation_ratio'] > 0:
                main_pbar.set_postfix({"å½“å‰æ­¥éª¤": "ç”¨æˆ·å¢å¼º"})
                augment_result = self._augment_existing_users()
                enhancement_results['user_augmentation'] = augment_result
                main_pbar.update(1)
        
        # 3. è®¡ç®—å¢å¼ºç»Ÿè®¡
        print("ğŸ“Š è®¡ç®—å¢å¼ºç»Ÿè®¡...")
        original_interactions = sum(len(items) for items in self.original_dataset.train_items.values())
        enhanced_interactions = sum(len(items) for items in self.enhanced_train_items.values())
        
        self.enhancement_stats = {
            'original_users': len(self.original_dataset.train_items),
            'enhanced_users': len(self.enhanced_train_items),
            'original_interactions': original_interactions,
            'enhanced_interactions': enhanced_interactions,
            'enhancement_ratio': enhanced_interactions / original_interactions if original_interactions > 0 else 0,
            'strategies_applied': list(enhancement_results.keys())
        }
        
        print(f"âœ… æ•°æ®é›†å¢å¼ºå®Œæˆ!")
        print(f"   ğŸ‘¥ ç”¨æˆ·æ•°: {self.enhancement_stats['original_users']} â†’ {self.enhancement_stats['enhanced_users']}")
        print(f"   ğŸ”— äº¤äº’æ•°: {original_interactions} â†’ {enhanced_interactions}")
        print(f"   ğŸ“ˆ å¢é•¿ç‡: +{self.enhancement_stats['enhancement_ratio']:.1%}")
        
        self.logger.info(f"âœ… æ•°æ®é›†å¢å¼ºå®Œæˆ: {len(self.enhanced_train_items)}ç”¨æˆ·, "
                        f"{enhanced_interactions}äº¤äº’ "
                        f"(å¢é•¿{self.enhancement_stats['enhancement_ratio']:.1%})")
        
        return enhancement_results
    
    def _boost_diversity(self) -> Dict:
        """å¢å¼ºç”¨æˆ·è¡Œä¸ºå¤šæ ·æ€§"""
        print("ğŸŒˆ æ­£åœ¨å¢å¼ºç”¨æˆ·è¡Œä¸ºå¤šæ ·æ€§...")
        enhanced_count = 0
        total_users = len(self.enhanced_train_items)
        
        with tqdm(list(self.enhanced_train_items.items()), 
                 desc="å¤šæ ·æ€§å¢å¼º", ncols=80) as pbar:
            for user_id, items in pbar:
                if len(items) < 5:  # å¯¹äº¤äº’è¾ƒå°‘çš„ç”¨æˆ·å¢åŠ å¤šæ ·æ€§
                    # è®¡ç®—å½“å‰ç”¨æˆ·çš„ç‰©å“å¹´ä»½åˆ†å¸ƒ
                    current_years = []
                    for item_id in items:
                        if item_id < len(self.original_dataset.item_attributes):
                            year = self.original_dataset.item_attributes.iloc[item_id]['year']
                            current_years.append(year)
                    
                    # å¦‚æœå¹´ä»½è¿‡äºé›†ä¸­ï¼Œæ·»åŠ ä¸åŒå¹´ä»½çš„ç‰©å“
                    if len(set(current_years)) <= 2 and len(current_years) > 0:
                        # é€‰æ‹©ä¸åŒå¹´ä»½çš„ç‰©å“
                        available_items = list(range(self.original_dataset.n_items))
                        new_items = np.random.choice(available_items, size=2, replace=False)
                        
                        extended_items = list(set(items + new_items.tolist()))
                        self.enhanced_train_items[user_id] = extended_items
                        enhanced_count += 1
                
                pbar.set_postfix({"å¢å¼ºç”¨æˆ·": enhanced_count})
        
        print(f"âœ… å¤šæ ·æ€§å¢å¼ºå®Œæˆï¼Œå¢å¼ºäº† {enhanced_count} ä¸ªç”¨æˆ·")
        return {'users_enhanced': enhanced_count}
    
    def _emphasize_long_tail(self) -> Dict:
        """å¼ºè°ƒé•¿å°¾ç‰©å“"""
        print("ğŸ¦’ æ­£åœ¨å¼ºè°ƒé•¿å°¾ç‰©å“...")
        
        # è®¡ç®—ç‰©å“æµè¡Œåº¦
        print("ğŸ“Š è®¡ç®—ç‰©å“æµè¡Œåº¦...")
        item_popularity = {}
        for items in tqdm(self.enhanced_train_items.values(), 
                         desc="ç»Ÿè®¡ç‰©å“æµè¡Œåº¦", ncols=80):
            for item_id in items:
                item_popularity[item_id] = item_popularity.get(item_id, 0) + 1
        
        # è¯†åˆ«é•¿å°¾ç‰©å“ï¼ˆå‡ºç°æ¬¡æ•°å°‘äºå¹³å‡å€¼çš„50%ï¼‰
        avg_popularity = np.mean(list(item_popularity.values())) if item_popularity else 1
        long_tail_items = [item_id for item_id, pop in item_popularity.items() 
                          if pop < avg_popularity * 0.5]
        
        print(f"ğŸ” è¯†åˆ«å‡º {len(long_tail_items)} ä¸ªé•¿å°¾ç‰©å“")
        
        enhanced_count = 0
        user_list = list(self.enhanced_train_items.keys())
        
        # ä¸ºéƒ¨åˆ†ç”¨æˆ·æ·»åŠ é•¿å°¾ç‰©å“
        with tqdm(user_list, desc="æ·»åŠ é•¿å°¾ç‰©å“", ncols=80) as pbar:
            for user_id in pbar:
                if np.random.random() < 0.3 and long_tail_items:  # 30%çš„ç”¨æˆ·
                    selected_longtail = np.random.choice(long_tail_items, size=1)[0]
                    if selected_longtail not in self.enhanced_train_items[user_id]:
                        self.enhanced_train_items[user_id].append(selected_longtail)
                        enhanced_count += 1
                
                pbar.set_postfix({"æ·»åŠ é•¿å°¾": enhanced_count})
        
        print(f"âœ… é•¿å°¾å¼ºè°ƒå®Œæˆï¼Œä¸º {enhanced_count} ä¸ªç”¨æˆ·æ·»åŠ äº†é•¿å°¾ç‰©å“")
        return {'long_tail_items': len(long_tail_items), 'users_enhanced': enhanced_count}
    
    def _add_synthetic_users(self) -> Dict:
        """æ·»åŠ åˆæˆç”¨æˆ·"""
        n_synthetic = int(len(self.original_dataset.train_items) * self.config['synthetic_users'])
        print(f"ğŸ¤– æ­£åœ¨æ·»åŠ  {n_synthetic} ä¸ªåˆæˆç”¨æˆ·...")
        
        synthetic_added = 0
        
        if not self.enhanced_train_items:
            base_user_id = 0
        else:
            base_user_id = max(self.enhanced_train_items.keys()) + 1
        
        # åˆ†æç°æœ‰ç”¨æˆ·æ¨¡å¼
        interaction_lengths = [len(items) for items in self.enhanced_train_items.values()]
        avg_length = np.mean(interaction_lengths) if interaction_lengths else 3
        
        with tqdm(range(n_synthetic), desc="ç”Ÿæˆåˆæˆç”¨æˆ·", ncols=80) as pbar:
            for i in pbar:
                synthetic_user_id = base_user_id + i
                
                # ç”Ÿæˆåˆæˆäº¤äº’
                synthetic_length = max(1, int(np.random.poisson(avg_length)))
                synthetic_length = min(synthetic_length, 10)
                
                try:
                    synthetic_items = np.random.choice(
                        self.original_dataset.n_items, 
                        size=synthetic_length, 
                        replace=False
                    ).tolist()
                    
                    self.enhanced_train_items[synthetic_user_id] = synthetic_items
                    synthetic_added += 1
                except:
                    # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œè·³è¿‡
                    pass
                
                pbar.set_postfix({"å·²æ·»åŠ ": synthetic_added})
        
        print(f"âœ… åˆæˆç”¨æˆ·æ·»åŠ å®Œæˆï¼ŒæˆåŠŸæ·»åŠ  {synthetic_added} ä¸ªç”¨æˆ·")
        return {'synthetic_users_added': synthetic_added}
    
    def _augment_existing_users(self) -> Dict:
        """å¢å¼ºç°æœ‰ç”¨æˆ·æ•°æ®"""
        augment_count = int(len(self.enhanced_train_items) * self.config['augmentation_ratio'])
        print(f"ğŸ”„ æ­£åœ¨å¢å¼º {augment_count} ä¸ªç°æœ‰ç”¨æˆ·...")
        
        augmented_users = 0
        
        user_ids = list(self.enhanced_train_items.keys())
        if augment_count > len(user_ids):
            augment_count = len(user_ids)
        
        selected_users = np.random.choice(user_ids, size=augment_count, replace=False)
        
        with tqdm(selected_users, desc="å¢å¼ºç°æœ‰ç”¨æˆ·", ncols=80) as pbar:
            for user_id in pbar:
                current_items = self.enhanced_train_items[user_id]
                
                # æ·»åŠ 1-2ä¸ªæ–°ç‰©å“
                available_items = [i for i in range(self.original_dataset.n_items) 
                                 if i not in current_items]
                
                if available_items:
                    n_new_items = np.random.randint(1, 3)
                    try:
                        new_items = np.random.choice(
                            available_items, 
                            size=min(n_new_items, len(available_items)), 
                            replace=False
                        )
                        
                        self.enhanced_train_items[user_id].extend(new_items.tolist())
                        augmented_users += 1
                    except:
                        # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œè·³è¿‡
                        pass
                
                pbar.set_postfix({"å·²å¢å¼º": augmented_users})
        
        print(f"âœ… ç”¨æˆ·å¢å¼ºå®Œæˆï¼Œå¢å¼ºäº† {augmented_users} ä¸ªç”¨æˆ·")
        return {'users_augmented': augmented_users}
    
    def get_enhanced_dataset(self):
        """è·å–å¢å¼ºåçš„æ•°æ®é›†å¯¹è±¡"""
        # åˆ›å»ºå¢å¼ºåçš„æ•°æ®é›†å¯¹è±¡
        enhanced_dataset = type('EnhancedDataset', (), {
            'train_items': self.enhanced_train_items,
            'test_items': self.enhanced_test_items,
            'n_users': len(self.enhanced_train_items),
            'n_items': self.original_dataset.n_items,
            'n_train': sum(len(items) for items in self.enhanced_train_items.values()),
            'n_test': sum(len(items) for items in self.enhanced_test_items.values()),
            'item_attributes': self.original_dataset.item_attributes,
            'enhancement_stats': self.enhancement_stats
        })()
        
        return enhanced_dataset