import logging
import time
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from core.data_distribution_analyzer import DataDistributionAnalyzer
from core.dynamic_prompt_tuner import DynamicPromptTuner
from core.adversarial_quality_module import AdversarialQualityModule
from core.evaluation_metrics import InnovativeEvaluationMetrics

logger = logging.getLogger(__name__)

class LLMRecommendationInnovationExperiment:
    """
    LLMæ¨èæŠ€æœ¯åˆ›æ–°å®éªŒä¸»æ§åˆ¶å™¨
    å®ç°å®Œæ•´çš„é—­ç¯æ¡†æ¶ï¼šåˆ†å¸ƒè‡ªé€‚åº”è°ƒæ§ + å¯¹æŠ—æ€§è´¨é‡ä¿è¯
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or self._default_config()
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.distribution_analyzer = DataDistributionAnalyzer(self.config.get('analyzer_config'))
        self.prompt_tuner = DynamicPromptTuner(self.config.get('tuner_config'))
        self.adversarial_module = AdversarialQualityModule(self.config.get('adversarial_config'))
        self.evaluation_metrics = InnovativeEvaluationMetrics(self.config.get('metrics_config'))
        
        # å®éªŒçŠ¶æ€
        self.iteration_results = []
        self.real_data_vector = None
        
        logger.info("âœ… LLMæ¨èæŠ€æœ¯åˆ›æ–°å®éªŒåˆå§‹åŒ–å®Œæˆ")
    
    def _default_config(self) -> Dict:
        return {
            'max_iterations': 5,
            'convergence_threshold': 0.05,
            'quality_improvement_threshold': 0.1,
            'samples_per_iteration': 100,
            'enable_early_stopping': True,
            'save_intermediate_results': True,
            'experiment_name': 'LLM_Rec_Innovation_Experiment'
        }
    
    def run_complete_experiment(self, real_data: List[str]) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„åˆ›æ–°å®éªŒæµç¨‹
        """
        logger.info("ğŸš€ å¼€å§‹LLMæ¨èæŠ€æœ¯åˆ›æ–°å®Œæ•´å®éªŒ...")
        experiment_start_time = time.time()
        
        try:
            # Phase 1: å¯åŠ¨ - åˆ†æçœŸå®æ•°æ®
            logger.info("ğŸ“Š Phase 1: çœŸå®æ•°æ®åˆ†å¸ƒåˆ†æ...")
            self.real_data_vector = self.distribution_analyzer.analyze_real_data(real_data)
            
            # Phase 2: è¿­ä»£ä¼˜åŒ–å¾ªç¯
            logger.info("ğŸ”„ Phase 2: å¼€å§‹è¿­ä»£ä¼˜åŒ–...")
            iteration_results = self._run_iterative_optimization(real_data)
            
            # Phase 3: ç»¼åˆè¯„ä¼°
            logger.info("ğŸ“‹ Phase 3: ç»¼åˆåˆ›æ–°è¯„ä¼°...")
            final_evaluation = self._run_comprehensive_evaluation(real_data, iteration_results)
            
            # Phase 4: ç»“æœæ•´åˆ
            total_time = time.time() - experiment_start_time
            experiment_results = self._integrate_experiment_results(
                real_data, iteration_results, final_evaluation, total_time
            )
            
            logger.info("âœ… å®Œæ•´å®éªŒæµç¨‹æ‰§è¡Œå®Œæˆ")
            return experiment_results
            
        except Exception as e:
            logger.error(f"âŒ å®éªŒæ‰§è¡Œå¤±è´¥: {e}")
            return {'error': str(e), 'execution_time': time.time() - experiment_start_time}
    
    def _run_iterative_optimization(self, real_data: List[str]) -> List[Dict]:
        """
        è¿è¡Œè¿­ä»£ä¼˜åŒ–æµç¨‹
        """
        iteration_results = []
        current_prompt = "è¯·ç”Ÿæˆæ¨èç³»ç»Ÿçš„ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®ï¼Œæ ¼å¼ä¸º'ç”¨æˆ·ID ç‰©å“ID1 ç‰©å“ID2 ...'ï¼Œæ¯è¡Œä¸€ä¸ªç”¨æˆ·ã€‚"
        
        for iteration in range(1, self.config['max_iterations'] + 1):
            logger.info(f"ğŸ”„ æ‰§è¡Œç¬¬{iteration}è½®è¿­ä»£...")
            
            iteration_start_time = time.time()
            
            # Step 1: ç”Ÿæˆåˆæˆæ•°æ®
            synthetic_data = self.prompt_tuner.generate_synthetic_data(
                current_prompt, self.config['samples_per_iteration']
            )
            
            # Step 2: å¯¹æŠ—æ€§è´¨é‡ä¿è¯
            adversarial_results = self.adversarial_module.run_adversarial_round(
                real_data, synthetic_data, iteration
            )
            
            # Step 3: åˆ†æåˆæˆæ•°æ®åˆ†å¸ƒ
            synth_data_vector = self.distribution_analyzer.analyze_synthetic_data(
                adversarial_results['final_samples']
            )
            
            # Step 4: è®¡ç®—åˆ†å¸ƒåå·®
            divergence_analysis = self.distribution_analyzer.calculate_distribution_divergence(
                self.real_data_vector, synth_data_vector
            )
            
            # Step 5: åŠ¨æ€Promptè°ƒä¼˜
            updated_prompt, adjustment_details = self.prompt_tuner.adjust_prompt_strategy(
                self.real_data_vector, 
                synth_data_vector, 
                divergence_analysis,
                adversarial_results['discriminator_results']['detailed_report']
            )
            
            # è®°å½•è¿­ä»£ç»“æœ
            iteration_result = {
                'iteration': iteration,
                'execution_time': time.time() - iteration_start_time,
                'synthetic_data': synthetic_data,
                'final_samples': adversarial_results['final_samples'],
                'synth_data_vector': synth_data_vector,
                'divergence_analysis': divergence_analysis,
                'adversarial_results': adversarial_results,
                'adjustment_details': adjustment_details,
                'updated_prompt': updated_prompt,
                'js_divergence': divergence_analysis.get('js_divergence', 0),
                'retention_rate': adversarial_results.get('retention_rate', 0),
                'adversarial_success_rate': adversarial_results.get('adversarial_success_rate', 0)
            }
            
            iteration_results.append(iteration_result)
            current_prompt = updated_prompt
            
            # æ—©åœæ£€æŸ¥
            if self._should_early_stop(iteration_results):
                logger.info(f"ğŸ›‘ æ£€æµ‹åˆ°æ”¶æ•›ï¼Œåœ¨ç¬¬{iteration}è½®æå‰åœæ­¢")
                break
            
            logger.info(f"âœ… ç¬¬{iteration}è½®è¿­ä»£å®Œæˆ")
        
        return iteration_results
    
    def _should_early_stop(self, iteration_results: List[Dict]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        """
        if not self.config['enable_early_stopping'] or len(iteration_results) < 2:
            return False
        
        # æ£€æŸ¥JSæ•£åº¦æ”¶æ•›
        recent_divergences = [r['js_divergence'] for r in iteration_results[-2:]]
        divergence_improvement = abs(recent_divergences[0] - recent_divergences[1])
        
        if divergence_improvement < self.config['convergence_threshold']:
            return True
        
        # æ£€æŸ¥è´¨é‡æŒ‡æ ‡æ”¶æ•›
        recent_quality = [r['adversarial_success_rate'] for r in iteration_results[-2:]]
        quality_improvement = abs(recent_quality[1] - recent_quality[0])
        
        if quality_improvement < self.config['quality_improvement_threshold']:
            return True
        
        return False
    
    def _run_comprehensive_evaluation(self, real_data: List[str], iteration_results: List[Dict]) -> Dict:
        """
        è¿è¡Œç»¼åˆåˆ›æ–°è¯„ä¼°
        """
        if not iteration_results:
            return {'error': 'æ²¡æœ‰å¯è¯„ä¼°çš„è¿­ä»£ç»“æœ'}
        
        # æ”¶é›†æœ€ç»ˆçš„åˆæˆæ•°æ®
        final_synthetic_data = iteration_results[-1]['final_samples']
        
        # æ”¶é›†æ‰€æœ‰è½®æ¬¡çš„å¯¹æŠ—ç»“æœ
        all_adversarial_results = [r['adversarial_results'] for r in iteration_results]
        
        # ç»¼åˆè¯„ä¼°
        comprehensive_eval = self.evaluation_metrics.comprehensive_evaluation(
            real_data=real_data,
            synthetic_data=final_synthetic_data,
            adversarial_results=all_adversarial_results
        )
        
        return comprehensive_eval
    
    def _integrate_experiment_results(self, 
                                    real_data: List[str],
                                    iteration_results: List[Dict],
                                    final_evaluation: Dict,
                                    total_time: float) -> Dict:
        """
        æ•´åˆå®éªŒç»“æœ
        """
        # è®¡ç®—å…³é”®æŒ‡æ ‡çš„è¶‹åŠ¿
        convergence_trend = self._analyze_convergence_trend(iteration_results)
        quality_trend = self._analyze_quality_trend(iteration_results)
        
        # ç”Ÿæˆå®éªŒæ€»ç»“
        experiment_summary = self._generate_experiment_summary(
            iteration_results, final_evaluation, convergence_trend, quality_trend
        )
        
        # æ•´åˆå®Œæ•´ç»“æœ
        integrated_results = {
            'experiment_metadata': {
                'experiment_name': self.config['experiment_name'],
                'timestamp': datetime.now().isoformat(),
                'total_execution_time': total_time,
                'iterations_completed': len(iteration_results),
                'real_data_samples': len(real_data)
            },
            'real_data_analysis': {
                'distribution_vector': self.real_data_vector,
                'sample_count': len(real_data)
            },
            'iteration_results': iteration_results,
            'convergence_analysis': {
                'convergence_trend': convergence_trend,
                'quality_trend': quality_trend,
                'final_js_divergence': iteration_results[-1]['js_divergence'] if iteration_results else 0,
                'final_retention_rate': iteration_results[-1]['retention_rate'] if iteration_results else 0
            },
            'comprehensive_evaluation': final_evaluation,
            'experiment_summary': experiment_summary,
            'performance_metrics': {
                'avg_iteration_time': sum(r['execution_time'] for r in iteration_results) / len(iteration_results) if iteration_results else 0,
                'total_samples_generated': sum(len(r['synthetic_data']) for r in iteration_results),
                'total_samples_retained': sum(len(r['final_samples']) for r in iteration_results),
                'overall_retention_rate': sum(r['retention_rate'] for r in iteration_results) / len(iteration_results) if iteration_results else 0
            }
        }
        
        return integrated_results
    
    def _analyze_convergence_trend(self, iteration_results: List[Dict]) -> Dict:
        """
        åˆ†ææ”¶æ•›è¶‹åŠ¿
        """
        if not iteration_results:
            return {'trend': 'no_data'}
        
        js_divergences = [r['js_divergence'] for r in iteration_results]
        
        # è®¡ç®—è¶‹åŠ¿
        if len(js_divergences) >= 2:
            improvement = js_divergences[0] - js_divergences[-1]
            trend = 'converging' if improvement > 0 else 'diverging' if improvement < -0.01 else 'stable'
        else:
            trend = 'insufficient_data'
        
        return {
            'trend': trend,
            'initial_divergence': js_divergences[0],
            'final_divergence': js_divergences[-1],
            'total_improvement': js_divergences[0] - js_divergences[-1] if len(js_divergences) >= 2 else 0,
            'convergence_rate': self._calculate_convergence_rate(js_divergences)
        }
    
    def _analyze_quality_trend(self, iteration_results: List[Dict]) -> Dict:
        """
        åˆ†æè´¨é‡è¶‹åŠ¿
        """
        if not iteration_results:
            return {'trend': 'no_data'}
        
        success_rates = [r['adversarial_success_rate'] for r in iteration_results]
        retention_rates = [r['retention_rate'] for r in iteration_results]
        
        return {
            'adversarial_success_trend': self._calculate_trend(success_rates),
            'retention_rate_trend': self._calculate_trend(retention_rates),
            'quality_stability': 1 - (np.std(success_rates) if success_rates else 0),
            'final_quality_score': success_rates[-1] if success_rates else 0
        }
    
    def _calculate_convergence_rate(self, divergences: List[float]) -> float:
        """
        è®¡ç®—æ”¶æ•›é€Ÿç‡
        """
        if len(divergences) < 2:
            return 0.0
        
        improvements = []
        for i in range(1, len(divergences)):
            improvement = divergences[i-1] - divergences[i]
            improvements.append(improvement)
        
        return sum(improvements) / len(improvements) if improvements else 0.0
    
    def _calculate_trend(self, values: List[float]) -> str:
        """
        è®¡ç®—è¶‹åŠ¿æ–¹å‘
        """
        if len(values) < 2:
            return 'insufficient_data'
        
        slope = (values[-1] - values[0]) / (len(values) - 1)
        
        if slope > 0.01:
            return 'improving'
        elif slope < -0.01:
            return 'declining'
        else:
            return 'stable'
    
    def _generate_experiment_summary(self, 
                                   iteration_results: List[Dict],
                                   final_evaluation: Dict,
                                   convergence_trend: Dict,
                                   quality_trend: Dict) -> str:
        """
        ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š
        """
        if not iteration_results:
            return "å®éªŒæœªæˆåŠŸå®Œæˆï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"
        
        innovation_score = final_evaluation.get('innovation_score', 0)
        
        summary = f"""
LLMæ¨èæŠ€æœ¯åˆ›æ–°å®éªŒæ€»ç»“æŠ¥å‘Š
============================

å®éªŒåŸºæœ¬ä¿¡æ¯:
- å®Œæˆè¿­ä»£æ¬¡æ•°: {len(iteration_results)}
- åˆ›æ–°ç»¼åˆè¯„åˆ†: {innovation_score:.3f}
- å®éªŒè¯„çº§: {'ä¼˜ç§€' if innovation_score >= 0.8 else 'è‰¯å¥½' if innovation_score >= 0.6 else 'ä¸€èˆ¬' if innovation_score >= 0.4 else 'éœ€è¦æ”¹è¿›'}

æ”¶æ•›æ€§åˆ†æ:
- æ”¶æ•›è¶‹åŠ¿: {convergence_trend.get('trend', 'unknown')}
- åˆå§‹JSæ•£åº¦: {convergence_trend.get('initial_divergence', 0):.4f}
- æœ€ç»ˆJSæ•£åº¦: {convergence_trend.get('final_divergence', 0):.4f}
- æ€»ä½“æ”¹è¿›: {convergence_trend.get('total_improvement', 0):+.4f}

è´¨é‡æå‡åˆ†æ:
- å¯¹æŠ—æˆåŠŸç‡è¶‹åŠ¿: {quality_trend.get('adversarial_success_trend', 'unknown')}
- æ ·æœ¬ä¿ç•™ç‡è¶‹åŠ¿: {quality_trend.get('retention_rate_trend', 'unknown')}
- æœ€ç»ˆè´¨é‡è¯„åˆ†: {quality_trend.get('final_quality_score', 0):.3f}

åˆ›æ–°æ•ˆæœè¯„ä¼°:
- å¯¹æŠ—é‰´åˆ«æˆåŠŸç‡: {final_evaluation.get('adversarial_evaluation', {}).get('adversarial_success_rate', 0):.3f}
- é•¿å°¾æ¿€æ´»è¦†ç›–ç‡: {final_evaluation.get('long_tail_evaluation', {}).get('long_tail_coverage_rate', 0):.3f}
- æ³›åŒ–é²æ£’æ€§è¯„åˆ†: {final_evaluation.get('robustness_evaluation', {}).get('overall_robustness_score', 0):.3f}

å…³é”®æˆæœ:
1. å®ç°äº†åŠ¨æ€Promptè°ƒä¼˜ä¸å¯¹æŠ—è´¨é‡ä¿è¯çš„é—­ç¯æ¡†æ¶
2. åœ¨é•¿å°¾ç‰©å“æ¿€æ´»å’Œåˆ†å¸ƒé€‚åº”æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›
3. å±•ç¤ºäº†LLMåœ¨æ¨èç³»ç»Ÿæ•°æ®å¢å¼ºæ–¹é¢çš„åˆ›æ–°æ½œåŠ›

å»ºè®®ä¸å±•æœ›:
- ç»§ç»­ä¼˜åŒ–å¯¹æŠ—è®­ç»ƒç­–ç•¥ï¼Œæå‡ç”Ÿæˆå™¨æ¬ºéª—èƒ½åŠ›
- æ·±åŒ–é•¿å°¾ç‰©å“æ¿€æ´»æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æ”¹å–„æ¨èå…¬å¹³æ€§
- æ¢ç´¢æ›´å¤šçš„åˆ†å¸ƒè‡ªé€‚åº”è°ƒæ§ç»´åº¦å’Œç­–ç•¥
        """
        
        return summary.strip()
    
    def get_experiment_state(self) -> Dict:
        """
        è·å–å½“å‰å®éªŒçŠ¶æ€
        """
        return {
            'iterations_completed': len(self.iteration_results),
            'real_data_analyzed': self.real_data_vector is not None,
            'last_js_divergence': self.iteration_results[-1]['js_divergence'] if self.iteration_results else None,
            'last_retention_rate': self.iteration_results[-1]['retention_rate'] if self.iteration_results else None,
            'experiment_config': self.config
        }