import sys
import os
import json
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from llm_framework import LLMRecommendationFramework
from evaluation_metrics import LLMEvaluationMetrics
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/experiment_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LLMRecExperiment:
    """LLMæ¨èç³»ç»Ÿå®Œæ•´å®éªŒ"""
    
    def __init__(self, config: dict = None):
        self.config = config or self._default_config()
        self.results = {}
        
    def _default_config(self):
        return {
            'experiment_name': 'LLM_Rec_Innovation_Test',
            'data_config': {
                'n_users': 1000,
                'n_items': 500,
                'n_train_users': 200,  # å®é™…ç”Ÿæˆè®­ç»ƒæ•°æ®çš„ç”¨æˆ·æ•°
                'min_interactions': 1,
                'max_interactions': 15,
                'long_tail_ratio': 0.7  # é•¿å°¾ç‰©å“æ¯”ä¾‹
            },
            'framework_config': {
                'max_iterations': 5,
                'samples_per_iteration': 100,
                'adversarial_rounds': 3,
                'early_stopping': True,
                'convergence_tolerance': 0.05
            },
            'evaluation_config': {
                'metrics': ['adversarial_success_rate', 'long_tail_activation', 'generalization_robustness'],
                'save_detailed_results': True,
                'generate_visualizations': True
            }
        }
    
    def create_realistic_data_generator(self):
        """åˆ›å»ºç°å®çš„æ•°æ®ç”Ÿæˆå™¨"""
        logger.info("åˆ›å»ºç°å®æ•°æ®ç”Ÿæˆå™¨...")
        
        class RealisticDataGenerator:
            def __init__(self, config):
                self.n_users = config['n_users']
                self.n_items = config['n_items']
                self.n_train = 0
                self.n_test = 0
                self.train_items = {}
                
                # ç”Ÿæˆç°å®çš„äº¤äº’æ•°æ®
                self._generate_realistic_interactions(config)
            
            def _generate_realistic_interactions(self, config):
                import random
                import numpy as np
                
                random.seed(42)
                np.random.seed(42)
                
                # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„ç”¨æˆ·
                user_types = ['low_active', 'medium_active', 'high_active']
                type_ratios = [0.5, 0.3, 0.2]  # ç¬¦åˆç°å®çš„ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ
                
                for user_id in range(config['n_train_users']):
                    # éšæœºé€‰æ‹©ç”¨æˆ·ç±»å‹
                    user_type = np.random.choice(user_types, p=type_ratios)
                    
                    # æ ¹æ®ç”¨æˆ·ç±»å‹ç¡®å®šäº¤äº’æ•°é‡
                    if user_type == 'low_active':
                        n_interactions = random.randint(1, 3)
                    elif user_type == 'medium_active':
                        n_interactions = random.randint(3, 8)
                    else:  # high_active
                        n_interactions = random.randint(8, config['max_interactions'])
                    
                    # ç”Ÿæˆç‰©å“äº¤äº’ï¼ˆä½“ç°é•¿å°¾åˆ†å¸ƒï¼‰
                    items = set()
                    for _ in range(n_interactions):
                        if random.random() < 0.2:  # 20%æ¦‚ç‡é€‰æ‹©çƒ­é—¨ç‰©å“
                            item_id = random.randint(0, 49)  # å‰50ä¸ªä¸ºçƒ­é—¨ç‰©å“
                        elif random.random() < 0.5:  # 30%æ¦‚ç‡é€‰æ‹©ä¸­ç­‰ç‰©å“
                            item_id = random.randint(50, 199)
                        else:  # 50%æ¦‚ç‡é€‰æ‹©é•¿å°¾ç‰©å“
                            item_id = random.randint(200, config['n_items'] - 1)
                        
                        items.add(item_id)
                    
                    if items:
                        self.train_items[user_id] = list(items)
                        self.n_train += len(items)
                
                logger.info(f"ç”Ÿæˆäº†{len(self.train_items)}ä¸ªç”¨æˆ·ï¼Œ{self.n_train}ä¸ªäº¤äº’")
        
        return RealisticDataGenerator(self.config['data_config'])
    
    def run_complete_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        logger.info(f"ğŸš€ å¼€å§‹LLMæ¨èç³»ç»Ÿåˆ›æ–°å®éªŒ: {self.config['experiment_name']}")
        
        experiment_start_time = time.time()
        
        try:
            # 1. åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
            data_generator = self.create_realistic_data_generator()
            
            # 2. åˆå§‹åŒ–LLMæ¡†æ¶
            logger.info("åˆå§‹åŒ–LLMæ¨èæ¡†æ¶...")
            framework = LLMRecommendationFramework(
                data_generator=data_generator,
                config=self.config['framework_config']
            )
            
            # 3. è¿è¡Œæ¡†æ¶
            logger.info("è¿è¡ŒLLMæ¨èæŠ€æœ¯åˆ›æ–°æ¡†æ¶...")
            framework_results = framework.run_complete_framework()
            
            if 'error' in framework_results:
                logger.error(f"æ¡†æ¶è¿è¡Œå¤±è´¥: {framework_results['error']}")
                return None
            
            # 4. è®¡ç®—åˆ›æ–°è¯„ä¼°æŒ‡æ ‡
            logger.info("è®¡ç®—åˆ›æ–°è¯„ä¼°æŒ‡æ ‡...")
            evaluator = LLMEvaluationMetrics()
            
            # æ„å»ºè¯„ä¼°æ‰€éœ€çš„æ•°æ®ç»“æ„
            evaluation_data = {
                'real_stats': framework.real_stats,
                'synthetic_stats': framework.synthetic_stats_history,
                'convergence_history': framework_results['convergence_history'],
                'quality_history': framework_results['quality_history'],
                'adversarial_results': []  # è¿™é‡Œéœ€è¦ä»framework_resultsä¸­æå–å¯¹æŠ—ç»“æœ
            }
            
            # æå–å¯¹æŠ—ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            for iteration_result in framework_results['iterations']:
                if 'adversarial_results' in iteration_result:
                    evaluation_data['adversarial_results'].append(iteration_result['adversarial_results'])
            
            innovation_metrics = evaluator.calculate_comprehensive_metrics(evaluation_data)
            
            # 5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
            evaluation_report = evaluator.generate_evaluation_report(innovation_metrics)
            
            # 6. æ•´åˆå®éªŒç»“æœ
            total_time = time.time() - experiment_start_time
            
            experiment_results = {
                'experiment_config': self.config,
                'framework_results': framework_results,
                'innovation_metrics': innovation_metrics,
                'evaluation_report': evaluation_report,
                'execution_time': total_time,
                'timestamp': datetime.now().isoformat()
            }
            
            # 7. ä¿å­˜ç»“æœ
            self._save_experiment_results(experiment_results)
            
            # 8. æ‰“å°æ€»ç»“
            self._print_experiment_summary(experiment_results)
            
            return experiment_results
            
        except Exception as e:
            logger.error(f"å®éªŒæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _save_experiment_results(self, results):
        """ä¿å­˜å®éªŒç»“æœ"""
        os.makedirs('results', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_file = f'results/experiment_results_{timestamp}.json'
        try:
            # è¿‡æ»¤ä¸èƒ½JSONåºåˆ—åŒ–çš„å¯¹è±¡
            serializable_results = self._make_serializable(results)
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            logger.info(f"å®éªŒç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONç»“æœå¤±è´¥: {e}")
        
        # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        report_file = f'results/evaluation_report_{timestamp}.txt'
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(results['evaluation_report'])
            logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _make_serializable(self, obj):
        """ä½¿å¯¹è±¡å¯JSONåºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif hasattr(obj, 'tolist'):  # numpyæ•°ç»„
            return obj.tolist()
        elif isinstance(obj, (int, float, str, bool)) or obj is None:
            return obj
        else:
            return str(obj)
    
    def _print_experiment_summary(self, results):
        """æ‰“å°å®éªŒæ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ† LLMæ¨èç³»ç»Ÿåˆ›æ–°å®éªŒå®Œæˆ")
        print("="*80)
        
        framework_metrics = results['framework_results'].get('final_metrics', {})
        innovation_metrics = results['innovation_metrics']
        
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {results['execution_time']:.2f}ç§’")
        print(f"ğŸ”„ æ¡†æ¶è¿­ä»£æ¬¡æ•°: {framework_metrics.get('total_iterations', 0)}")
        print(f"ğŸ“ˆ æœ€ä½³æ”¶æ•›åˆ†æ•°: {framework_metrics.get('best_convergence_score', 0):.4f}")
        print(f"âœ¨ é«˜è´¨é‡æ ·æœ¬æ•°: {framework_metrics.get('total_high_quality_samples', 0)}")
        
        print(f"\nğŸ¯ åˆ›æ–°è¯„ä¼°æŒ‡æ ‡:")
        print(f"   å¯¹æŠ—æˆåŠŸç‡: {innovation_metrics['adversarial_success_rate']:.3f}")
        print(f"   é•¿å°¾æ¿€æ´»åº¦: {innovation_metrics['long_tail_activation']:.3f}")
        print(f"   æ³›åŒ–é²æ£’æ€§: {innovation_metrics['generalization_robustness']:.3f}")
        print(f"   æ€»ä½“åˆ›æ–°è¯„åˆ†: {innovation_metrics['overall_innovation_score']:.3f}")
        
        print(f"\nğŸ“‹ è¯¦ç»†è¯„ä¼°æŠ¥å‘Š:")
        print(results['evaluation_report'])


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('logs', exist_ok=True)
    os.makedirs('results', exist_ok=True)
    
    # å®éªŒé…ç½®
    experiment_config = {
        'experiment_name': 'LLM_Recommendation_Innovation_Experiment',
        'data_config': {
            'n_users': 1000,
            'n_items': 500,
            'n_train_users': 150,
            'min_interactions': 1,
            'max_interactions': 12,
            'long_tail_ratio': 0.7
        },
        'framework_config': {
            'max_iterations': 4,
            'samples_per_iteration': 50,
            'adversarial_rounds': 2,
            'early_stopping': True,
            'convergence_tolerance': 0.05,
            'simulation_mode': True
        }
    }
    
    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    experiment = LLMRecExperiment(experiment_config)
    results = experiment.run_complete_experiment()
    
    if results:
        print("\nğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°resultsç›®å½•")
    else:
        print("\nâŒ å®éªŒæ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")


if __name__ == "__main__":
    main()