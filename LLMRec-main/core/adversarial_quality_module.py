import logging
import numpy as np
from typing import Dict, List, Tuple, Any
import time
from datetime import datetime

logger = logging.getLogger(__name__)

class AdversarialQualityModule:
    """
    å¯¹æŠ—æ€§è´¨é‡ä¿è¯æ¨¡å—
    é€šè¿‡ç”Ÿæˆå™¨-åˆ¤åˆ«å™¨åšå¼ˆæå‡æ•°æ®è´¨é‡
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or self._default_config()
        self.discriminator_history = []
        self.generator_history = []
        
    def _default_config(self) -> Dict:
        return {
            'quality_threshold': 0.7,
            'max_adversarial_rounds': 3,
            'discriminator_confidence_threshold': 0.8,
            'generator_improvement_threshold': 0.1,
            'simulation_mode': True
        }
    
    def run_adversarial_round(self, 
                            real_samples: List[str], 
                            synthetic_samples: List[str],
                            round_num: int = 1) -> Dict:
        """
        è¿è¡Œä¸€è½®å¯¹æŠ—è¿‡ç¨‹
        """
        logger.info(f"âš”ï¸ å¼€å§‹ç¬¬{round_num}è½®å¯¹æŠ—è´¨é‡ä¿è¯...")
        
        start_time = time.time()
        
        # 1. åˆ¤åˆ«å™¨åˆ†æ
        discriminator_results = self._run_discriminator_analysis(
            real_samples, synthetic_samples, round_num
        )
        
        # 2. ç”Ÿæˆå™¨è‡ªæˆ‘åæ€
        generator_results = self._run_generator_reflection(
            discriminator_results['detailed_report'], round_num
        )
        
        # 3. è´¨é‡è¯„ä¼°ä¸ç­›é€‰
        quality_results = self._quality_assessment_and_filtering(
            synthetic_samples, discriminator_results, generator_results
        )
        
        # 4. æ•´åˆç»“æœ
        round_results = {
            'round': round_num,
            'execution_time': time.time() - start_time,
            'discriminator_results': discriminator_results,
            'generator_results': generator_results,
            'quality_results': quality_results,
            'final_samples': quality_results['high_quality_samples'],
            'retention_rate': quality_results['retention_rate'],
            'adversarial_success_rate': quality_results['adversarial_success_rate']
        }
        
        # 5. æ›´æ–°å†å²
        self._update_adversarial_history(round_results)
        
        logger.info(f"âœ… ç¬¬{round_num}è½®å¯¹æŠ—å®Œæˆï¼Œä¿ç•™ç‡: {quality_results['retention_rate']:.3f}")
        
        return round_results
    
    def _run_discriminator_analysis(self, 
                                  real_samples: List[str], 
                                  synthetic_samples: List[str],
                                  round_num: int) -> Dict:
        """
        è¿è¡Œåˆ¤åˆ«å™¨åˆ†æ
        """
        logger.info("ğŸ” åˆ¤åˆ«å™¨å¼€å§‹åˆ†æ...")
        
        if self.config['simulation_mode']:
            return self._simulate_discriminator_analysis(real_samples, synthetic_samples, round_num)
        else:
            return self._real_discriminator_analysis(real_samples, synthetic_samples)
    
    def _simulate_discriminator_analysis(self, 
                                       real_samples: List[str], 
                                       synthetic_samples: List[str],
                                       round_num: int) -> Dict:
        """
        æ¨¡æ‹Ÿåˆ¤åˆ«å™¨åˆ†æè¿‡ç¨‹
        """
        # æ¨¡æ‹Ÿåˆ¤åˆ«å™¨çš„è¯†åˆ«èƒ½åŠ›
        base_accuracy = 0.6 + (round_num - 1) * 0.1  # éšè½®æ¬¡æå‡
        accuracy = min(0.9, base_accuracy + np.random.normal(0, 0.05))
        
        # ç”Ÿæˆæ ·æœ¬çº§åˆ«çš„åˆ¤åˆ«ç»“æœ
        sample_predictions = []
        for i, sample in enumerate(synthetic_samples):
            # æ¨¡æ‹Ÿåˆ¤åˆ«å™¨å¯¹æ¯ä¸ªæ ·æœ¬çš„åˆ¤æ–­
            confidence = np.random.uniform(0.3, 1.0)
            is_detected = confidence > (1 - accuracy)
            
            sample_predictions.append({
                'sample_index': i,
                'sample': sample,
                'is_detected_as_synthetic': is_detected,
                'confidence': confidence,
                'quality_score': 1 - confidence if not is_detected else confidence
            })
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        detailed_report = self._generate_discriminator_report(
            sample_predictions, real_samples, synthetic_samples
        )
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        detected_count = sum(1 for p in sample_predictions if p['is_detected_as_synthetic'])
        detection_rate = detected_count / len(synthetic_samples) if synthetic_samples else 0
        
        return {
            'accuracy': accuracy,
            'detection_rate': detection_rate,
            'sample_predictions': sample_predictions,
            'detailed_report': detailed_report,
            'discriminator_confidence': np.mean([p['confidence'] for p in sample_predictions])
        }
    
    def _generate_discriminator_report(self, 
                                     sample_predictions: List[Dict],
                                     real_samples: List[str],
                                     synthetic_samples: List[str]) -> str:
        """
        ç”Ÿæˆåˆ¤åˆ«å™¨çš„è¯¦ç»†åˆ†ææŠ¥å‘Š
        """
        detected_samples = [p for p in sample_predictions if p['is_detected_as_synthetic']]
        
        # åˆ†ææ£€æµ‹åˆ°çš„æ ·æœ¬çš„ç‰¹å¾
        issues = []
        
        # é•¿åº¦åˆ†æ
        detected_lengths = [len(p['sample'].split()) for p in detected_samples]
        real_lengths = [len(s.split()) for s in real_samples]
        
        if detected_lengths and real_lengths:
            avg_detected_length = np.mean(detected_lengths)
            avg_real_length = np.mean(real_lengths)
            
            if abs(avg_detected_length - avg_real_length) > 2:
                if avg_detected_length > avg_real_length:
                    issues.append("åˆæˆæ•°æ®çš„äº¤äº’é•¿åº¦æ™®éè¿‡é•¿ï¼Œä¸ç¬¦åˆçœŸå®ç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼")
                else:
                    issues.append("åˆæˆæ•°æ®çš„äº¤äº’é•¿åº¦è¿‡çŸ­ï¼Œç¼ºä¹è¶³å¤Ÿçš„ç”¨æˆ·å…´è¶£ä¿¡æ¯")
        
        # æ¨¡æ‹Ÿå…¶ä»–é—®é¢˜
        if np.random.random() < 0.3:
            issues.append("åˆæˆæ•°æ®ä¸­ç‰©å“IDçš„åˆ†å¸ƒè¿‡äºé›†ä¸­ï¼Œç¼ºä¹çœŸå®çš„å¤šæ ·æ€§")
        
        if np.random.random() < 0.2:
            issues.append("ç”¨æˆ·è¡Œä¸ºæ¨¡å¼è¿‡äºè§„å¾‹ï¼Œç¼ºä¹çœŸå®ç”¨æˆ·çš„éšæœºæ€§å’Œä¸ªæ€§åŒ–ç‰¹å¾")
        
        if np.random.random() < 0.4:
            issues.append("é•¿å°¾ç‰©å“çš„å‡ºç°é¢‘ç‡ä¸çœŸå®åˆ†å¸ƒå­˜åœ¨æ˜æ˜¾å·®å¼‚")
        
        # æ„å»ºæŠ¥å‘Š
        report = f"""
åˆ¤åˆ«å™¨åˆ†ææŠ¥å‘Š (Round {len(self.discriminator_history) + 1}):

æ£€æµ‹ç»Ÿè®¡:
- æ€»æ ·æœ¬æ•°: {len(sample_predictions)}
- è¢«æ£€æµ‹ä¸ºåˆæˆæ•°æ®: {len(detected_samples)}
- æ£€æµ‹ç‡: {len(detected_samples)/len(sample_predictions):.3f}

å‘ç°çš„ä¸»è¦é—®é¢˜:
"""
        
        for i, issue in enumerate(issues, 1):
            report += f"{i}. {issue}\n"
        
        if not issues:
            report += "æœªå‘ç°æ˜æ˜¾çš„è´¨é‡é—®é¢˜ï¼Œåˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ç›¸ä¼¼åº¦è¾ƒé«˜ã€‚\n"
        
        report += f"""
å»ºè®®æ”¹è¿›æ–¹å‘:
- ä¼˜åŒ–æ•°æ®ç”Ÿæˆçš„é•¿åº¦åˆ†å¸ƒæ§åˆ¶
- å¢å¼ºç‰©å“é€‰æ‹©çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§
- æ”¹å–„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼çš„è‡ªç„¶æ€§
        """
        
        return report.strip()
    
    def _run_generator_reflection(self, discriminator_report: str, round_num: int) -> Dict:
        """
        è¿è¡Œç”Ÿæˆå™¨è‡ªæˆ‘åæ€
        """
        logger.info("ğŸ¤” ç”Ÿæˆå™¨å¼€å§‹è‡ªæˆ‘åæ€...")
        
        if self.config['simulation_mode']:
            return self._simulate_generator_reflection(discriminator_report, round_num)
        else:
            return self._real_generator_reflection(discriminator_report)
    
    def _simulate_generator_reflection(self, discriminator_report: str, round_num: int) -> Dict:
        """
        æ¨¡æ‹Ÿç”Ÿæˆå™¨åæ€è¿‡ç¨‹
        """
        # åˆ†æåˆ¤åˆ«å™¨æŠ¥å‘Šä¸­çš„å…³é”®é—®é¢˜
        key_issues = self._extract_key_issues(discriminator_report)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = self._generate_improvement_suggestions(key_issues)
        
        # æ¨¡æ‹Ÿåæ€è´¨é‡è¯„åˆ†
        reflection_quality = np.random.uniform(0.6, 0.9)
        
        reflection_text = f"""
ç”Ÿæˆå™¨è‡ªæˆ‘åæ€ (Round {round_num}):

é—®é¢˜åˆ†æ:
æ ¹æ®åˆ¤åˆ«å™¨çš„åé¦ˆï¼Œæˆ‘è¯†åˆ«å‡ºä»¥ä¸‹å…³é”®é—®é¢˜ï¼š
"""
        
        for issue in key_issues:
            reflection_text += f"- {issue}\n"
        
        reflection_text += f"""
æ”¹è¿›ç­–ç•¥:
åŸºäºä¸Šè¿°é—®é¢˜ï¼Œæˆ‘æå‡ºä»¥ä¸‹å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼š
"""
        
        for i, suggestion in enumerate(improvement_suggestions, 1):
            reflection_text += f"{i}. {suggestion}\n"
        
        return {
            'reflection_text': reflection_text,
            'key_issues': key_issues,
            'improvement_suggestions': improvement_suggestions,
            'reflection_quality': reflection_quality
        }
    
    def _extract_key_issues(self, discriminator_report: str) -> List[str]:
        """
        ä»åˆ¤åˆ«å™¨æŠ¥å‘Šä¸­æå–å…³é”®é—®é¢˜
        """
        issues = []
        
        if 'é•¿åº¦' in discriminator_report:
            if 'è¿‡é•¿' in discriminator_report:
                issues.append("äº¤äº’åºåˆ—é•¿åº¦æ§åˆ¶ä¸å½“ï¼Œç”Ÿæˆè¿‡é•¿çš„ç”¨æˆ·ä¼šè¯")
            elif 'è¿‡çŸ­' in discriminator_report:
                issues.append("äº¤äº’åºåˆ—è¿‡çŸ­ï¼Œæ— æ³•å……åˆ†åæ˜ ç”¨æˆ·å…´è¶£")
        
        if 'é›†ä¸­' in discriminator_report or 'å¤šæ ·æ€§' in discriminator_report:
            issues.append("ç‰©å“é€‰æ‹©ç¼ºä¹å¤šæ ·æ€§ï¼Œå­˜åœ¨æ˜æ˜¾çš„é€‰æ‹©åå‘")
        
        if 'è§„å¾‹' in discriminator_report or 'éšæœºæ€§' in discriminator_report:
            issues.append("ç”¨æˆ·è¡Œä¸ºæ¨¡å¼è¿‡äºæœºæ¢°åŒ–ï¼Œç¼ºä¹çœŸå®çš„éšæœºæ€§")
        
        if 'é•¿å°¾' in discriminator_report:
            issues.append("é•¿å°¾ç‰©å“åˆ†å¸ƒä¸çœŸå®æ•°æ®å­˜åœ¨æ˜¾è‘—å·®å¼‚")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å…·ä½“é—®é¢˜ï¼Œæ·»åŠ é€šç”¨é—®é¢˜
        if not issues:
            issues.append("ç”Ÿæˆæ•°æ®çš„æ•´ä½“çœŸå®æ€§æœ‰å¾…æå‡")
        
        return issues
    
    def _generate_improvement_suggestions(self, key_issues: List[str]) -> List[str]:
        """
        æ ¹æ®å…³é”®é—®é¢˜ç”Ÿæˆæ”¹è¿›å»ºè®®
        """
        suggestions = []
        
        for issue in key_issues:
            if 'é•¿åº¦' in issue:
                if 'è¿‡é•¿' in issue:
                    suggestions.append("åœ¨Promptä¸­æ·»åŠ æ˜ç¡®çš„é•¿åº¦ä¸Šé™çº¦æŸï¼Œå¦‚'æ¯ä¸ªç”¨æˆ·æœ€å¤šäº¤äº’8ä¸ªç‰©å“'")
                else:
                    suggestions.append("åœ¨Promptä¸­è¦æ±‚ç”Ÿæˆè¶³å¤Ÿé•¿åº¦çš„äº¤äº’åºåˆ—ï¼Œå¦‚'ç¡®ä¿æ¯ä¸ªç”¨æˆ·è‡³å°‘ä¸5ä¸ªç‰©å“äº¤äº’'")
            
            elif 'å¤šæ ·æ€§' in issue:
                suggestions.append("åœ¨Promptä¸­å¼ºè°ƒç‰©å“é€‰æ‹©çš„å¤šæ ·æ€§ï¼Œè¦æ±‚è¦†ç›–ä¸åŒç±»åˆ«å’Œæµè¡Œåº¦çš„ç‰©å“")
            
            elif 'æœºæ¢°åŒ–' in issue or 'éšæœºæ€§' in issue:
                suggestions.append("åœ¨ç”ŸæˆæŒ‡ä»¤ä¸­åŠ å…¥æ›´å¤šå…³äºç”¨æˆ·ä¸ªæ€§åŒ–å’Œè¡Œä¸ºéšæœºæ€§çš„è¦æ±‚")
            
            elif 'é•¿å°¾' in issue:
                suggestions.append("ç‰¹åˆ«å¼ºè°ƒé•¿å°¾ç‰©å“çš„é‡è¦æ€§ï¼Œè¦æ±‚åœ¨ç”Ÿæˆä¸­åŒ…å«è¶³å¤Ÿæ¯”ä¾‹çš„å†·é—¨ç‰©å“")
        
        # æ·»åŠ é€šç”¨æ”¹è¿›å»ºè®®
        suggestions.append("å¢å¼ºå¯¹çœŸå®ç”¨æˆ·è¡Œä¸ºæ¨¡å¼çš„å­¦ä¹ å’Œæ¨¡æ‹Ÿ")
        suggestions.append("åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥æ›´å¤šçš„éšæœºæ€§å’Œä¸ªæ€§åŒ–å› ç´ ")
        
        return suggestions[:3]  # é™åˆ¶å»ºè®®æ•°é‡
    
    def _quality_assessment_and_filtering(self, 
                                        synthetic_samples: List[str],
                                        discriminator_results: Dict,
                                        generator_results: Dict) -> Dict:
        """
        è´¨é‡è¯„ä¼°ä¸ç­›é€‰
        """
        logger.info("ğŸ“Š è¿›è¡Œè´¨é‡è¯„ä¼°ä¸ç­›é€‰...")
        
        sample_predictions = discriminator_results['sample_predictions']
        quality_threshold = self.config['quality_threshold']
        
        # ç»¼åˆè¯„åˆ†
        filtered_samples = []
        quality_scores = []
        
        for pred in sample_predictions:
            # ç»¼åˆåˆ¤åˆ«å™¨è¯„åˆ†å’Œç”Ÿæˆå™¨åæ€è´¨é‡
            discriminator_score = pred['quality_score']
            generator_boost = generator_results['reflection_quality'] * 0.1  # åæ€è´¨é‡çš„åŠ æˆ
            
            comprehensive_score = discriminator_score + generator_boost
            comprehensive_score = min(1.0, comprehensive_score)  # é™åˆ¶åœ¨[0,1]èŒƒå›´
            
            quality_scores.append(comprehensive_score)
            
            if comprehensive_score >= quality_threshold:
                filtered_samples.append(pred['sample'])
        
        # è®¡ç®—å¯¹æŠ—æˆåŠŸç‡ï¼ˆç”Ÿæˆå™¨æˆåŠŸæ¬ºéª—åˆ¤åˆ«å™¨çš„æ¯”ä¾‹ï¼‰
        not_detected_count = sum(1 for p in sample_predictions if not p['is_detected_as_synthetic'])
        adversarial_success_rate = not_detected_count / len(sample_predictions) if sample_predictions else 0
        
        return {
            'high_quality_samples': filtered_samples,
            'quality_scores': quality_scores,
            'retention_rate': len(filtered_samples) / len(synthetic_samples) if synthetic_samples else 0,
            'adversarial_success_rate': adversarial_success_rate,
            'avg_quality_score': np.mean(quality_scores) if quality_scores else 0,
            'quality_distribution': {
                'high_quality': len([s for s in quality_scores if s >= 0.8]),
                'medium_quality': len([s for s in quality_scores if 0.6 <= s < 0.8]),
                'low_quality': len([s for s in quality_scores if s < 0.6])
            }
        }
    
    def _update_adversarial_history(self, round_results: Dict):
        """
        æ›´æ–°å¯¹æŠ—å†å²è®°å½•
        """
        self.discriminator_history.append({
            'round': round_results['round'],
            'accuracy': round_results['discriminator_results']['accuracy'],
            'detection_rate': round_results['discriminator_results']['detection_rate'],
            'timestamp': datetime.now().isoformat()
        })
        
        self.generator_history.append({
            'round': round_results['round'],
            'reflection_quality': round_results['generator_results']['reflection_quality'],
            'improvement_suggestions_count': len(round_results['generator_results']['improvement_suggestions']),
            'timestamp': datetime.now().isoformat()
        })
    
    def get_adversarial_summary(self) -> Dict:
        """
        è·å–å¯¹æŠ—è®­ç»ƒæ€»ç»“
        """
        if not self.discriminator_history:
            return {'rounds_completed': 0}
        
        discriminator_accuracies = [h['accuracy'] for h in self.discriminator_history]
        generator_qualities = [h['reflection_quality'] for h in self.generator_history]
        
        return {
            'rounds_completed': len(self.discriminator_history),
            'discriminator_improvement': discriminator_accuracies[-1] - discriminator_accuracies[0] if len(discriminator_accuracies) > 1 else 0,
            'generator_improvement': generator_qualities[-1] - generator_qualities[0] if len(generator_qualities) > 1 else 0,
            'avg_discriminator_accuracy': np.mean(discriminator_accuracies),
            'avg_generator_quality': np.mean(generator_qualities),
            'final_discriminator_accuracy': discriminator_accuracies[-1],
            'final_generator_quality': generator_qualities[-1]
        }
    
    def _real_discriminator_analysis(self, real_samples: List[str], synthetic_samples: List[str]) -> Dict:
        """
        çœŸå®çš„åˆ¤åˆ«å™¨åˆ†æï¼ˆéœ€è¦æ¥å…¥LLM APIï¼‰
        """
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„LLM APIè°ƒç”¨
        pass
    
    def _real_generator_reflection(self, discriminator_report: str) -> Dict:
        """
        çœŸå®çš„ç”Ÿæˆå™¨åæ€ï¼ˆéœ€è¦æ¥å…¥LLM APIï¼‰
        """
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„LLM APIè°ƒç”¨
        pass