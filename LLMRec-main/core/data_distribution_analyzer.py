import numpy as np
import json
import logging
from typing import Dict, List, Tuple, Any
from collections import Counter
from scipy.stats import entropy
import pandas as pd

logger = logging.getLogger(__name__)

class DataDistributionAnalyzer:
    """
    æ•°æ®åˆ†å¸ƒåˆ†æå™¨
    åˆ†æçœŸå®æ•°æ®å’Œåˆæˆæ•°æ®çš„åˆ†å¸ƒç‰¹å¾ï¼Œç”Ÿæˆå¯é‡åŒ–çš„ç‰¹å¾å‘é‡
    """
    
    def __init__(self, data_generator=None, config: Dict = None):
        """
        å…¼å®¹å®éªŒæ¡†æ¶çš„æ„é€ å‡½æ•°
        """
        self.data_generator = data_generator
        self.config = config or self._default_config()
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥data_generatorï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        if not data_generator:
            self.real_vector_cache = None
    
    def _default_config(self) -> Dict:
        """
        è¿”å›é»˜è®¤é…ç½®
        """
        return {
            'user_activity_bins': 10,
            'item_popularity_bins': 20,
            'session_length_bins': 15,
            'min_interactions_threshold': 2,
            'clustering_k': 5,
            'similarity_threshold': 0.8,
            'min_samples': 10,
            'max_iterations': 100,
            'convergence_tolerance': 1e-6,
            'feature_weights': {
                'user_activity': 0.3,
                'item_popularity': 0.3,
                'temporal_patterns': 0.2,
                'diversity_metrics': 0.2
            },
            'analysis_methods': [
                'user_activity_distribution',
                'item_popularity_distribution',
                'interaction_temporal_patterns',
                'diversity_analysis'
            ]
        }
    
    def generate_feature_vector(self):
        """
        é€‚é…å™¨æ–¹æ³•ï¼Œä¸ºå®éªŒæ¡†æ¶æä¾›å…¼å®¹æ¥å£
        """
        if hasattr(self, 'data_generator') and self.data_generator:
            # ä»æ•°æ®ç”Ÿæˆå™¨æå–æ ·æœ¬
            real_samples = []
            for user_id, items in self.data_generator.train_items.items():
                if items:
                    sample = f"{user_id} " + " ".join(map(str, items))
                    real_samples.append(sample)
            
            # ä½¿ç”¨ç°æœ‰çš„analyze_real_dataæ–¹æ³•
            try:
                if real_samples:
                    real_vector = self.analyze_real_data(real_samples)
                    
                    # è½¬æ¢ä¸ºå®éªŒæ¡†æ¶æœŸæœ›çš„æ ¼å¼
                    return {
                        'feature_vector': list(real_vector.values()),
                        'user_stats': {
                            'mean': real_vector.get('user_activity_mean', 3.0),
                            'std': real_vector.get('user_activity_std', 1.5)
                        },
                        'item_stats': {
                            'gini': real_vector.get('item_popularity_gini', 0.5),
                            'long_tail_ratio': 1.0 - real_vector.get('head_items_dominance', 0.7)
                        }
                    }
                else:
                    return self._get_default_feature_vector()
                    
            except Exception as e:
                print(f"   âš ï¸ æ•°æ®åˆ†æå‡ºé”™: {e}")
                # è¿”å›åŸºäºæ•°æ®ç”Ÿæˆå™¨ç»Ÿè®¡çš„é»˜è®¤å€¼
                return self._generate_stats_from_data_generator()
        else:
            return self._get_default_feature_vector()
    
    def _generate_stats_from_data_generator(self):
        """
        ä»æ•°æ®ç”Ÿæˆå™¨ç›´æ¥è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not self.data_generator or not self.data_generator.train_items:
                return self._get_default_feature_vector()
            
            # è®¡ç®—ç”¨æˆ·æ´»è·ƒåº¦ç»Ÿè®¡
            user_activities = [len(items) for items in self.data_generator.train_items.values()]
            user_mean = np.mean(user_activities) if user_activities else 3.0
            user_std = np.std(user_activities) if len(user_activities) > 1 else 1.5
            
            # è®¡ç®—ç‰©å“æµè¡Œåº¦ç»Ÿè®¡
            item_counts = {}
            for items in self.data_generator.train_items.values():
                for item in items:
                    item_counts[item] = item_counts.get(item, 0) + 1
            
            if item_counts:
                count_values = list(item_counts.values())
                gini = self._calculate_gini_coefficient(count_values)
                
                # è®¡ç®—é•¿å°¾æ¯”ä¾‹
                median_count = np.median(count_values)
                long_tail_count = sum(1 for c in count_values if c <= median_count)
                long_tail_ratio = long_tail_count / len(count_values)
            else:
                gini = 0.5
                long_tail_ratio = 0.3
            
            return {
                'feature_vector': [user_mean, user_std, gini, long_tail_ratio],
                'user_stats': {
                    'mean': float(user_mean),
                    'std': float(user_std)
                },
                'item_stats': {
                    'gini': float(gini),
                    'long_tail_ratio': float(long_tail_ratio)
                }
            }
            
        except Exception as e:
            print(f"   âš ï¸ ä»æ•°æ®ç”Ÿæˆå™¨è®¡ç®—ç»Ÿè®¡å¤±è´¥: {e}")
            return self._get_default_feature_vector()
    
    def _get_default_feature_vector(self):
        """è¿”å›é»˜è®¤çš„ç‰¹å¾å‘é‡"""
        return {
            'feature_vector': [3.0, 1.5, 0.5, 0.3],
            'user_stats': {'mean': 3.0, 'std': 1.5},
            'item_stats': {'gini': 0.5, 'long_tail_ratio': 0.3}
        }
    
    def analyze_real_data(self, real_data: List[str]) -> Dict[str, float]:
        """
        åˆ†æçœŸå®æ•°æ®åˆ†å¸ƒç‰¹å¾
        è¿”å›é‡åŒ–çš„ç‰¹å¾å‘é‡
        """
        logger.info("ğŸ” å¼€å§‹åˆ†æçœŸå®æ•°æ®åˆ†å¸ƒ...")
        
        # è§£ææ•°æ®
        parsed_data = self._parse_interaction_data(real_data)
        
        if not parsed_data:
            logger.warning("âš ï¸ çœŸå®æ•°æ®ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å‘é‡")
            return self._get_default_vector()
        
        # 1. ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒåˆ†æ
        user_activity_features = self._analyze_user_activity(parsed_data)
        
        # 2. ç‰©å“æµè¡Œåº¦åˆ†æ
        item_popularity_features = self._analyze_item_popularity(parsed_data)
        
        # 3. äº¤äº’ä¼šè¯åˆ†æ
        session_features = self._analyze_session_patterns(parsed_data)
        
        # 4. ç”¨æˆ·èšç±»åˆ†æ
        user_clustering_features = self._analyze_user_clustering(parsed_data)
        
        # æ•´åˆç‰¹å¾å‘é‡
        feature_vector = {
            **user_activity_features,
            **item_popularity_features,
            **session_features,
            **user_clustering_features
        }
        
        # ç¼“å­˜ç»“æœ
        self.real_vector_cache = feature_vector
        
        logger.info(f"âœ… çœŸå®æ•°æ®åˆ†æå®Œæˆï¼Œæå–{len(feature_vector)}ä¸ªç‰¹å¾")
        return feature_vector
    
    def analyze_synthetic_data(self, synthetic_data: List[str]) -> Dict[str, float]:
        """
        åˆ†æåˆæˆæ•°æ®åˆ†å¸ƒç‰¹å¾
        """
        logger.info("ğŸ” å¼€å§‹åˆ†æåˆæˆæ•°æ®åˆ†å¸ƒ...")
        
        # è§£ææ•°æ®
        parsed_data = self._parse_interaction_data(synthetic_data)
        
        if not parsed_data:
            logger.warning("âš ï¸ åˆæˆæ•°æ®ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å‘é‡")
            return self._get_default_vector()
        
        # ä½¿ç”¨ç›¸åŒçš„åˆ†ææµç¨‹
        user_activity_features = self._analyze_user_activity(parsed_data)
        item_popularity_features = self._analyze_item_popularity(parsed_data)
        session_features = self._analyze_session_patterns(parsed_data)
        user_clustering_features = self._analyze_user_clustering(parsed_data)
        
        feature_vector = {
            **user_activity_features,
            **item_popularity_features,
            **session_features,
            **user_clustering_features
        }
        
        logger.info(f"âœ… åˆæˆæ•°æ®åˆ†æå®Œæˆï¼Œæå–{len(feature_vector)}ä¸ªç‰¹å¾")
        return feature_vector
    
    def _parse_interaction_data(self, data: List[str]) -> List[Dict]:
        """
        è§£æäº¤äº’æ•°æ®
        æ ¼å¼: "user_id item1 item2 item3 ..."
        """
        parsed = []
        for line in data:
            if not line.strip():
                continue
            
            parts = line.strip().split()
            if len(parts) < 2:
                continue
            
            try:
                user_id = int(parts[0])
                item_ids = [int(x) for x in parts[1:]]
                
                parsed.append({
                    'user_id': user_id,
                    'items': item_ids,
                    'session_length': len(item_ids)
                })
            except ValueError:
                continue
        
        return parsed
    
    def _analyze_user_activity(self, parsed_data: List[Dict]) -> Dict[str, float]:
        """
        ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒåˆ†æ
        """
        user_interactions = Counter()
        for record in parsed_data:
            user_interactions[record['user_id']] += record['session_length']
        
        if not user_interactions:
            return {'user_activity_mean': 0.0, 'user_activity_std': 0.0, 'user_activity_gini': 0.0}
        
        activity_values = list(user_interactions.values())
        
        return {
            'user_activity_mean': float(np.mean(activity_values)),
            'user_activity_std': float(np.std(activity_values)),
            'user_activity_gini': self._calculate_gini_coefficient(activity_values),
            'active_user_ratio': len([v for v in activity_values if v >= self.config['min_interactions_threshold']]) / len(activity_values)
        }
    
    def _analyze_item_popularity(self, parsed_data: List[Dict]) -> Dict[str, float]:
        """
        ç‰©å“æµè¡Œåº¦åˆ†æ
        """
        item_counts = Counter()
        for record in parsed_data:
            for item_id in record['items']:
                item_counts[item_id] += 1
        
        if not item_counts:
            return {'item_popularity_gini': 0.0, 'item_diversity_entropy': 0.0}
        
        popularity_values = list(item_counts.values())
        
        # è®¡ç®—åŸºå°¼ç³»æ•°
        gini = self._calculate_gini_coefficient(popularity_values)
        
        # è®¡ç®—ç‰©å“ç±»åˆ«ç†µå€¼
        popularity_probs = np.array(popularity_values) / sum(popularity_values)
        diversity_entropy = entropy(popularity_probs)
        
        return {
            'item_popularity_gini': gini,
            'item_diversity_entropy': float(diversity_entropy),
            'unique_items_ratio': len(item_counts) / sum(popularity_values),
            'head_items_dominance': sum(sorted(popularity_values, reverse=True)[:int(len(popularity_values)*0.2)]) / sum(popularity_values)
        }
    
    def _analyze_session_patterns(self, parsed_data: List[Dict]) -> Dict[str, float]:
        """
        äº¤äº’ä¼šè¯æ¨¡å¼åˆ†æ
        """
        session_lengths = [record['session_length'] for record in parsed_data]
        
        if not session_lengths:
            return {'session_length_mean': 0.0, 'session_length_std': 0.0}
        
        return {
            'session_length_mean': float(np.mean(session_lengths)),
            'session_length_std': float(np.std(session_lengths)),
            'session_length_median': float(np.median(session_lengths)),
            'short_session_ratio': len([s for s in session_lengths if s <= 3]) / len(session_lengths),
            'long_session_ratio': len([s for s in session_lengths if s >= 10]) / len(session_lengths)
        }
    
    def _analyze_user_clustering(self, parsed_data: List[Dict]) -> Dict[str, float]:
        """
        ç”¨æˆ·èšç±»åˆ†æ
        """
        # ç®€åŒ–çš„ç”¨æˆ·è¡Œä¸ºç‰¹å¾æå–
        user_features = {}
        for record in parsed_data:
            user_id = record['user_id']
            if user_id not in user_features:
                user_features[user_id] = {
                    'total_interactions': 0,
                    'unique_items': set(),
                    'avg_session_length': 0,
                    'session_count': 0
                }
            
            user_features[user_id]['total_interactions'] += record['session_length']
            user_features[user_id]['unique_items'].update(record['items'])
            user_features[user_id]['session_count'] += 1
        
        if not user_features:
            return {'user_type_diversity': 0.0, 'casual_user_ratio': 0.0, 'power_user_ratio': 0.0}
        
        # è®¡ç®—ç”¨æˆ·ç±»å‹åˆ†å¸ƒ
        user_types = []
        for user_id, features in user_features.items():
            total_int = features['total_interactions']
            unique_items = len(features['unique_items'])
            
            if total_int <= 5:
                user_types.append('casual')
            elif total_int <= 20:
                user_types.append('regular')
            else:
                user_types.append('power')
        
        type_counts = Counter(user_types)
        total_users = len(user_types)
        
        return {
            'user_type_diversity': entropy(list(type_counts.values())),
            'casual_user_ratio': type_counts.get('casual', 0) / total_users,
            'regular_user_ratio': type_counts.get('regular', 0) / total_users,
            'power_user_ratio': type_counts.get('power', 0) / total_users
        }
    
    def _calculate_gini_coefficient(self, values: List) -> float:
        """
        è®¡ç®—åŸºå°¼ç³»æ•°
        """
        if not values:
            return 0.0
        
        sorted_values = sorted([float(x) for x in values])
        n = len(sorted_values)
        
        if n == 0:
            return 0.0
        
        values_array = np.array(sorted_values, dtype=float)
        index_array = np.arange(1, n + 1, dtype=float)
        
        total_sum = float(np.sum(values_array))
        if total_sum == 0:
            return 0.0
        
        gini = (2 * float(np.sum(index_array * values_array))) / (n * total_sum) - (n + 1) / n
        return max(0.0, min(1.0, float(gini)))
    
    def _get_default_vector(self) -> Dict[str, float]:
        """
        è¿”å›é»˜è®¤ç‰¹å¾å‘é‡
        """
        return {
            'user_activity_mean': 0.0,
            'user_activity_std': 0.0,
            'user_activity_gini': 0.0,
            'active_user_ratio': 0.0,
            'item_popularity_gini': 0.0,
            'item_diversity_entropy': 0.0,
            'unique_items_ratio': 0.0,
            'head_items_dominance': 0.0,
            'session_length_mean': 0.0,
            'session_length_std': 0.0,
            'session_length_median': 0.0,
            'short_session_ratio': 0.0,
            'long_session_ratio': 0.0,
            'user_type_diversity': 0.0,
            'casual_user_ratio': 0.0,
            'regular_user_ratio': 0.0,
            'power_user_ratio': 0.0
        }
    
    def calculate_distribution_divergence(self, real_vector: Dict, synth_vector: Dict) -> Dict:
        """
        è®¡ç®—çœŸå®æ•°æ®ä¸åˆæˆæ•°æ®çš„åˆ†å¸ƒåå·®
        """
        divergence_results = {}
        
        # è®¡ç®—å„ç»´åº¦å·®å€¼
        for key in real_vector.keys():
            if key in synth_vector:
                diff = abs(real_vector[key] - synth_vector[key])
                divergence_results[f"{key}_diff"] = diff
        
        # è®¡ç®—JSæ•£åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        real_values = list(real_vector.values())
        synth_values = [synth_vector.get(k, 0) for k in real_vector.keys()]
        
        # å½’ä¸€åŒ–
        real_norm = np.array(real_values) / (np.sum(np.abs(real_values)) + 1e-8)
        synth_norm = np.array(synth_values) / (np.sum(np.abs(synth_values)) + 1e-8)
        
        # è®¡ç®—JSæ•£åº¦
        js_divergence = self._jensen_shannon_divergence(real_norm, synth_norm)
        divergence_results['js_divergence'] = js_divergence
        
        return divergence_results
    
    def _jensen_shannon_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
        """
        è®¡ç®—JSæ•£åº¦
        """
        # ç¡®ä¿æ¦‚ç‡åˆ†å¸ƒéè´Ÿä¸”å½’ä¸€åŒ–
        p = np.abs(p) + 1e-8
        q = np.abs(q) + 1e-8
        p = p / np.sum(p)
        q = q / np.sum(q)
        
        m = 0.5 * (p + q)
        
        kl_pm = entropy(p, m)
        kl_qm = entropy(q, m)
        
        js_div = 0.5 * kl_pm + 0.5 * kl_qm
        return float(js_div)