import logging
from typing import Dict, List, Tuple, Any
import json
import numpy as np

logger = logging.getLogger(__name__)

class DynamicPromptTuner:
    """
    åŠ¨æ€Promptè°ƒä¼˜å™¨
    æ ¹æ®åˆ†å¸ƒåå·®å’Œåˆ¤åˆ«å™¨åé¦ˆæ™ºèƒ½è°ƒæ•´ç”Ÿæˆç­–ç•¥
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or self._default_config()
        self.prompt_library = self._initialize_prompt_library()
        self.adjustment_history = []
        
    def _default_config(self) -> Dict:
        return {
            'divergence_threshold': 0.1,
            'max_prompt_length': 512,
            'adjustment_strength': 0.3,
            'feedback_weight': 0.7,
            'simulation_mode': True,
            'max_iterations': 5,
            'convergence_tolerance': 0.05
        }
    
    def _initialize_prompt_library(self) -> Dict:
        """
        åˆå§‹åŒ–ç»“æ„åŒ–Promptåº“
        """
        return {
            'base_prompt': "è¯·ç”Ÿæˆæ¨èç³»ç»Ÿçš„ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®",
            'session_length_control': {
                'shorten': "é™åˆ¶æ¯ä¸ªç”¨æˆ·çš„äº¤äº’ç‰©å“æ•°é‡åœ¨{max_items}ä¸ªä»¥å†…",
                'lengthen': "ç¡®ä¿æ¯ä¸ªç”¨æˆ·è‡³å°‘ä¸{min_items}ä¸ªç‰©å“äº¤äº’",
                'balance': "ä¿æŒäº¤äº’é•¿åº¦åœ¨{min_items}åˆ°{max_items}ä¸ªç‰©å“ä¹‹é—´"
            },
            'user_activity_control': {
                'increase_casual': "å¢åŠ ä½æ´»è·ƒåº¦ç”¨æˆ·çš„æ¯”ä¾‹ï¼Œç”Ÿæˆæ›´å¤šä»…æœ‰å°‘é‡äº¤äº’çš„ç”¨æˆ·",
                'increase_power': "å¢åŠ é«˜æ´»è·ƒåº¦ç”¨æˆ·çš„æ¯”ä¾‹ï¼Œç”Ÿæˆæ›´å¤šæœ‰å¤§é‡äº¤äº’çš„ç”¨æˆ·",
                'balance_activity': "å¹³è¡¡ä¸åŒæ´»è·ƒåº¦ç”¨æˆ·çš„åˆ†å¸ƒ"
            },
            'item_popularity_control': {
                'reduce_head_bias': "å‡å°‘çƒ­é—¨ç‰©å“çš„é›†ä¸­åº¦ï¼Œå¢åŠ é•¿å°¾ç‰©å“çš„å‡ºç°é¢‘ç‡",
                'increase_diversity': "æé«˜ç‰©å“å¤šæ ·æ€§ï¼Œé¿å…è¿‡åº¦é›†ä¸­åœ¨å°‘æ•°ç‰©å“ä¸Š",
                'natural_distribution': "éµå¾ªè‡ªç„¶çš„ç‰©å“æµè¡Œåº¦åˆ†å¸ƒ"
            },
            'quality_enhancement': {
                'realism': "ç¡®ä¿ç”Ÿæˆçš„æ•°æ®ç¬¦åˆçœŸå®ç”¨æˆ·è¡Œä¸ºæ¨¡å¼",
                'diversity': "ä¿æŒç”¨æˆ·å…´è¶£å’Œç‰©å“ç±»å‹çš„å¤šæ ·æ€§",
                'consistency': "ç»´æŒæ•°æ®çš„å†…åœ¨é€»è¾‘ä¸€è‡´æ€§"
            }
        }
    
    def optimize_prompt(self, real_stats, synthetic_stats_history, iteration):
        """
        é€‚é…å™¨æ–¹æ³•ï¼Œä¸ºå®éªŒæ¡†æ¶æä¾›æ¥å£
        å…¼å®¹ adjust_prompt_strategy çš„åŠŸèƒ½
        """
        try:
            # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œè®¡ç®—æ•£åº¦åˆ†æ
            if synthetic_stats_history and len(synthetic_stats_history) > 0:
                latest_synth_stats = synthetic_stats_history[-1]
                
                # è®¡ç®—ç®€åŒ–çš„æ•£åº¦åˆ†æ
                divergence_analysis = self._calculate_simple_divergence(real_stats, latest_synth_stats)
                
                # ä½¿ç”¨ç°æœ‰çš„ adjust_prompt_strategy æ–¹æ³•
                optimized_prompt, adjustment_details = self.adjust_prompt_strategy(
                    real_vector=real_stats,
                    synth_vector=latest_synth_stats,
                    divergence_analysis=divergence_analysis,
                    discriminator_feedback=""
                )
            else:
                # é¦–æ¬¡è¿­ä»£ï¼Œç”ŸæˆåŸºç¡€prompt
                optimized_prompt = self._generate_initial_prompt(real_stats)
                adjustment_details = {
                    'critical_dimensions': [],
                    'adjustment_strategies': {},
                    'discriminator_feedback_used': False,
                    'js_divergence': 0.0
                }
            
            # ç”Ÿæˆåé¦ˆä¿¡æ¯
            feedback = self._generate_feedback(real_stats, synthetic_stats_history, iteration)
            
            return {
                'optimized_prompt': optimized_prompt,
                'feedback': feedback,
                'iteration': iteration,
                'prompt_version': f'v{iteration + 1}',
                'adjustment_details': adjustment_details
            }
            
        except Exception as e:
            logger.error(f"   âš ï¸ Promptä¼˜åŒ–å¤±è´¥: {e}")
            return {
                'optimized_prompt': self._get_fallback_prompt(real_stats),
                'feedback': "",
                'iteration': iteration,
                'prompt_version': 'fallback'
            }

    def _calculate_simple_divergence(self, real_stats, synth_stats):
        """
        è®¡ç®—ç®€åŒ–çš„æ•£åº¦åˆ†æ
        """
        divergence_analysis = {}
        
        # æ¯”è¾ƒç”¨æˆ·ç»Ÿè®¡
        real_user = real_stats.get('user_stats', {})
        synth_user = synth_stats.get('user_stats', {})
        
        for key in ['mean', 'std']:
            real_val = real_user.get(key, 0)
            synth_val = synth_user.get(key, 0)
            divergence_analysis[f'user_activity_{key}_diff'] = abs(real_val - synth_val)
        
        # æ¯”è¾ƒç‰©å“ç»Ÿè®¡
        real_item = real_stats.get('item_stats', {})
        synth_item = synth_stats.get('item_stats', {})
        
        for key in ['gini', 'long_tail_ratio']:
            real_val = real_item.get(key, 0)
            synth_val = synth_item.get(key, 0)
            divergence_analysis[f'item_{key}_diff'] = abs(real_val - synth_val)
        
        # è®¡ç®—æ•´ä½“JSæ•£åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        real_vector = real_stats.get('feature_vector', [])
        synth_vector = synth_stats.get('feature_vector', [])
        
        if real_vector and synth_vector:
            min_len = min(len(real_vector), len(synth_vector))
            real_arr = np.array(real_vector[:min_len])
            synth_arr = np.array(synth_vector[:min_len])
            
            # ç®€åŒ–çš„JSæ•£åº¦è®¡ç®—
            js_div = np.mean(np.abs(real_arr - synth_arr))
            divergence_analysis['js_divergence'] = js_div
        else:
            divergence_analysis['js_divergence'] = 0.0
        
        return divergence_analysis

    def _generate_initial_prompt(self, real_stats):
        """
        ç”Ÿæˆåˆå§‹prompt
        """
        user_stats = real_stats.get('user_stats', {})
        item_stats = real_stats.get('item_stats', {})
        
        base_prompt = self.prompt_library['base_prompt']
        
        enhanced_prompt = f"""
{base_prompt}ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š

æ•°æ®ç‰¹å¾è¦æ±‚ï¼š
1. ç”¨æˆ·å¹³å‡äº¤äº’æ•°çº¦{user_stats.get('mean', 3):.1f}ä¸ªç‰©å“
2. ç‰©å“æµè¡Œåº¦åŸºå°¼ç³»æ•°çº¦{item_stats.get('gini', 0.5):.2f}
3. é•¿å°¾ç‰©å“æ¯”ä¾‹çº¦{item_stats.get('long_tail_ratio', 0.3):.2f}

æ ¼å¼è¦æ±‚ï¼š
- æ¯è¡Œä¸€ä¸ªç”¨æˆ·çš„äº¤äº’è®°å½•
- æ ¼å¼ï¼šç”¨æˆ·ID ç‰©å“ID1 ç‰©å“ID2 ç‰©å“ID3...
- ç”¨æˆ·IDèŒƒå›´ï¼š0-999ï¼Œç‰©å“IDèŒƒå›´ï¼š0-499
- æ¯ä¸ªç”¨æˆ·2-10ä¸ªä¸é‡å¤ç‰©å“äº¤äº’

è¯·ç”ŸæˆçœŸå®ã€å¤šæ ·åŒ–çš„æ¨èç³»ç»Ÿäº¤äº’æ•°æ®ã€‚
"""
        
        return enhanced_prompt

    def _generate_feedback(self, real_stats, synthetic_stats_history, iteration):
        """
        ç”Ÿæˆåé¦ˆä¿¡æ¯
        """
        if iteration == 0:
            return "é¦–æ¬¡ç”Ÿæˆï¼Œè¯·ç¡®ä¿æ•°æ®çœŸå®æ€§å’Œå¤šæ ·æ€§"
        
        if not synthetic_stats_history:
            return "ç¼ºå°‘å†å²æ•°æ®ï¼Œè¯·ä¿æŒæ•°æ®è´¨é‡"
        
        latest_stats = synthetic_stats_history[-1]
        feedback_items = []
        
        # æ£€æŸ¥ç”¨æˆ·æ´»è·ƒåº¦
        real_mean = real_stats.get('user_stats', {}).get('mean', 3)
        synth_mean = latest_stats.get('user_stats', {}).get('mean', 3)
        
        if abs(real_mean - synth_mean) > 0.5:
            if synth_mean > real_mean:
                feedback_items.append("å‡å°‘ç”¨æˆ·äº¤äº’æ•°é‡ï¼Œé™ä½å¹³å‡æ´»è·ƒåº¦")
            else:
                feedback_items.append("å¢åŠ ç”¨æˆ·äº¤äº’æ•°é‡ï¼Œæé«˜å¹³å‡æ´»è·ƒåº¦")
        
        # æ£€æŸ¥ç‰©å“æµè¡Œåº¦
        real_gini = real_stats.get('item_stats', {}).get('gini', 0.5)
        synth_gini = latest_stats.get('item_stats', {}).get('gini', 0.5)
        
        if abs(real_gini - synth_gini) > 0.1:
            if synth_gini > real_gini:
                feedback_items.append("å‡å°‘ç‰©å“æµè¡Œåº¦é›†ä¸­åº¦ï¼Œå¢åŠ åˆ†å¸ƒå‡åŒ€æ€§")
            else:
                feedback_items.append("å¢åŠ çƒ­é—¨ç‰©å“çš„é›†ä¸­åº¦ï¼Œç¬¦åˆçœŸå®åˆ†å¸ƒ")
        
        if feedback_items:
            return "ç¬¬{}è½®ä¼˜åŒ–å»ºè®®ï¼š{}".format(iteration, "ï¼›".join(feedback_items))
        else:
            return f"ç¬¬{iteration}è½®ï¼šæ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ"

    def _get_fallback_prompt(self, real_stats):
        """
        è·å–åå¤‡prompt
        """
        return """
ç”Ÿæˆæ¨èç³»ç»Ÿç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®ï¼š

è¦æ±‚ï¼š
1. æ¯è¡Œæ ¼å¼ï¼šç”¨æˆ·ID ç‰©å“ID1 ç‰©å“ID2 ...
2. ç”¨æˆ·IDï¼š0-999ï¼Œç‰©å“IDï¼š0-499
3. æ¯ç”¨æˆ·2-8ä¸ªä¸é‡å¤ç‰©å“
4. ä½“ç°çœŸå®çš„ç”¨æˆ·è¡Œä¸ºå’Œç‰©å“æµè¡Œåº¦åˆ†å¸ƒ

è¯·ç”Ÿæˆé«˜è´¨é‡çš„äº¤äº’æ•°æ®ã€‚
"""
    
    def adjust_prompt_strategy(self, 
                             real_vector: Dict, 
                             synth_vector: Dict, 
                             divergence_analysis: Dict,
                             discriminator_feedback: str = "") -> Tuple[str, Dict]:
        """
        æ ¹æ®åå·®åˆ†æå’Œåˆ¤åˆ«å™¨åé¦ˆè°ƒæ•´Promptç­–ç•¥
        
        Returns:
            Tuple[str, Dict]: (è°ƒæ•´åçš„prompt, è°ƒæ•´è¯¦æƒ…)
        """
        logger.info("ğŸ¯ å¼€å§‹åŠ¨æ€Promptè°ƒä¼˜...")
        
        # 1. åˆ†æå…³é”®åå·®ç»´åº¦
        critical_dimensions = self._identify_critical_divergences(divergence_analysis)
        
        # 2. æ ¹æ®åå·®ç”Ÿæˆè°ƒæ•´ç­–ç•¥
        adjustment_strategies = self._generate_adjustment_strategies(
            real_vector, synth_vector, critical_dimensions
        )
        
        # 3. èåˆåˆ¤åˆ«å™¨åé¦ˆ
        if discriminator_feedback:
            feedback_strategies = self._parse_discriminator_feedback(discriminator_feedback)
            adjustment_strategies.update(feedback_strategies)
        
        # 4. æ„å»ºä¼˜åŒ–åçš„Prompt
        optimized_prompt = self._construct_optimized_prompt(adjustment_strategies)
        
        # 5. è®°å½•è°ƒæ•´å†å²
        adjustment_details = {
            'critical_dimensions': critical_dimensions,
            'adjustment_strategies': adjustment_strategies,
            'discriminator_feedback_used': bool(discriminator_feedback),
            'js_divergence': divergence_analysis.get('js_divergence', 0)
        }
        
        self.adjustment_history.append(adjustment_details)
        
        logger.info(f"âœ… Promptè°ƒä¼˜å®Œæˆï¼Œè¯†åˆ«{len(critical_dimensions)}ä¸ªå…³é”®åå·®ç»´åº¦")
        
        return optimized_prompt, adjustment_details
    
    def _identify_critical_divergences(self, divergence_analysis: Dict) -> List[str]:
        """
        è¯†åˆ«å…³é”®åå·®ç»´åº¦
        """
        critical_dims = []
        threshold = self.config['divergence_threshold']
        
        for key, value in divergence_analysis.items():
            if key.endswith('_diff') and value > threshold:
                # ç§»é™¤'_diff'åç¼€è·å–åŸå§‹ç»´åº¦å
                original_dim = key[:-5]
                critical_dims.append(original_dim)
        
        # æ ¹æ®é‡è¦æ€§æ’åº
        critical_dims.sort(key=lambda x: divergence_analysis.get(f"{x}_diff", 0), reverse=True)
        
        return critical_dims[:5]  # åªå…³æ³¨å‰5ä¸ªæœ€é‡è¦çš„åå·®
    
    def _generate_adjustment_strategies(self, 
                                      real_vector: Dict, 
                                      synth_vector: Dict, 
                                      critical_dimensions: List[str]) -> Dict:
        """
        æ ¹æ®åå·®ç»´åº¦ç”Ÿæˆè°ƒæ•´ç­–ç•¥
        """
        strategies = {}
        
        for dim in critical_dimensions:
            real_val = real_vector.get(dim, 0)
            synth_val = synth_vector.get(dim, 0)
            
            if 'session_length' in dim:
                strategies.update(self._adjust_session_length(real_val, synth_val, dim))
            elif 'user_activity' in dim:
                strategies.update(self._adjust_user_activity(real_val, synth_val, dim))
            elif 'item_popularity' in dim or 'item_diversity' in dim:
                strategies.update(self._adjust_item_distribution(real_val, synth_val, dim))
            elif 'user_type' in dim:
                strategies.update(self._adjust_user_types(real_val, synth_val, dim))
        
        return strategies
    
    def _adjust_session_length(self, real_val: float, synth_val: float, dim: str) -> Dict:
        """
        è°ƒæ•´ä¼šè¯é•¿åº¦ç›¸å…³ç­–ç•¥
        """
        strategies = {}
        
        if 'mean' in dim:
            if synth_val > real_val:
                # åˆæˆæ•°æ®ä¼šè¯è¿‡é•¿
                max_items = max(3, int(real_val * 1.2))
                strategies['session_control'] = self.prompt_library['session_length_control']['shorten'].format(max_items=max_items)
            else:
                # åˆæˆæ•°æ®ä¼šè¯è¿‡çŸ­
                min_items = max(2, int(real_val * 0.8))
                strategies['session_control'] = self.prompt_library['session_length_control']['lengthen'].format(min_items=min_items)
        
        elif 'short_session_ratio' in dim:
            if synth_val > real_val:
                strategies['session_diversity'] = "å‡å°‘è¿‡çŸ­ä¼šè¯çš„æ¯”ä¾‹ï¼Œå¢åŠ ä¸­ç­‰é•¿åº¦çš„äº¤äº’"
            else:
                strategies['session_diversity'] = "å¢åŠ ä¸€äº›ç®€çŸ­äº¤äº’ï¼Œä¿æŒç”¨æˆ·è¡Œä¸ºçš„å¤šæ ·æ€§"
        
        return strategies
    
    def _adjust_user_activity(self, real_val: float, synth_val: float, dim: str) -> Dict:
        """
        è°ƒæ•´ç”¨æˆ·æ´»è·ƒåº¦ç›¸å…³ç­–ç•¥
        """
        strategies = {}
        
        if 'gini' in dim:
            if synth_val > real_val:
                strategies['activity_balance'] = self.prompt_library['user_activity_control']['balance_activity']
            else:
                strategies['activity_diversity'] = "å¢åŠ ç”¨æˆ·æ´»è·ƒåº¦çš„å·®å¼‚åŒ–ï¼Œåˆ›é€ æ›´æ˜æ˜¾çš„æ´»è·ƒåº¦å±‚æ¬¡"
        
        elif 'casual_user_ratio' in dim:
            if synth_val < real_val:
                strategies['user_type_adjust'] = self.prompt_library['user_activity_control']['increase_casual']
        
        elif 'power_user_ratio' in dim:
            if synth_val < real_val:
                strategies['user_type_adjust'] = self.prompt_library['user_activity_control']['increase_power']
        
        return strategies
    
    def _adjust_item_distribution(self, real_val: float, synth_val: float, dim: str) -> Dict:
        """
        è°ƒæ•´ç‰©å“åˆ†å¸ƒç›¸å…³ç­–ç•¥
        """
        strategies = {}
        
        if 'popularity_gini' in dim:
            if synth_val < real_val:
                strategies['item_concentration'] = "å¢åŠ çƒ­é—¨ç‰©å“çš„é›†ä¸­åº¦ï¼Œç¬¦åˆçœŸå®çš„å¹‚å¾‹åˆ†å¸ƒ"
            else:
                strategies['item_distribution'] = self.prompt_library['item_popularity_control']['reduce_head_bias']
        
        elif 'diversity_entropy' in dim:
            if synth_val < real_val:
                strategies['item_variety'] = self.prompt_library['item_popularity_control']['increase_diversity']
        
        elif 'head_items_dominance' in dim:
            if synth_val < real_val:
                strategies['popularity_realism'] = "å¢å¼ºå¤´éƒ¨ç‰©å“çš„ä¸»å¯¼åœ°ä½ï¼Œåæ˜ çœŸå®çš„æµè¡Œåº¦åˆ†å¸ƒ"
        
        return strategies
    
    def _adjust_user_types(self, real_val: float, synth_val: float, dim: str) -> Dict:
        """
        è°ƒæ•´ç”¨æˆ·ç±»å‹åˆ†å¸ƒç­–ç•¥
        """
        strategies = {}
        
        if 'diversity' in dim:
            if synth_val < real_val:
                strategies['user_type_variety'] = "å¢åŠ ç”¨æˆ·ç±»å‹çš„å¤šæ ·æ€§ï¼Œåˆ›é€ æ›´ä¸°å¯Œçš„ç”¨æˆ·ç”»åƒ"
        
        return strategies
    
    def _parse_discriminator_feedback(self, feedback: str) -> Dict:
        """
        è§£æåˆ¤åˆ«å™¨åé¦ˆå¹¶ç”Ÿæˆå¯¹åº”ç­–ç•¥
        """
        strategies = {}
        
        # ç®€åŒ–çš„åé¦ˆè§£æé€»è¾‘
        feedback_lower = feedback.lower()
        
        if 'è¿‡äºé›†ä¸­' in feedback or 'concentrated' in feedback_lower:
            strategies['anti_concentration'] = self.prompt_library['item_popularity_control']['increase_diversity']
        
        if 'ä¸å¤ŸçœŸå®' in feedback or 'unrealistic' in feedback_lower:
            strategies['realism_boost'] = self.prompt_library['quality_enhancement']['realism']
        
        if 'æ¨¡å¼å•ä¸€' in feedback or 'pattern' in feedback_lower:
            strategies['pattern_diversity'] = self.prompt_library['quality_enhancement']['diversity']
        
        if 'é•¿åº¦å¼‚å¸¸' in feedback or 'length' in feedback_lower:
            strategies['length_normalize'] = "ç¡®ä¿äº¤äº’é•¿åº¦ç¬¦åˆè‡ªç„¶åˆ†å¸ƒ"
        
        if 'ç”¨æˆ·è¡Œä¸º' in feedback or 'behavior' in feedback_lower:
            strategies['behavior_realism'] = "æ¨¡æ‹Ÿæ›´çœŸå®çš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼å’Œåå¥½"
        
        return strategies
    
    def _construct_optimized_prompt(self, strategies: Dict) -> str:
        """
        æ„å»ºä¼˜åŒ–åçš„Prompt
        """
        base_prompt = self.prompt_library['base_prompt']
        
        # æ·»åŠ ç­–ç•¥æŒ‡ä»¤
        strategy_instructions = []
        for strategy_name, instruction in strategies.items():
            strategy_instructions.append(f"- {instruction}")
        
        if strategy_instructions:
            optimized_prompt = f"{base_prompt}ï¼Œè¯·ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š\n" + "\n".join(strategy_instructions)
        else:
            optimized_prompt = base_prompt
        
        # æ·»åŠ æ ¼å¼è¦æ±‚
        format_instruction = "\n\nè¯·æŒ‰ä»¥ä¸‹æ ¼å¼ç”Ÿæˆæ•°æ®ï¼šæ¯è¡Œä¸€ä¸ªç”¨æˆ·çš„äº¤äº’è®°å½•ï¼Œæ ¼å¼ä¸º'ç”¨æˆ·ID ç‰©å“ID1 ç‰©å“ID2 ...'ï¼Œç”¨æˆ·IDå’Œç‰©å“IDéƒ½æ˜¯æ­£æ•´æ•°ã€‚"
        optimized_prompt += format_instruction
        
        return optimized_prompt
    
    def generate_synthetic_data(self, prompt: str, num_samples: int = 50) -> List[str]:
        """
        ä½¿ç”¨ä¼˜åŒ–åçš„Promptç”Ÿæˆåˆæˆæ•°æ®
        """
        logger.info(f"ğŸ”„ ä½¿ç”¨è°ƒä¼˜Promptç”Ÿæˆ{num_samples}ä¸ªåˆæˆæ ·æœ¬...")
        
        if self.config['simulation_mode']:
            return self._simulate_data_generation(prompt, num_samples)
        else:
            # è¿™é‡Œåº”è¯¥æ¥å…¥çœŸå®çš„LLM API
            return self._call_llm_api(prompt, num_samples)
    
    def _simulate_data_generation(self, prompt: str, num_samples: int) -> List[str]:
        """
        æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        synthetic_data = []
        
        # æ ¹æ®promptä¸­çš„æŒ‡ä»¤è°ƒæ•´ç”Ÿæˆå‚æ•°
        avg_length = 5
        if 'é™åˆ¶' in prompt and 'ä¸ªä»¥å†…' in prompt:
            try:
                # æå–æœ€å¤§é•¿åº¦é™åˆ¶
                import re
                match = re.search(r'(\d+)ä¸ªä»¥å†…', prompt)
                if match:
                    avg_length = min(avg_length, int(match.group(1)))
            except:
                pass
        
        if 'è‡³å°‘' in prompt and 'ä¸ªç‰©å“' in prompt:
            try:
                import re
                match = re.search(r'è‡³å°‘(\d+)ä¸ªç‰©å“', prompt)
                if match:
                    avg_length = max(avg_length, int(match.group(1)))
            except:
                pass
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        for i in range(num_samples):
            user_id = i
            # æ ¹æ®promptè°ƒæ•´ä¼šè¯é•¿åº¦
            if 'å¢åŠ ä½æ´»è·ƒåº¦' in prompt:
                session_length = np.random.poisson(2) + 1
            elif 'å¢åŠ é«˜æ´»è·ƒåº¦' in prompt:
                session_length = np.random.poisson(8) + 3
            else:
                session_length = np.random.poisson(avg_length) + 1
            
            session_length = max(1, min(session_length, 15))
            
            # ç”Ÿæˆç‰©å“ID
            if 'é•¿å°¾' in prompt or 'å¤šæ ·æ€§' in prompt:
                # å¢åŠ é•¿å°¾ç‰©å“
                items = list(np.random.choice(range(100, 1000), size=session_length, replace=False))
            else:
                # å¸¸è§„ç‰©å“åˆ†å¸ƒ
                items = list(np.random.choice(range(1, 100), size=session_length, replace=False))
            
            items_str = ' '.join(map(str, items))
            synthetic_data.append(f"{user_id} {items_str}")
        
        return synthetic_data
    
    def _call_llm_api(self, prompt: str, num_samples: int) -> List[str]:
        """
        è°ƒç”¨çœŸå®LLM APIç”Ÿæˆæ•°æ®
        """
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„APIè°ƒç”¨
        # ä¾‹å¦‚OpenAI GPT-4 APIè°ƒç”¨
        pass
    
    def get_adjustment_summary(self) -> Dict:
        """
        è·å–è°ƒæ•´å†å²æ€»ç»“
        """
        if not self.adjustment_history:
            return {'total_adjustments': 0}
        
        return {
            'total_adjustments': len(self.adjustment_history),
            'avg_js_divergence': np.mean([h.get('js_divergence', 0) for h in self.adjustment_history]),
            'most_critical_dimensions': self._get_most_frequent_dimensions(),
            'feedback_usage_rate': sum(1 for h in self.adjustment_history if h['discriminator_feedback_used']) / len(self.adjustment_history)
        }
    
    def _get_most_frequent_dimensions(self) -> List[str]:
        """
        è·å–æœ€é¢‘ç¹çš„åå·®ç»´åº¦
        """
        dimension_counts = {}
        for history in self.adjustment_history:
            for dim in history.get('critical_dimensions', []):
                dimension_counts[dim] = dimension_counts.get(dim, 0) + 1
        
        return sorted(dimension_counts.keys(), key=lambda x: dimension_counts[x], reverse=True)[:3]